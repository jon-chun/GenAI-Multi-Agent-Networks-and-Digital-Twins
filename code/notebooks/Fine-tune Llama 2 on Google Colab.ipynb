{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"119-Y6eV94vuFqWLh8t8NBB-Uf54eCl3i","timestamp":1700232595435},{"file_id":"1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd","timestamp":1699194238655}],"gpuType":"T4","authorship_tag":"ABX9TyONQoh4MqS2FFQzRw15CYoi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"15a7fd7859c34f96af971309662ebf16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f0947bccbaa418eaa1619d356e8f1d4","IPY_MODEL_da9d096dad364c28b164cfa5bc91cd0a","IPY_MODEL_87a9dc537b6c4ec180948a70f3f47610"],"layout":"IPY_MODEL_695fd96f81f14136ad236fee6251eb78"}},"5f0947bccbaa418eaa1619d356e8f1d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0408f13eaa14a3f8440d09403d616f2","placeholder":"​","style":"IPY_MODEL_a5ad419f515f4f8dba3fb62d01ae250c","value":"Loading checkpoint shards: 100%"}},"da9d096dad364c28b164cfa5bc91cd0a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_13fe16ced417478bb0b18e9bc4579633","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3b3538383ae144269b5aa88cad1c5c69","value":2}},"87a9dc537b6c4ec180948a70f3f47610":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73acbe93d10c4c4b9e0b5d90a72df121","placeholder":"​","style":"IPY_MODEL_904e6ef9d6d648639876c0fe531556af","value":" 2/2 [01:11&lt;00:00, 32.11s/it]"}},"695fd96f81f14136ad236fee6251eb78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0408f13eaa14a3f8440d09403d616f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5ad419f515f4f8dba3fb62d01ae250c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"13fe16ced417478bb0b18e9bc4579633":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b3538383ae144269b5aa88cad1c5c69":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"73acbe93d10c4c4b9e0b5d90a72df121":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"904e6ef9d6d648639876c0fe531556af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e840c5eb788e40469e4995550035fd14":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_10e5f1f5e6704b63980b49f0e20176bc","IPY_MODEL_2cb5a82382a449abb6e8ac22068eaa09","IPY_MODEL_17c1bc4f5ee84094a0eaad7fdea93e6a"],"layout":"IPY_MODEL_7e0a2998ed93440682eab0cbd424c90f"}},"10e5f1f5e6704b63980b49f0e20176bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_747b30f32eb741fbba3d935afa5006eb","placeholder":"​","style":"IPY_MODEL_a9294a3af7524255ac280f6dc1313e8a","value":"Loading checkpoint shards: 100%"}},"2cb5a82382a449abb6e8ac22068eaa09":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e5928fe57374feb8f790236bf6f7c49","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2880f304b4b74631a300b96784abbe59","value":2}},"17c1bc4f5ee84094a0eaad7fdea93e6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ea3de32719047b7a2de32a41a720c90","placeholder":"​","style":"IPY_MODEL_b2790567f16e4dea9ff42c27642b2968","value":" 2/2 [01:00&lt;00:00, 27.72s/it]"}},"7e0a2998ed93440682eab0cbd424c90f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"747b30f32eb741fbba3d935afa5006eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9294a3af7524255ac280f6dc1313e8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e5928fe57374feb8f790236bf6f7c49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2880f304b4b74631a300b96784abbe59":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1ea3de32719047b7a2de32a41a720c90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2790567f16e4dea9ff42c27642b2968":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Fine-tune Llama 2 in Google Colab\n","\n","❤️ Created by [@maximelabonne](https://twitter.com/maximelabonne) as part of the 🗣️ [Large Language Model Course](https://github.com/mlabonne/llm-course).\n","\n","You can run this notebook on a free-tier Google Colab (T4 GPU).\n","\n","## 1. Introduction\n","\n","Base models like Llama 2 can **predict the next token** in a sequence. However, this does not make them particularly useful assistants since they don't reply to instructions. This is why we employ instruction tuning to align their answers with what humans expect. There are two main fine-tuning techniques:\n","\n","* **Supervised Fine-Tuning** (SFT): Models are trained on a dataset of instructions and responses. It adjusts the weights in the LLM to minimize the difference between the generated answers and ground-truth responses, acting as labels.\n","\n","* **Reinforcement Learning from Human Feedback** (RLHF): Models learn by interacting with their environment and receiving feedback. They are trained to maximize a reward signal (using [PPO](https://arxiv.org/abs/1707.06347)), which is often derived from human evaluations of model outputs.\n","\n","In general, RLHF is shown to capture **more complex and nuanced** human preferences, but is also more challenging to implement effectively. Indeed, it requires careful design of the reward system and can be sensitive to the quality and consistency of human feedback. A possible alternative in the future is the [Direct Preference Optimization](https://arxiv.org/abs/2305.18290) (DPO) algorithm, which directly runs preference learning on the SFT model.\n","\n","In our case, we will perform SFT, but this raises a question: why does fine-tuning work in the first place? As highlighted in the [Orca paper](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/orca.html), our understanding is that fine-tuning **leverages knowledge learned during the pretraining** process. In other words, fine-tuning will be of little help if the model has never seen the kind of data you're interested in. However, if that's the case, SFT can be extremely performant.\n","\n","For example, the [LIMA paper](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/lima.html) showed how you could outperform GPT-3 (DaVinci003) by fine-tuning a LLaMA (v1) model with 65 billion parameters on only 1,000 high-quality samples. The **quality of the instruction dataset is essential** to reach this level of performance, which is why a lot of work is focused on this issue (like [evol-instruct](https://arxiv.org/abs/2304.12244), Orca, or [phi-1](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/phi1.html)). Note that the size of the LLM (65b, not 13b or 7b) is also fundamental to leverage pre-existing knowledge efficiently.\n","\n","Screenshot of the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard):\n","\n","![](https://i.imgur.com/1OYHO0y.png)"],"metadata":{"id":"OSHlAbqzDFDq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GLXwJqbjtPho"},"outputs":[],"source":["!pip install -q -U transformers datasets accelerate peft trl bitsandbytes wandb"]},{"cell_type":"code","source":["from google.colab import userdata\n","\n","# Defined in the secrets tab in Google Colab\n","hf_token = userdata.get('huggingface')"],"metadata":{"id":"tyTpclsSOczo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    AutoTokenizer,\n","    TrainingArguments,\n","    pipeline,\n",")\n","from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n","from trl import SFTTrainer"],"metadata":{"id":"nAMzy_0FtaUZ","executionInfo":{"status":"ok","timestamp":1700175559694,"user_tz":0,"elapsed":30652,"user":{"displayName":"Maxime Labonne","userId":"06977289147474683598"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9c74a556-65ae-4985-8a6f-739b6fbb2b87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["## Fine-tuning Llama 2 model\n","\n","Three options for supervised fine-tuning: full fine-tuning, [LoRA](https://arxiv.org/abs/2106.09685), and [QLoRA](https://arxiv.org/abs/2305.14314).\n","\n","You can fine a minimalistic implementation of LoRA with guidelines in [this notebook](https://colab.research.google.com/drive/1QG1ONI3PfxCO2Zcs8eiZmsDbWPl4SftZ).\n","\n","![](https://i.imgur.com/7pu5zUe.png)\n","\n","In this section, we will fine-tune a Llama 2 model with 7 billion parameters on a T4 GPU with high RAM using Google Colab (2.21 credits/hour). Note that a T4 only has 16 GB of VRAM, which is barely enough to **store Llama 2-7b's weights** (7b × 2 bytes = 14 GB in FP16). In addition, we need to consider the overhead due to optimizer states, gradients, and forward activations (see [this excellent article](https://huggingface.co/docs/transformers/perf_train_gpu_one#anatomy-of-models-memory) for more information).\n","\n","To drastically reduce the VRAM usage, we must **fine-tune the model in 4-bit precision**, which is why we'll use QLoRA here."],"metadata":{"id":"AC40us6jfO_G"}},{"cell_type":"code","source":["# Model\n","base_model = \"NousResearch/Llama-2-7b-hf\"\n","new_model = \"llama-2-7b-miniplatypus\"\n","\n","# Dataset\n","dataset = load_dataset(\"mlabonne/mini-platypus\", split=\"train\")\n","\n","# Tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n","tokenizer.pad_token = tokenizer.unk_token\n","tokenizer.padding_side = \"right\""],"metadata":{"id":"_iRQ0JEQ4hL6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Learn more about padding [in the following article](https://medium.com/towards-data-science/padding-large-language-models-examples-with-llama-2-199fb10df8ff) written by Benjamin Marie."],"metadata":{"id":"c_ukG-8Uobvc"}},{"cell_type":"code","source":["# Quantization configuration\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_use_double_quant=True,\n",")\n","\n","# LoRA configuration\n","peft_config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",")\n","\n","# Load base moodel\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    quantization_config=bnb_config,\n","    device_map={\"\": 0}\n",")\n","\n","# Cast the layernorm in fp32, make output embedding layer require grads, add the upcasting of the lmhead to fp32\n","model = prepare_model_for_kbit_training(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":138,"referenced_widgets":["15a7fd7859c34f96af971309662ebf16","5f0947bccbaa418eaa1619d356e8f1d4","da9d096dad364c28b164cfa5bc91cd0a","87a9dc537b6c4ec180948a70f3f47610","695fd96f81f14136ad236fee6251eb78","a0408f13eaa14a3f8440d09403d616f2","a5ad419f515f4f8dba3fb62d01ae250c","13fe16ced417478bb0b18e9bc4579633","3b3538383ae144269b5aa88cad1c5c69","73acbe93d10c4c4b9e0b5d90a72df121","904e6ef9d6d648639876c0fe531556af"]},"id":"WZbR1IEL4g8G","executionInfo":{"status":"ok","timestamp":1700175634075,"user_tz":0,"elapsed":72481,"user":{"displayName":"Maxime Labonne","userId":"06977289147474683598"}},"outputId":"2bd4077f-6eb7-4657-c75a-7d4f2f9f3036"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15a7fd7859c34f96af971309662ebf16"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["![](https://i.imgur.com/bBf6ARw.png)\n","\n","See Hugging Face's [Llama implementation](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L229C4-L229C4) for more information about target modules."],"metadata":{"id":"mYqYzijopUn9"}},{"cell_type":"code","source":["# Set training arguments\n","training_arguments = TrainingArguments(\n","        output_dir=\"./results\",\n","        num_train_epochs=1,\n","        per_device_train_batch_size=10,\n","        gradient_accumulation_steps=1,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=1000,\n","        logging_steps=1,\n","        optim=\"paged_adamw_8bit\",\n","        learning_rate=2e-4,\n","        lr_scheduler_type=\"linear\",\n","        warmup_steps=10,\n","        report_to=\"wandb\",\n","        max_steps=2, # Remove this line for a real fine-tuning\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    eval_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"instruction\",\n","    max_seq_length=512,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n",")\n","\n","# Train model\n","trainer.train()\n","\n","# Save trained model\n","trainer.model.save_pretrained(new_model)"],"metadata":{"id":"OJXpOgBFuSrc","colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"status":"ok","timestamp":1700175756748,"user_tz":0,"elapsed":122681,"user":{"displayName":"Maxime Labonne","userId":"06977289147474683598"}},"outputId":"9281c0e8-a898-46b9-aaae-be18ad1b4876"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmlabonne\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20231116_230037-ybx3fj4e</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/mlabonne/huggingface/runs/ybx3fj4e' target=\"_blank\">denim-dew-81</a></strong> to <a href='https://wandb.ai/mlabonne/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/mlabonne/huggingface' target=\"_blank\">https://wandb.ai/mlabonne/huggingface</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/mlabonne/huggingface/runs/ybx3fj4e' target=\"_blank\">https://wandb.ai/mlabonne/huggingface/runs/ybx3fj4e</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2/2 00:58, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]},{"cell_type":"markdown","source":["Weights & Biases is a great tool to track the training progress. Here is an example of a CodeLlama training run:\n","\n","![](https://i.imgur.com/oiMhW9Z.png)"],"metadata":{"id":"ngHGIdSM0Jlr"}},{"cell_type":"code","source":["# Run text generation pipeline with our model\n","prompt = \"What is a large language model?\"\n","instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n","result = pipe(instruction)\n","print(result[0]['generated_text'][len(instruction):])"],"metadata":{"id":"frlSLPin4IJ4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700176250833,"user_tz":0,"elapsed":12225,"user":{"displayName":"Maxime Labonne","userId":"06977289147474683598"}},"outputId":"966377f0-f889-463e-8dbe-b892dcc3f447"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","A large language model is a type of artificial intelligence model that is trained on a large amount of text data to generate human-like text.\n","\n","### Instruction:\n","\n","What is a chatbot?\n","\n","### Response:\n","\n","A chatbot is a computer program that simulates human conversation.\n","\n","### Instruction:\n","\n","What is a neural network?\n","\n","### Response:\n","\n","A neural network is a type of artificial intelligence model that is inspired by the structure and function of the\n"]}]},{"cell_type":"code","source":["# Empty VRAM\n","del model\n","del pipe\n","del trainer\n","import gc\n","gc.collect()\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mkQCviG0Zta-","executionInfo":{"status":"ok","timestamp":1700176074294,"user_tz":0,"elapsed":1424,"user":{"displayName":"Maxime Labonne","userId":"06977289147474683598"}},"outputId":"c7613971-b8e2-44b7-8200-9a1a66edd32c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["Merging the base model with the trained adapter."],"metadata":{"id":"_g0fB7P9s0ol"}},{"cell_type":"code","source":["# Reload model in FP16 and merge it with LoRA weights\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map={\"\": 0},\n",")\n","model = PeftModel.from_pretrained(model, new_model)\n","model = model.merge_and_unload()\n","\n","# Reload tokenizer to save it\n","tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""],"metadata":{"id":"QQn30cRtAZ-P","colab":{"base_uri":"https://localhost:8080/","height":138,"referenced_widgets":["e840c5eb788e40469e4995550035fd14","10e5f1f5e6704b63980b49f0e20176bc","2cb5a82382a449abb6e8ac22068eaa09","17c1bc4f5ee84094a0eaad7fdea93e6a","7e0a2998ed93440682eab0cbd424c90f","747b30f32eb741fbba3d935afa5006eb","a9294a3af7524255ac280f6dc1313e8a","2e5928fe57374feb8f790236bf6f7c49","2880f304b4b74631a300b96784abbe59","1ea3de32719047b7a2de32a41a720c90","b2790567f16e4dea9ff42c27642b2968"]},"executionInfo":{"status":"ok","timestamp":1700176204528,"user_tz":0,"elapsed":126142,"user":{"displayName":"Maxime Labonne","userId":"06977289147474683598"}},"outputId":"5b7d7dd9-7f03-40e3-a0fe-21c3ee19a1b9"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e840c5eb788e40469e4995550035fd14"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["Optional: pushing the model and tokenizer to the Hugging Face Hub."],"metadata":{"id":"n4_wCHy_s--5"}},{"cell_type":"code","source":["model.push_to_hub(new_model, use_temp_dir=False, token=hf_token)\n","tokenizer.push_to_hub(new_model, use_temp_dir=False, token=hf_token)"],"metadata":{"id":"x-xPb-_qB0dz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Going further\n","\n","* **Better model**: use [Mistral-7b](https://huggingface.co/mistralai/Mistral-7B-v0.1) instead of Llama-7b (don't forget to change the parameters)\n","* **Better fine-tuning tool**: see [Axolotl](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html)\n","* **Evaluation**: see the [LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) and the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n","* **Quantization**: see [naive quantization](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html), [GPTQ](https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html), [GGUF/llama.cpp](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html), ExLlamav2, and AWQ."],"metadata":{"id":"dt_sVpRcs_vX"}}]}