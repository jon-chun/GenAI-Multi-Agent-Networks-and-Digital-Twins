{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06d4e21dfa0740febe31d787e1b8b921": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92c0910ec234458f8120d0e59de237b0",
              "IPY_MODEL_c89a0c4e825044b4ae203fc9a80a53dd",
              "IPY_MODEL_942ff6e9243d4167b7fe3852d4fc2579"
            ],
            "layout": "IPY_MODEL_fcec48380c0348de9afee360459afa86"
          }
        },
        "92c0910ec234458f8120d0e59de237b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ada7089cd9f2448898fd7abcdc105502",
            "placeholder": "​",
            "style": "IPY_MODEL_f0aca2e3bcac4d3fa2457be7c0a56e42",
            "value": "config.json: 100%"
          }
        },
        "c89a0c4e825044b4ae203fc9a80a53dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9baeaf3748fd4b4ba0e9ea57cac715b5",
            "max": 1256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31d44aafde664f2e9a2d9fb16c27d041",
            "value": 1256
          }
        },
        "942ff6e9243d4167b7fe3852d4fc2579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51db2fb3e8a04a6484a5e61a23a55d2e",
            "placeholder": "​",
            "style": "IPY_MODEL_53f0deea33134b0f923953e0d0ca3563",
            "value": " 1.26k/1.26k [00:00&lt;00:00, 90.2kB/s]"
          }
        },
        "fcec48380c0348de9afee360459afa86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ada7089cd9f2448898fd7abcdc105502": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0aca2e3bcac4d3fa2457be7c0a56e42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9baeaf3748fd4b4ba0e9ea57cac715b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31d44aafde664f2e9a2d9fb16c27d041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51db2fb3e8a04a6484a5e61a23a55d2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53f0deea33134b0f923953e0d0ca3563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6efc2ad100cd4cf8b166c1f7d005485f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_353e518acf444b68bea4e58402814a32",
              "IPY_MODEL_b21f767a54e44f289ed1001845072917",
              "IPY_MODEL_43175d63a2484f08a2533b8005fd6d48"
            ],
            "layout": "IPY_MODEL_9969f9be3e6e4cb88ee98be2e006f071"
          }
        },
        "353e518acf444b68bea4e58402814a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da4d06d649044baf9ef7a957c6f64a6b",
            "placeholder": "​",
            "style": "IPY_MODEL_b761a7c2b7314d73982f5f259fc5b5bf",
            "value": "model.safetensors: 100%"
          }
        },
        "b21f767a54e44f289ed1001845072917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d33a0faec79540c9b2dbe16dad528eb9",
            "max": 1617824864,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9c5c7052a7043619682b8d6734106b6",
            "value": 1617824864
          }
        },
        "43175d63a2484f08a2533b8005fd6d48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_896b93cc77414809b74fea1c2987fd68",
            "placeholder": "​",
            "style": "IPY_MODEL_175db21f80f44964a1e607923523a627",
            "value": " 1.62G/1.62G [00:17&lt;00:00, 170MB/s]"
          }
        },
        "9969f9be3e6e4cb88ee98be2e006f071": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da4d06d649044baf9ef7a957c6f64a6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b761a7c2b7314d73982f5f259fc5b5bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d33a0faec79540c9b2dbe16dad528eb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9c5c7052a7043619682b8d6734106b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "896b93cc77414809b74fea1c2987fd68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "175db21f80f44964a1e607923523a627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d57141da18bd431287496011cac0f594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5350d457f45c45d49cb3b2fcb0ea7d2c",
              "IPY_MODEL_56cc169f853b48629300a42d32bf45de",
              "IPY_MODEL_6d1ad87d884142f88b750943d7c950e8"
            ],
            "layout": "IPY_MODEL_8e4e8bb2c01c473790d5dd26122a8a51"
          }
        },
        "5350d457f45c45d49cb3b2fcb0ea7d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc4ff99111b44642ba8e273c7e07c56a",
            "placeholder": "​",
            "style": "IPY_MODEL_ac498d9f63964290b0a1dc582cd72ee8",
            "value": "generation_config.json: 100%"
          }
        },
        "56cc169f853b48629300a42d32bf45de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d66e7ee7a094696b555560186d57540",
            "max": 3772,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d28618c6178044548d95fe400735b681",
            "value": 3772
          }
        },
        "6d1ad87d884142f88b750943d7c950e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_528d2ac1aeda43a2877683edf2ab8dad",
            "placeholder": "​",
            "style": "IPY_MODEL_8d97b6efb0c04545be498e18abde0f19",
            "value": " 3.77k/3.77k [00:00&lt;00:00, 40.3kB/s]"
          }
        },
        "8e4e8bb2c01c473790d5dd26122a8a51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc4ff99111b44642ba8e273c7e07c56a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac498d9f63964290b0a1dc582cd72ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d66e7ee7a094696b555560186d57540": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d28618c6178044548d95fe400735b681": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "528d2ac1aeda43a2877683edf2ab8dad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d97b6efb0c04545be498e18abde0f19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f35196cdfa9240cab00cc051121f63ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a10894ad2784262b995b683d5dfff1b",
              "IPY_MODEL_fb04c09339944a9681fc1e66a3bf1e5d",
              "IPY_MODEL_c6a2a04a3992423899a34b15760b1a76"
            ],
            "layout": "IPY_MODEL_d8f700ababbc4101999d2451db3574b9"
          }
        },
        "9a10894ad2784262b995b683d5dfff1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47a79d62ad9b410593686e8f24e8db22",
            "placeholder": "​",
            "style": "IPY_MODEL_c5774a309f42468d8525cae432083fda",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "fb04c09339944a9681fc1e66a3bf1e5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f0e0ce5747c4e99b4945bd8c1a706df",
            "max": 282843,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_427022283abb4d768e34018fc8c3d313",
            "value": 282843
          }
        },
        "c6a2a04a3992423899a34b15760b1a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f26291f0b9ec44ac9b3da7d31c56cc47",
            "placeholder": "​",
            "style": "IPY_MODEL_c18f4dd879e74dadb5a02ab58289bbac",
            "value": " 283k/283k [00:00&lt;00:00, 1.18MB/s]"
          }
        },
        "d8f700ababbc4101999d2451db3574b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47a79d62ad9b410593686e8f24e8db22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5774a309f42468d8525cae432083fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f0e0ce5747c4e99b4945bd8c1a706df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "427022283abb4d768e34018fc8c3d313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f26291f0b9ec44ac9b3da7d31c56cc47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c18f4dd879e74dadb5a02ab58289bbac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d012e3f6a444cf9be2fab5dacd0da4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff8b9b472cd74bbaa806827ae9c1c1f6",
              "IPY_MODEL_3a37c497f2ef4ff2bcc60c52225cbcc0",
              "IPY_MODEL_c95dd25317df442393b3b158bdbeeba3"
            ],
            "layout": "IPY_MODEL_9fc51430c56e47a7b70ef48ec4414273"
          }
        },
        "ff8b9b472cd74bbaa806827ae9c1c1f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6247ad8dc1a54abaa992c339a60895cc",
            "placeholder": "​",
            "style": "IPY_MODEL_44a2805ad3524a179fc85194d361cc1d",
            "value": "vocab.json: 100%"
          }
        },
        "3a37c497f2ef4ff2bcc60c52225cbcc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa8f24da287647c68f415ef74015730e",
            "max": 1036558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ac37e93bcf34eef9777fc2e4ecc9def",
            "value": 1036558
          }
        },
        "c95dd25317df442393b3b158bdbeeba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d77024e88f54f3990ed1003fca4c6ba",
            "placeholder": "​",
            "style": "IPY_MODEL_8793c63b6dbf45aba65a6658e25d55db",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 3.15MB/s]"
          }
        },
        "9fc51430c56e47a7b70ef48ec4414273": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6247ad8dc1a54abaa992c339a60895cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44a2805ad3524a179fc85194d361cc1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa8f24da287647c68f415ef74015730e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ac37e93bcf34eef9777fc2e4ecc9def": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d77024e88f54f3990ed1003fca4c6ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8793c63b6dbf45aba65a6658e25d55db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a8f6fe778a441a499de1b16687e6071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eed13ec24fcc475cb9810056cbd1ec8f",
              "IPY_MODEL_197bbf87d2c84a899240f93a8b5c1593",
              "IPY_MODEL_f542670e50cf478fb612731494ebbd2b"
            ],
            "layout": "IPY_MODEL_df040d98077249bcad4e3eec38f9c0e4"
          }
        },
        "eed13ec24fcc475cb9810056cbd1ec8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39579240c11d4e1ba6aaa7cb8d7d8ac0",
            "placeholder": "​",
            "style": "IPY_MODEL_2b49a40f6c9241d4add62b4dd51acb28",
            "value": "tokenizer.json: 100%"
          }
        },
        "197bbf87d2c84a899240f93a8b5c1593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e54f5a57aab4ce89c0c921fe2eecb2a",
            "max": 2710337,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ca2316240084369b7df2d138509cc60",
            "value": 2710337
          }
        },
        "f542670e50cf478fb612731494ebbd2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33b6814424594de7af965fc98f79521c",
            "placeholder": "​",
            "style": "IPY_MODEL_c90de97dd17c41cd8c40621c00accb81",
            "value": " 2.71M/2.71M [00:00&lt;00:00, 5.65MB/s]"
          }
        },
        "df040d98077249bcad4e3eec38f9c0e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39579240c11d4e1ba6aaa7cb8d7d8ac0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b49a40f6c9241d4add62b4dd51acb28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e54f5a57aab4ce89c0c921fe2eecb2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ca2316240084369b7df2d138509cc60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33b6814424594de7af965fc98f79521c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c90de97dd17c41cd8c40621c00accb81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1db00d67ec44b65a808ca58a8de97c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d24c59255ab4a02a19289ab3e77182a",
              "IPY_MODEL_5e39e81104544d7c9eb9af24e850f4fe",
              "IPY_MODEL_d327d4f9034f459694b343e6c1c341e4"
            ],
            "layout": "IPY_MODEL_2ffc1f22979d44eb851359da50bc1ee4"
          }
        },
        "2d24c59255ab4a02a19289ab3e77182a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeacbe00b59a4041bd9bd12eb9ad1c9f",
            "placeholder": "​",
            "style": "IPY_MODEL_6cec96dfa9c142c39c4ade1c34db63a6",
            "value": "merges.txt: 100%"
          }
        },
        "5e39e81104544d7c9eb9af24e850f4fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18a73556bd3f49c9b30d0d0e637a3892",
            "max": 493869,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bc699f52c46456d8b161a3a314912e5",
            "value": 493869
          }
        },
        "d327d4f9034f459694b343e6c1c341e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f2b560644fb40c68739980008a9f09b",
            "placeholder": "​",
            "style": "IPY_MODEL_aab280c13da0439ea7d07bb0a4a40479",
            "value": " 494k/494k [00:00&lt;00:00, 2.05MB/s]"
          }
        },
        "2ffc1f22979d44eb851359da50bc1ee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeacbe00b59a4041bd9bd12eb9ad1c9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cec96dfa9c142c39c4ade1c34db63a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18a73556bd3f49c9b30d0d0e637a3892": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bc699f52c46456d8b161a3a314912e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f2b560644fb40c68739980008a9f09b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aab280c13da0439ea7d07bb0a4a40479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60865f6eb9234bc2aef778673ad0890c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d696c14f2d345afa90b08464b0020c3",
              "IPY_MODEL_fbb81fe38e4b48dbaece3a1358f53ab4",
              "IPY_MODEL_f8836c3fc32c4797b9e51858a48d4b7f"
            ],
            "layout": "IPY_MODEL_cf7b34538879440f9eca664b8492b665"
          }
        },
        "0d696c14f2d345afa90b08464b0020c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1abf0259d9334d9caffb5977d20709fc",
            "placeholder": "​",
            "style": "IPY_MODEL_fc653070fc6c45be9870a735911afe6a",
            "value": "normalizer.json: 100%"
          }
        },
        "fbb81fe38e4b48dbaece3a1358f53ab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2910a889111479a8a71009c9a9b9a06",
            "max": 52666,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d69d9aec50b448c4998b50886202d492",
            "value": 52666
          }
        },
        "f8836c3fc32c4797b9e51858a48d4b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7028d13b14434bb19e40b2118ee2639c",
            "placeholder": "​",
            "style": "IPY_MODEL_5dd7fb037e9844eda2dbbf26ae4e4a7f",
            "value": " 52.7k/52.7k [00:00&lt;00:00, 2.97MB/s]"
          }
        },
        "cf7b34538879440f9eca664b8492b665": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1abf0259d9334d9caffb5977d20709fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc653070fc6c45be9870a735911afe6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2910a889111479a8a71009c9a9b9a06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d69d9aec50b448c4998b50886202d492": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7028d13b14434bb19e40b2118ee2639c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dd7fb037e9844eda2dbbf26ae4e4a7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e34267770f7542cab2f4b5dbdbd03479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e8965c21b4e4c2f90c7bd7ddfdef1ec",
              "IPY_MODEL_e50acd01e4ac4657abed1f231efa5dec",
              "IPY_MODEL_1e15ad38bd044c4db3f16ced749863e7"
            ],
            "layout": "IPY_MODEL_10ea929382d148df83224dba77f77c22"
          }
        },
        "1e8965c21b4e4c2f90c7bd7ddfdef1ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7859442f1b14d57b0fdda17d9c50970",
            "placeholder": "​",
            "style": "IPY_MODEL_5d75dd4d353c426f925ab7700e57e3fe",
            "value": "added_tokens.json: 100%"
          }
        },
        "e50acd01e4ac4657abed1f231efa5dec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4ca45976090464abab5f44091e9f25f",
            "max": 34648,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4ff9cfd6b4d47e0986fcc499e60b868",
            "value": 34648
          }
        },
        "1e15ad38bd044c4db3f16ced749863e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b317dcbfa7746cdb7096ef710627587",
            "placeholder": "​",
            "style": "IPY_MODEL_c326a54cd41e46899f26b5f7cfe0a998",
            "value": " 34.6k/34.6k [00:00&lt;00:00, 1.36MB/s]"
          }
        },
        "10ea929382d148df83224dba77f77c22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7859442f1b14d57b0fdda17d9c50970": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d75dd4d353c426f925ab7700e57e3fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4ca45976090464abab5f44091e9f25f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4ff9cfd6b4d47e0986fcc499e60b868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b317dcbfa7746cdb7096ef710627587": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c326a54cd41e46899f26b5f7cfe0a998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01be323d2468483e934ab1ef5eacec8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6250fa324d3c4270a48ae9e33be1962d",
              "IPY_MODEL_dc71a180e33b4063a67b058e87c95430",
              "IPY_MODEL_ddad3db08e964d6993951424ebb4dd8d"
            ],
            "layout": "IPY_MODEL_e5ca2b5b2c154d828e0bb98d4cb65f07"
          }
        },
        "6250fa324d3c4270a48ae9e33be1962d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d561b95f90314f479d6466c6bce0815f",
            "placeholder": "​",
            "style": "IPY_MODEL_ec5a4871c4d84e149103bb084a1efa8e",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "dc71a180e33b4063a67b058e87c95430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_550fcaff792c47a7aeba3cb8d943d62f",
            "max": 2186,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e69280c1f5d4a74bf30e3b6a85935e5",
            "value": 2186
          }
        },
        "ddad3db08e964d6993951424ebb4dd8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5223bd2d00d4e3a90df73cda88f6122",
            "placeholder": "​",
            "style": "IPY_MODEL_882e32faa6254145843688b766f59a92",
            "value": " 2.19k/2.19k [00:00&lt;00:00, 115kB/s]"
          }
        },
        "e5ca2b5b2c154d828e0bb98d4cb65f07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d561b95f90314f479d6466c6bce0815f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec5a4871c4d84e149103bb084a1efa8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "550fcaff792c47a7aeba3cb8d943d62f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e69280c1f5d4a74bf30e3b6a85935e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5223bd2d00d4e3a90df73cda88f6122": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "882e32faa6254145843688b766f59a92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2f7032cc26642f99d5703c7af0885a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe4e5db6164c4798b9058032a637fe30",
              "IPY_MODEL_1dfa0124e57a4b509603d3f1a3db8d22",
              "IPY_MODEL_17a3de8da13d4c3a97dfefb557aca01a"
            ],
            "layout": "IPY_MODEL_164720f326a141308cbfbce4d9749843"
          }
        },
        "fe4e5db6164c4798b9058032a637fe30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_702fbac044784218816ca5bd71e68649",
            "placeholder": "​",
            "style": "IPY_MODEL_762194d3e84e435e9fce535ae7b48df3",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "1dfa0124e57a4b509603d3f1a3db8d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da3a1718dbca478cb20f7d2f8ae73184",
            "max": 340,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb228c616ac04dbeb8d907501e4bc672",
            "value": 340
          }
        },
        "17a3de8da13d4c3a97dfefb557aca01a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e516f4e25e94f52bb0b666e2ad7ca89",
            "placeholder": "​",
            "style": "IPY_MODEL_4f67eaf6d2694e2a93425f9b1481aca9",
            "value": " 340/340 [00:00&lt;00:00, 22.2kB/s]"
          }
        },
        "164720f326a141308cbfbce4d9749843": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "702fbac044784218816ca5bd71e68649": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "762194d3e84e435e9fce535ae7b48df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da3a1718dbca478cb20f7d2f8ae73184": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb228c616ac04dbeb8d907501e4bc672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e516f4e25e94f52bb0b666e2ad7ca89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f67eaf6d2694e2a93425f9b1481aca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LuSYSs26W8om",
        "outputId": "01126ea6-5dd3-4e35-cfc6-5c8e7fe62299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-of7nlvvu\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-of7nlvvu\n",
            "  Resolved https://github.com/huggingface/transformers to commit 074aa3b3fd340c697632c2188ee2a4561df71b76\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (2.32.3)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.46.0.dev0)\n",
            "  Downloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (4.66.5)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0 (from gradio)\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.31.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<1.0->gradio)\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading ruff-0.6.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.31.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.46.0.dev0-py3-none-any.whl size=9914790 sha256=4898f6cf7224a7d9b31f86ef787f4e6fdf48447aef7743cc7c4674dad6d874a2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s9qm3aan/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
            "Successfully built transformers\n",
            "Installing collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, h11, ffmpy, aiofiles, uvicorn, starlette, httpcore, tokenizers, httpx, fastapi, transformers, gradio-client, gradio\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.0 ffmpy-0.4.0 gradio-4.44.1 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.12 ruff-0.6.8 semantic-version-2.10.0 starlette-0.38.6 tokenizers-0.20.0 tomlkit-0.12.0 transformers-4.46.0.dev0 uvicorn-0.31.0 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "ZNhEHB0OXAas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"automatic-speech-recognition\",\n",
        "               \"openai/whisper-large-v3-turbo\",\n",
        "               torch_dtype=torch.float16,\n",
        "               device=\"cuda:0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478,
          "referenced_widgets": [
            "06d4e21dfa0740febe31d787e1b8b921",
            "92c0910ec234458f8120d0e59de237b0",
            "c89a0c4e825044b4ae203fc9a80a53dd",
            "942ff6e9243d4167b7fe3852d4fc2579",
            "fcec48380c0348de9afee360459afa86",
            "ada7089cd9f2448898fd7abcdc105502",
            "f0aca2e3bcac4d3fa2457be7c0a56e42",
            "9baeaf3748fd4b4ba0e9ea57cac715b5",
            "31d44aafde664f2e9a2d9fb16c27d041",
            "51db2fb3e8a04a6484a5e61a23a55d2e",
            "53f0deea33134b0f923953e0d0ca3563",
            "6efc2ad100cd4cf8b166c1f7d005485f",
            "353e518acf444b68bea4e58402814a32",
            "b21f767a54e44f289ed1001845072917",
            "43175d63a2484f08a2533b8005fd6d48",
            "9969f9be3e6e4cb88ee98be2e006f071",
            "da4d06d649044baf9ef7a957c6f64a6b",
            "b761a7c2b7314d73982f5f259fc5b5bf",
            "d33a0faec79540c9b2dbe16dad528eb9",
            "a9c5c7052a7043619682b8d6734106b6",
            "896b93cc77414809b74fea1c2987fd68",
            "175db21f80f44964a1e607923523a627",
            "d57141da18bd431287496011cac0f594",
            "5350d457f45c45d49cb3b2fcb0ea7d2c",
            "56cc169f853b48629300a42d32bf45de",
            "6d1ad87d884142f88b750943d7c950e8",
            "8e4e8bb2c01c473790d5dd26122a8a51",
            "fc4ff99111b44642ba8e273c7e07c56a",
            "ac498d9f63964290b0a1dc582cd72ee8",
            "7d66e7ee7a094696b555560186d57540",
            "d28618c6178044548d95fe400735b681",
            "528d2ac1aeda43a2877683edf2ab8dad",
            "8d97b6efb0c04545be498e18abde0f19",
            "f35196cdfa9240cab00cc051121f63ad",
            "9a10894ad2784262b995b683d5dfff1b",
            "fb04c09339944a9681fc1e66a3bf1e5d",
            "c6a2a04a3992423899a34b15760b1a76",
            "d8f700ababbc4101999d2451db3574b9",
            "47a79d62ad9b410593686e8f24e8db22",
            "c5774a309f42468d8525cae432083fda",
            "3f0e0ce5747c4e99b4945bd8c1a706df",
            "427022283abb4d768e34018fc8c3d313",
            "f26291f0b9ec44ac9b3da7d31c56cc47",
            "c18f4dd879e74dadb5a02ab58289bbac",
            "3d012e3f6a444cf9be2fab5dacd0da4b",
            "ff8b9b472cd74bbaa806827ae9c1c1f6",
            "3a37c497f2ef4ff2bcc60c52225cbcc0",
            "c95dd25317df442393b3b158bdbeeba3",
            "9fc51430c56e47a7b70ef48ec4414273",
            "6247ad8dc1a54abaa992c339a60895cc",
            "44a2805ad3524a179fc85194d361cc1d",
            "fa8f24da287647c68f415ef74015730e",
            "1ac37e93bcf34eef9777fc2e4ecc9def",
            "3d77024e88f54f3990ed1003fca4c6ba",
            "8793c63b6dbf45aba65a6658e25d55db",
            "4a8f6fe778a441a499de1b16687e6071",
            "eed13ec24fcc475cb9810056cbd1ec8f",
            "197bbf87d2c84a899240f93a8b5c1593",
            "f542670e50cf478fb612731494ebbd2b",
            "df040d98077249bcad4e3eec38f9c0e4",
            "39579240c11d4e1ba6aaa7cb8d7d8ac0",
            "2b49a40f6c9241d4add62b4dd51acb28",
            "5e54f5a57aab4ce89c0c921fe2eecb2a",
            "2ca2316240084369b7df2d138509cc60",
            "33b6814424594de7af965fc98f79521c",
            "c90de97dd17c41cd8c40621c00accb81",
            "f1db00d67ec44b65a808ca58a8de97c9",
            "2d24c59255ab4a02a19289ab3e77182a",
            "5e39e81104544d7c9eb9af24e850f4fe",
            "d327d4f9034f459694b343e6c1c341e4",
            "2ffc1f22979d44eb851359da50bc1ee4",
            "eeacbe00b59a4041bd9bd12eb9ad1c9f",
            "6cec96dfa9c142c39c4ade1c34db63a6",
            "18a73556bd3f49c9b30d0d0e637a3892",
            "0bc699f52c46456d8b161a3a314912e5",
            "4f2b560644fb40c68739980008a9f09b",
            "aab280c13da0439ea7d07bb0a4a40479",
            "60865f6eb9234bc2aef778673ad0890c",
            "0d696c14f2d345afa90b08464b0020c3",
            "fbb81fe38e4b48dbaece3a1358f53ab4",
            "f8836c3fc32c4797b9e51858a48d4b7f",
            "cf7b34538879440f9eca664b8492b665",
            "1abf0259d9334d9caffb5977d20709fc",
            "fc653070fc6c45be9870a735911afe6a",
            "d2910a889111479a8a71009c9a9b9a06",
            "d69d9aec50b448c4998b50886202d492",
            "7028d13b14434bb19e40b2118ee2639c",
            "5dd7fb037e9844eda2dbbf26ae4e4a7f",
            "e34267770f7542cab2f4b5dbdbd03479",
            "1e8965c21b4e4c2f90c7bd7ddfdef1ec",
            "e50acd01e4ac4657abed1f231efa5dec",
            "1e15ad38bd044c4db3f16ced749863e7",
            "10ea929382d148df83224dba77f77c22",
            "d7859442f1b14d57b0fdda17d9c50970",
            "5d75dd4d353c426f925ab7700e57e3fe",
            "b4ca45976090464abab5f44091e9f25f",
            "d4ff9cfd6b4d47e0986fcc499e60b868",
            "5b317dcbfa7746cdb7096ef710627587",
            "c326a54cd41e46899f26b5f7cfe0a998",
            "01be323d2468483e934ab1ef5eacec8b",
            "6250fa324d3c4270a48ae9e33be1962d",
            "dc71a180e33b4063a67b058e87c95430",
            "ddad3db08e964d6993951424ebb4dd8d",
            "e5ca2b5b2c154d828e0bb98d4cb65f07",
            "d561b95f90314f479d6466c6bce0815f",
            "ec5a4871c4d84e149103bb084a1efa8e",
            "550fcaff792c47a7aeba3cb8d943d62f",
            "1e69280c1f5d4a74bf30e3b6a85935e5",
            "d5223bd2d00d4e3a90df73cda88f6122",
            "882e32faa6254145843688b766f59a92",
            "a2f7032cc26642f99d5703c7af0885a9",
            "fe4e5db6164c4798b9058032a637fe30",
            "1dfa0124e57a4b509603d3f1a3db8d22",
            "17a3de8da13d4c3a97dfefb557aca01a",
            "164720f326a141308cbfbce4d9749843",
            "702fbac044784218816ca5bd71e68649",
            "762194d3e84e435e9fce535ae7b48df3",
            "da3a1718dbca478cb20f7d2f8ae73184",
            "eb228c616ac04dbeb8d907501e4bc672",
            "3e516f4e25e94f52bb0b666e2ad7ca89",
            "4f67eaf6d2694e2a93425f9b1481aca9"
          ]
        },
        "collapsed": true,
        "id": "DB9dvh8YXDsX",
        "outputId": "17395eb5-d036-497d-ff1f-c8e143dbf28e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.26k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06d4e21dfa0740febe31d787e1b8b921"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.62G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6efc2ad100cd4cf8b166c1f7d005485f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/3.77k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d57141da18bd431287496011cac0f594"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f35196cdfa9240cab00cc051121f63ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d012e3f6a444cf9be2fab5dacd0da4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.71M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a8f6fe778a441a499de1b16687e6071"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1db00d67ec44b65a808ca58a8de97c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60865f6eb9234bc2aef778673ad0890c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e34267770f7542cab2f4b5dbdbd03479"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01be323d2468483e934ab1ef5eacec8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2f7032cc26642f99d5703c7af0885a9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable return_timestamps for long-form generation\n",
        "result = pipe(\"/content/audio.mp3\", return_timestamps=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYT-HDxMZjZA",
        "outputId": "2a9e5781-2a9b-43b9-adbf-5eba39d1c986"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BURaj5QKdbvp",
        "outputId": "238975ab-6f69-443f-da7b-eb78fbf2e219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \" One of the most effective strategies to improve the performance of your language model applications is to split your large data into smaller chunks. The goal is to give the language model only the information that it needs for your task and nothing more. This practice is the art and science of text splitting. It is one of the first and most foundational decisions a language model practitioner will need to make. Text splitting takes a minute to learn, but in this video, you're going to learn the five levels of text splitting that squeeze out more performance from from your language model applications using the same data that you already have. Now, there's something for everyone in this video. For the beginners, we're gonna start from the very basics. And for the advanced devs, I'm gonna give you plenty that you're gonna wanna argue with me on. But either way, I guarantee you're gonna learn something along the way. This is gonna be a longer video and we're gonna cover a lot, but that's on purpose. I wanna take our time and I guarantee that if you make it to the end, you're gonna have a solid grasp on chunking theory, strategies, and resources to go learn more. For those that are just joining us, my name is Greg and I'm exploring the AI space through the lens of business value. You see, models are cool, stats are cool, but I wanna find out how businesses will actually be taking advantage of AI and language models. This video will be split up into six different sections. First, we're gonna talk about theory. We'll talk about what splitting and chunking are, why we need them, and why they're important. I even made a cool tool called chunkviz.com to help us visualize along the way. Then we're gonna jump into the five levels of text splitting. For each level, we're gonna progressively get more complex and introduce topics along the way for you to consider when you're building your own language model applications. For level one, we're gonna talk about character splitting. This is when you split your documents by a static character limit. For level two, we're gonna talk about recursive character text splitting. This is when you start with your long document and then recursively go through it and split it by a different list of separators. For level three, we're going to talk about document specific text splitting. So if you have Python docs or JavaScript docs, or maybe PDFs with images, we're going to include multimodal in this level as well. For level four, this is where it gets interesting. We're going to talk about semantic splitting. So the first three levels were all naive ways of splitting. These levels focused on the physical positioning and structure of the text chunks. These first three levels, it's a bit like sorting a library based off the book sizes and shelf space, rather than the actual content of the books. But in level four here, we're not just gonna look at where the text sits or its structure. Instead, we're gonna start to delve into the what and the why of the text, the actual meaning and context of these chunks. It's like understanding and categorizing the books by their genre and themes instead. And then with level five, we're gonna talk about agentic splitting. So we're gonna look at an experimental method where you actually build an agent-like system that's going to review our text and split it for us. And then to finish it off, we're going to end with some dessert. A bonus level that shows the advanced tactics that start to creep a little beyond text splitting, but are going to be important for your overall knowledge about how to do retrieval in general. My goal isn't to prescribe the best or most powerful method. You'll see why that's actually not possible. My goal is to expose you to the different strategies and considerations of splitting, your own data, so you're able to make a more informed decision when you're building. This is part of a larger series on retrieval I have going. And if you want to check out more or get the code for this content, head over to fullstackretrieval.com and I can go send it to you. Lastly, I do a lot of workshops with individuals and teams. If you, your team, or your company want to chat live or do a custom workshop, just feel free to reach out. So without further ado, let's jump into it. First, we're going to start off with a theory behind text splitting. What is it and why do we even need to do it in the first place? You see, well, applications are better when you give it your own data or maybe your user's data, but you can't pass unlimited data to your language model. And there's two main reasons for this. Number one, applications have a context limit. This is an upper bound on the amount of data that you can actually give to a language model. You can see the context windows on OpenAI's websites for their own models. And number two, language models do better when you increase the signal to noise ratio. Let's see what Anton, co-founder of Chroma, has to say about this. distracting information in the model's context window does tend to measurably destroy the performance of the overall application. So instead of giving your language model the kitchen sink and hoping the language model can figure it out, you want to prune the fluff from your data whenever possible. Now text splitting or chunking is the process of splitting your data into smaller pieces so you can make it optimal for your task and your language model. Now I really want to emphasize this point. The whole goal of splitting your text is to best prepare it for the task that you actually have at hand. So rather than starting with, Hey, how should I chunk my data? Your question should really be, what's the optimal way for me to pass the data that my language model needs for my task. Our goal is not to chunk just for chunking sake. Our goal is to get the data in a format where it can be retrieved for value later. So let's talk about retrieval in general. So in the bigger picture, the act of gathering the right information for your language models is called retrieval. This is the orchestration of tools and techniques to surface up what your language model actually needs to complete its task. Let's take a look at where chunking fits into the retrieval process. So here we're taking a look at the full stack retrieval process. We have everything from your raw data sources to your response here. If you want an overview about this entire process, head over to fullstackretrieval.com where I do a separate tutorial on this. Now, the important part is we're all going to have our raw data sources down at the bottom here, and they eventually need to make it into our knowledge base, right? However, we can't just put our raw data sources, we're going to need to chunk them, which is what this video is about. Now, right when you do your data loading, this is where your chunking strategy is going to come into play. How you choose to split up your documents is a very important decision. As you go through this, you'll see that there isn't one right way to do your chunking strategy, or really your retrieval strategy for that matter. For example, take a look at this tweet from Robert Hamir. For those that need a translation, Robert is basically saying that he employs many alternative strategies across his retrieval stack. What works for him may not work for you. The last thing I'll comment on is the topic of evaluations. Evaluations are super important when you're developing your language model applications. You won't know if your performance is improving without rigorous testing. One of the most popular retrieval evaluation frameworks out there is Ragus. I encourage you to go check it out. I won't be covering those today because it's more of a retrieval topic rather than this narrow niche that we're going to be covering today. Plus, they're very domain-specific and application-specific. If you want my taken evals, please head over to fullstackretrieval.com and you'll get a notice when I start to cover it. All right, now that is enough talking for now. I finally want to get into some code. Let's move on to level one character split. All right, so level one character splitting. Before we jump into that, I want to talk about the chunking commandment. Your goal is not to chunk for chunking's sake. Your goal is to get our data in a format where it can be retrieved for value later. I'm placing so much emphasis on this point because it doesn't matter what your chunking strategy is if it doesn't serve your downstream task. Keep that in mind as we keep going here. So level one character splitting. This is the most basic form of splitting and this is when you're gonna chunk up your text by a fixed static character length. Let's talk about what that means. First the pros it's extremely simple and easy. The cons it's very rigid and doesn't take into account the structure of your text. and to be honest, I don't know anybody that does this in production. I don't. Let me know if you do because I'm curious. The two concepts I want to talk about for this one, we're going to see what chunk size is and we're going to talk about what chunk overlap is, but let's use examples to explain those. For our text, this is the text I would like to chunk up. It is an example text for this exercise. Cool, we got that one. So before we talk about packages that help us do this automatically, I want to show you how we do this manually first just so you can appreciate the nuances for how cool some of this stuff is. So in order to create our chunks, I'm going to first create an empty list of chunks. Now my chunk size is going to be 35. This stands for 35 characters. So I'm going to count 35 characters in, count that as chunk one. Next 35 characters is chunk two. I'm going to run through this. I'm going to create a range and the range length is going to be the length of my text that I have up above here. And then the iteration step or the step we're going to take is the chunk size. So we're going to skip ahead every 35 characters. I'm going to get the chunk. I'm going to unpend it and let's see what our chunks are. This is the text I would like to unk up. It is an example text for this exercise. First off, congratulations. You just did your first chunking exercise. Do you feel like a language model practitioner yet? I sure do. Let's keep on going here. There's a couple of problems with this. This is the text I would like to chuck, well, it's stuck in the middle of the word. That's no good. How are we supposed to know how long this 35 character length? We could change this to 40. And then all of a sudden, this goes a little bit further. But dang, again, we have it one more time. That's giving us a hard time. That's not too good. So yes, it's quick. Yes, it's simple. But we need to fix this problem. Now, before we move on to level two, I want to talk about Langchain's character splitter. So their character splitter, that's going to do the exact same thing for us. But it's going to be through a Langchain one-liner. And the way that you do that is you're going to initialize a character text splitter. You're going to tell it how much of a chunk size you want. You're going to tell it how much of a chunk overlap you want. We'll talk about that in a second. And when you do a blank or an empty string as a separator, we'll talk about that in a second too, that means it's just going to split by character. And Langchain by default, they will strip the white space. And so they'll remove the spaces on the end of your chunks. I don't want them to do that quite yet. So I'm going to say false. So this is, we just made our character splitter. Now we're actually going to go and split the documents. And so I'm going to say dot create documents and this create documents function, it expects a list. And because our string up above is just a plain old string, I need to wrap it in a list right here. Let's go do that text splitter. So now what we get returned is we get three chunks. However, they look a little bit different than plain strings. The reason why is because they're actually a document object. Now in Langchain, a document object is, well, an object that holds a string, but it can also hold metadata, which is important for us to understand when we start doing more advanced techniques. So don't get scared. Documents still have our string and they're held within page content. This is the text I would like to chunk. It's the same thing that we had up above. Cool. That makes sense. So let's talk about overlaps and separators here. So I'm going to make the character splitter again. It's going to be 35 characters, but this time we're going to have a chunk overlap of four. Now what this means is that the tail end of chunk number one is going to overlap a little bit with the head or the beginning of chunk number two. And the overlap is going to be four characters. So the last four characters of chunk one will be the same four characters of chunk two. Let's see what this looks like here. And again, I'm just going to make these. And what we have is this is the chunk or this is the text that I would like to, we still have the first same split, the chunk size is the same, but check this out. Now the first four characters of chunk number two are the same four characters as chunk number one because we have that chunk overlap. All right. Now, when I was getting ready for this exercise, I thought I remember a tool that actually visually showed you different chunking techniques with highlights of the different chunks, but I couldn't find it. So I ended up making a tool and that tool is called chunkviz.com. And this is just a quick snippet of it, but I want to show you this while we're on the topic. So chunk number one was this first beginning part, and then we have the overlap, and then we have chunk number two, and then the overlap, and then chunk number three. But if you want to try this out for yourself, it's kind of cool. You can go to chunkviz.com, and then what you'll get here is you'll get a tool where you can input different text and play with your different chunk sizes that we'd get out. So let's go back up and let's grab the text that we had. I'm going to bring this over here. I'm just going to replace this. And so you can see here that right now our chunk size is one with no overlap. That doesn't make any sense. Although it's visually cool, it won't do any good for us because we have 83 chunks here. What are you going to do with 83 one-character chunks? I don't know. I'm not going to do much with that. But as you start to increase this number, you can see that these different chunk sizes are going to start to get bigger. So we're going to take this all the way up, and as we go through it, I'm going to put it at what we had it before, which is 35. So this is a text that I would like to, and you see it ends right in the middle of the CH, just like we had beforehand. Let me zoom in just a little bit more here. It ends just in the CH that we had beforehand. Now, if I start to introduce overlap, you can see that now we have a little bit of an overlapping section. So we had chunk over size or chunk overlap before. This is the text that I would like to. So with chunk one still ends here, but now there's the overlap that comes with it. So what's cool is you can switch this around yourself. You see above a certain mark, it ends up being, I wanna get rid of this overlap. Over a certain mark, well, this chunk size is bigger than the document that we have. So it's just gonna encapsulate everything, but you can go to chunkviz.com and go play around with this. We'll take a look at one more of these sections in a minute here. Cool, so let's go down there. Those are characters, those are separators, fabulous. And so the next thing I wanna talk about is separator. So beforehand, we just had a blank string as a separator, which means you're gonna split by character, right? However, if we specify another separator, in this case, I'm gonna do ch, well, let's see what that does here. This is the text that I would like to, And you see here that the CH is missing and the space is missing because I removed the strip white space and so it's default true. So that's gone. And you see that this word is supposed to be chunk up, but it's not anymore because we removed the CH. It's not too helpful when we do CH. You could do the letter E if you wanted and it's gonna be a little bit different. Either way, unless you know exactly what you're doing, I wouldn't suggest messing around with the separator to try to get better results here. All right, so that's the Lang chain side of the house. The next one I wanna show you is the Lama index side of the house. So they have what they call a sentence splitter, but also I'm gonna use their simple directory reader because this time, instead of just using a static string that I put in the code, we're actually gonna load some essays from a directory. So I'm gonna make the sentence splitter, and this time I'm gonna have a chunk size of 200 and a chunk overlap of 15. And then I'm gonna load up some essays. So my input files, I'm just gonna load up one essay. This data is also in the repo. So if you go and clone this repo, you can also get this data pretty easily. This is gonna be a Paul Graham essay and this is gonna be his MIT essay. So we can go check out Paul Graham's MIT essay, and we can see what it is right here. You can go and read it. I have it loaded up for you. So let's go ahead and load this. Now we have our documents, but this document is just going to be, let me show you here. Let's check out, let's check how long this document is, and it's just one big long document because the entire essay was loaded into this variable here. However, we wanna chunk it up. And the way we're gonna chunk it up is with our splitter, We're gonna say get nodes from documents. Now you may be asking, hey Greg, wait, what's a node? Well, Llama Index's nomenclature for a chunk or a subsection of a doc is gonna be nodes. And so that's what we're gonna get here, same thing. So now that we have our nodes, I wanna take a look at one. So as you can see, this node is quite long based off of the amount of text that's in here, but there's some really cool information that comes out of the box. So first of all, this node has an ID, and we can see here that it's a text node because they delineate from other nodes. So we have a node ID, so we can go and use it later deal with its uniqueness and then we also have some metadata so we can tell where it came from we can have last modified day etc but then one of the other parts i like a lot is going to be around node relationships so here we have a relationships key and we can take a look at other relationships this node has so node relationship we can see the source node that it came from but then also we can take a look at the next node so what node actually comes next and this is really helpful for when you start doing some traversing across your documents. But either way, I won't go too far into that one. Well, congratulations. We just finished level number one. Let's head off to level number two, recursive character text splitting. So you'll notice that in our previous level one, we split by a static number of chunk sizes each time. So 35 characters by 35 characters. However, there's other chunking mechanisms that will actually look at the physical structure of your text and it will infer what type of chunk sizes you should have. So instead of specifying by 35 characters, you can then specify, well, give me every new line or give me every double new line. And that's what recursive character text splitter does. So what it's going to do is it's actually going to have a series of separators and it's going to recursively go through these documents and it's going to start at its first separator and it's going to first chunk up by every new double new line that you have here for any chunks that are still too large. after that first iteration, it's going to go to its next separator. And that's going to be just new lines. And then it's going to go to spaces. And then it's going to go to characters. So now I don't need to specify 35 characters or 200. All I can do is I can just pass it my text and it will infer what the structure should be. Now, the cool part about this one is if you think about how you write text, you're probably going to separate your ideas by paragraphs. And those paragraphs are separated by double new lines. This method takes advantage of that fact. And so we can start to be smart about which separators that we use to take advantage of how humans naturally write a text. All right. So let's go ahead and let's check this out. We're going to do Langchain TextSplitter. We're going to do the recursive character text splitter. Now, again, I'm going to take some text, but this one's going to be a little bit longer that we have here. Right. And so let's do that text. And then let's put it through our recursive character text splitter. This is going to be a 65 character limit. We're going to pass it through. And then all of a sudden you can see that we get a whole bunch of different chunks here. Now, I'm not even sure how many chunks. Let's see how many we actually have. We have 16 different chunks, all right? So with that, one of the most important things I didn't understand about the for end of sentence, end of sentence day, you can see here that what's cool is that we're ending on words quite often. And that's because words have spaces in between them. And that is one of the separators that we try out. So this is cool. Now we're not splitting in between words anymore. However, we are starting to split in between sentences. That's not so good. That's not so fun. One of the ways that we can combat that is we can increase the chunk size because our hypothesis is that if we increase the chunk size, we can start to take advantage of the paragraph splits a little bit more. All right. So now what I'm going to do is I'm going to increase the chunk size to 450, still chunk overlap of zero. And let's see what we have here. One of the most important things I didn't understand about the world without a child was the degree to which performances are super linear. Cool. So there's a period. Here's a period. And here's the end of it. Now what's interesting is all three of those, let me scroll down to this vis again. This is the same exact string that we have. Those are all three different paragraph breaks. That's pretty interesting. So one of the important things to note here is look how these paragraphs are different lengths. If I use level one in the 35 character split or any character split, I'd start to cut in the middle of them. But now I can get these paragraphs grouped together. and the hypothesis behind this method is that these paragraphs will hold semantically similar information that should be held together so the recursive character text splitter this is pretty awesome let's go take a look at what this looks like at chunkviz.com i'm just going to go ahead and copy this go back to chunkviz.com let me put this text in there and you can see if we did the character splitter of 35 text we have 26 different chunks we're chunking all over the place this doesn't make any sense but what I'm going to do is I'm going to actually scroll down I'm going to select the recursive character text splitter and now I still have a chunk size of 35 but we're going to increase this one so the first thing I want to show you is as I start to increase this notice how I go between 35 and 36 the split the first chunk here doesn't switch sizes that's because it's looking for the space to actually split on and because there's a space here it snaps to the nearest word this is why it's so cool so I'm going to increase this let's see when it finally does split. And there it goes. It just jumped up to a degree. But either way, let me save you for this here. I'm going to select the chunk size. I think we wanted maybe like 450. I forget what it was. Let me zoom out just a little bit. Let's go for maybe four, four. I mean, either way, look at now we're splitting all these three different paragraphs and we can even increase this size and it doesn't really do much for us. But all of a sudden if I go too big, well, it's going to chunk the first two paragraphs together because this 493 is around the size of these two combined. Anyway, you can go and split this again. You can get so big that it finally takes over the third paragraph. All right. So that's it for level two. Congratulations, recursive character text splitter. If I'm starting a project, this is my go-to splitter that I use each time. The ROI for your energy to split up your docs is pretty awesome. It's a one-liner. It goes really quick. There's no extra processing that's needed. So if you're looking for a go-to place to start, I recommend with level two, the recursive character text splitter. Let's move on to level three, document-specific splitting. So up until now, we've been splitting just regular old prose. We've had some static strings and we have had some Paul Graham essays. But what if you have Markdown? What if you have Python docs? What if you have JavaScript docs? There's probably a better way to split on those because we can infer more about the document structure from special characters within those documents. Because when you have code, you start to have some code formatters and we can take advantage of those. All right. So the first one I want to look at here is for markdown. So we're still going to have something that's like the recursive character text splitter. However, we have a lot more separators now. The reason why this is so cool is because let's take a look at this first separator. It's a new line, and then it's going to be a pound symbol, which indicates a heading within markdown. And this regex here means one pound symbol between one and six times. So it's a new line followed by a header, h1 through h6. Why would we do this? Well, headers usually denote what you're gonna be talking about. So this is a cool way to try to group similar items together. So these are the Langchain splitters. If you have your own different package, you might see other splitters. But if you wanna see the Langchain side of the house, you can head over to their GitHub and you can go find on LangchainLibs, LangchainLangchain, textsplitter.py, you can see that they have a markdown language and here are the splitters that they actually end up using. All right, so let's go ahead and load up our Langchain markdown splitter. I'm gonna do a chunk size of 40, which is, again, this is really, really small. My first go-to for chunk sizes is gonna be anywhere between 2,000, 4,000, 5,000, 6,000 different chunks. And as context lengths for language models get better, and their performance with large context gets even better, well, you're gonna start to increase this a whole lot because it can infer what you want there, all right? So let's go ahead and do that. And then, so here's some markdown text, fun in California, H2 driving, blah, blah, blah, blah, blah. Cool. And let's split these up. And so we can see here, the first document is fun in California driving. And what's cool is that it split it on these headers. And so split on header here, et cetera, et cetera, split on header here, which is nice. That's markdown. You can do this also for Python, but instead of using the markdown splitters, you're going to want to have your own Python splitters. So again, Langchain is going to split on classes, functions, indent and functions. So these might be methods within your class, or you might have double new lines, new lines, spaces, and characters. They have a Python code text splitter. Let's go ahead and run this. Let's see what we got, chunk size of 100. And we scroll and we can see that we have, this whole class is enveloped in within one document, which is cool, because that's what we'd want. But then we have p1 equals john equals person. We have this stuff and we have the ranges right here. I put this over on the chunkfiz.com and you can go ahead and you can throw this in there. You can go to Python splitters let's see how long this that was 100 let's go ahead and bump this up to 100. we have it at 100 right here and langchain i couldn't figure out how to undo the strip white space with them within the javascript version to make this tool which is why i'm a little hesitant to show up but either way you can see that we're splitting right there that's what we'd want nice all right same thing we have javascript we also have just a bunch of different separators that we have here this is very similar but in this case we're going to do our recursive character text splitter again but now we're going to specify which language we wanted to split by so here's our text recursive character text splitter dot from language this time and we're going to do a language which is going to be language.js chunk size 65 let's go through there let's do it and then all of a sudden we split up our javascript code as well cool those are all strings those are all pretty easy because because they might just be in.txt files, which is simple enough for us to work with. However, what if you have PDFs? Everyone loves talking about PDFs. They especially love talking about pulling tables from PDFs. This is because there's a lot of old school industries that still put information inside of PDF. So when it comes to chunking, you're not only just gonna split text, but you just wanna pull out all the different elements within your documents. And some of those might actually be tables. It might be pictures, it might be graphs. Let's take a look at how we do this here. So the way I'm going to do this is I'm going to load up a PDF right here. And this is just going to be a Salesforce financial PDF. I went over and I just pulled one of their random PDFs that we have here so I can have a table. All right. So we have this. And the way I'm going to do it is I'm actually going to do it via unstructured. Now unstructured is another library. We can go check them out. They're at unstructured.io. We get your data LLM ready. So they have some really cool parsers that you can use, which is going to be advantageous for when you start to get more complicated data types or your data gets a whole lot more messier. So for example, if you had a million PDFs that you somehow needed to get into a structured form, unstructured would be who you went with that. Not a sponsored video by them at all. All right, so we're gonna load up two elements by them. It's gonna be partition PDF and then elements to JSON. We're gonna get our PDF. And if we take a look at this Salesforce PDF here, you can see that we have a few kind of just like notes or whatever. Then we have some paragraphs, but then we have this table and this is where I'm gonna start to place more emphasis on. And then we're going to do partition PDF. We're going to give it the file name and then we're going to give it some unstructured helpers. This is just a little bit of config for them. So if we take a look here, let's load it up. Let's see what elements I actually found. And so I found a whole bunch of them here. It looks like we just have some narrative text. This is going to be your regular text, but then we have this table and that's actually what I want to double click on. And so what I'm going to do is I'm just going to go grab the fourth from the last element, which is going to be this table right here at one, two, three, four. And let's look at what the HTML looks like. So here we have the HTML. And now you might be saying, well, Greg, how come the HTML is important? Why won't you just want to read it like a table? Well, tables are easy for us to read. They're not so easy for the language model to read. However, the language model has been trained on HTML tables, not only HTML, but also Markdown. But in this case, I want to pull out the HTML table because the language model is going to be able to make more sense of this than I can. So when I pass my data to the language model, I'm going to pass the HTML or you can pass Markdown, whatever works for you. If you want to see what this actually looks like, you can head over to an HTML viewer and you can see what the unstructured actually pulled out, which is pretty cool. Nice. So that's how you do tables within PDFs. But now let's say you have images within PDFs or maybe you have images elsewhere. How are you gonna take advantage of those? How are you gonna extract those? Well, let's take a look at how you do that. And I'm gonna use unstructured once more this time. I'm gonna do their partition PDF, which is the same as last time. And here I have a visual fine tuning paper. You can go and check this out. Let's go look at the archive one. I'm gonna download the PDF just so you can see it because there's this wacky photo up at the front. All right, so here's the same one. I'm gonna load up that page that I had, and then I'm gonna get the partition PDF ready for me. And this time I'm gonna do extract images in PDF equals true. So now it's gonna extract all the parts for me, which is the chunking process, but it's gonna treat the images separately, which is nice. Also infer table structure, blah, blah, blah. Let's go from there. image output directory path, this is in this repo. So you don't have to reload the images if you don't want to. But either way, I'm gonna load those up and I don't wanna make you wait. This does take just a little bit of time. So let me come back to you. One minute later. Awesome, so that just finished uploading for us. Let's take a look here. So I know it's kind of small on the screen, but they found, looks like 15 or so different images or 16 different images on here. And that was all extracted from the PDF that I supplied. Now the interesting thing about this is how are we gonna make those images useful, right? we would need to take an embedding of them because we're probably gonna do semantic search later. However, embedding models usually don't cross paths between text and images, meaning there's embedding models for images and there's embedding models for text, but generally their vector length isn't gonna line up. And if you don't have the same model for each one, doing similarity search between the two may give you a hard time. Yes, I know for all the perfectionists out there, there is something called the clip model, which is gonna do embeddings for both images and text so you can take advantage of them. However, the tech is still not quite there and I haven't found the same performance with clip that I have with other ones So I'm going to show you a different method here I will note this to say that in the future and when you're watching this or maybe really good models that do both of these If it's true, then I would take advantage of those instead of the method I show you but either way Let's take a look here So what I want to do actually is I want to generate a text summary of each image And then I'm going to do an embedding of that text summary So now what I can do is I'm going to go do semantic search. Maybe the text summary will get returned for me. If so, then I can pass that image to a multimodal LLM, or I can just use the text summary on its own to answer my question or do my task. Cool. The way we're going to do this is we're actually going to use LangChain. And here we are. We're going to load up chat OpenAI. And we're going to use the GPT-4 vision preview model. All right. So I'm going to load that up. And I made a quick function here. This is just going to convert the physical file on my local machine to Base64, which we then can go and pass to the language model. So string to base64. Now we have this image string. Let's go take a look at what this looks like. It just looks like a bunch of gobbledygook, which that doesn't mean much to me, but it will mean something to OpenAI, and I'm glad that it does. All right, let's close this. Let's get that out of there. So what I'm going to do is I'm going to use the GPT-4 Vision 1 again, and we're going to construct a human message. This just means it acts as if it's coming from the human content type. Please give me a summary of the image provided. be descriptive. And then we're going to pass it in image URL. And here we are, the URL is we're going to pass in our image base 64, what we had there. Let's go ahead and pass that over. And let's see what OpenAI thinks the image actually is. I haven't shown you what the image is, but let's look at the summary. The image shows a baking tray with pieces of food, like cookies or some baked goods arranged loosely to resemble the continents on earth as seen from space. Hmm. What do you know? There you go. Yeah, that makes sense. So now when I do my retrieval process, I can either just use this text in lieu of the picture if I don't want to work with a multimodal LLM or I can do semantic search, have this summary get returned and then pass this image over to the language model, the LLM. All right. So that seems about right. So what I've done in level three here is emphasizing that your chunking strategy really depends on your data types. So in this case, I was pretty explicit about that and I showed you what it would mean for Python and JavaScript and if you have images. But in your industry, in your vertical, you may have different data formats and you'll want to pick a chunking strategy that is going to adapt to those data formats. Because remember, the ultimate goal is that you want to group similar items together so that you can get them ready and prepared for your language model task in the end. Now you'll see there that I even made an assumption that you want to group similar items together. I'm just saying that because generally you're doing question and answer and generally you want to combine similar items together for context to answer a question. However, if you're not doing that, maybe for some reason you want to combine opposite items together, in which case your chunking strategy would be a lot different. I don't know of anybody who actually do that, but let's get back to the tutorial here. All right, so now we're moving on to level four, semantic chunking. Now the interesting part about levels one through three here is we all took physical positioning into account. Doesn't it seem kind of weird that we would split up a document with the intention of grouping similar items together, we just assume that paragraphs have similar information in there? What if they don't? What if we have a really messy information and doing recursive character text splitting doesn't really do anything for us? You know, I saw this tweet from Linus. We can go take a look at this one. He says, weird idea. Chunk size when doing this, when doing retrieval augmented generation is annoying hyperprim and feels naive to turn it into a global constant value. I totally agree. Now he recommends, could we train an end to end chunking model? I didn't want to go quite that far because I think there's a little easier step that we could try beforehand and I wanted to do an exploration. But now what I'm going to do is I'm going to do an embedding based chunking method. It's a little bit more expensive and it's definitely more work and it's definitely slower than what we talked about for the first three, but it starts to take the meaning and the content of the text into account to make our chunks. The analogy I looked up beforehand is imagine the first three levels that's like having a bunch of books and putting them on a bookshelf depending on their size right and the bookshelf size but what if you want to group the books together by genre or by theme or by author well then you actually need to know what the books are about and that's what we're going to try doing level four here all right so when i thought about level four what i wanted to do was obviously semantic chunking and i chose an embedding based way to do this so what i wanted to do was i wanted to take embeddings at certain positions of our document. And then I wanted to compare those embeddings together, right? So if two embeddings are close to each other distance wise, well, maybe they're talking about the same thing. That's the assumption that we're going to make. If they're further from each other, that means that they're maybe not talking about the same thing, right? So what I'd imagine is I would have a big long essay. And then with those, I take an embedding of every single sentence that we have, right? And then I want to compare those embeddings together. Now the comparing the embeddings, that's going to be the important part and where all the magic is going to be for this. And I did two different methods that I wanted to share with you. The first one is I did hierarchical, that's a mouthful, hierarchical clustering with positional reward. So my first thought is, well, you know, let's just do a clustering algorithm and let's see which embeddings are clustered together. And then let's assume that those are the chunks that we're going to have. But one thing I wanted to do was take into account short sentences that appear after a long sentence, you know just like that i wanted the you know to be included with that long sentence because it's likely needs to be relevant with it and so i added a little bit of a positional reward so hierarchical clustering generally is just going to be based off of distance but i had a little a little uh extra sauce to it and did some positional reward all right this one was okay but it was kind of messy to uh to work with and it wasn't as logical as i wanted it to be and i couldn't really tune this intuitively like I wanted. So I wanted to find something just a little bit easier as an exploration for me. So what I did was, is the next method was to find break points between sequential sentences. So I got embedding number one of sentence number one, and I compared that to sentence number two's embedding. And I measured the distance between them. And then I got two, compared it to three, and then three compared it to four, and so on and so forth. I do a visual with this, and so I guarantee it's going to make more sense in a second. We're going to use Paul Graham's essay. And what I'm going to do is I'm first going to split all of my different sentences. And I'm going to do that just via some regex with a period, a question mark, or an explanation point. There's likely a lot of better ways to do this. Don't come at me with that. But either way, we have 317 different sentences in this Paul Graham essay. All right. So what I want to do is I want to start adding more information to each one of these sentences. So it's like I have a Langchain document, but I'm just going to do my own to show you how we're going to do this. So instead of having a list of sentences, I want to have a list of dictionaries of which the sentence is placed in it. I'm going to add in the index just because it's fun. Why not? And let's take a look at these first three after I do that. Now I have a list of dictionaries and one of the keys is sentence. And now we have these different sentences up here. Because if I were to go up here, let me just show you what this looks like. The single sentence list. I want to do this with this, the first three again. It's just a list of strings. Now these list of strings are a list of dictionaries. All right, cool well now what I want to do is I actually want to do some combining of the sentences like I said if I just did sentence one compared to sentence two compared to sentence three it was a little noisy it was kind of all over the place and it didn't tell me much I thought you know what if I combined the sentences so there's a little less movement from each one because now what I want to do instead of comparing one to two comparing to three comparing to four etc I'm going to compare the embedding of sentence one two and three combined with sentence two three and four combined then compare that with sentence three four and five combined so it's a little bit more of a group i did a just a small little function you could take advantage of here i have a buffer size of one means one sentence before and one sentence afterwards you can do whatever you want go and switch around with this and go play with it i won't go through this code but i've commented it so you can follow along if you want now let's take a look at what that does so here we have our original sentence but now we have our combined sentence all right and this combined sentence is going to be what comes before and after it because this is the first one there's nothing before it's only after so get funded by y combinator is the sentence of number two nice so we have a combined sentence here which is want to start our startup that's what sentence number one is and then uh something in the grad school and that's what sentence number three is cool now that we have those what i want to do is i want to get an embedding of the grouped sentences of this combined sentence key so i'm going to use open ai embeddings and let's go through this and i'm going to get all the embeddings which is basically get the combined sentence for x in each one of those sentences and this is going to be we're going to go grab all those which is really nice we have our embeddings now i need to put those embeddings with its proper list. All right. So sentence with it. Now I'm going to make a new key, the combined sentence embeddings, and I'm just going to go through and add those. And let's go take a look at what that looks like now, because of course this is fun. And I like doing this in an iterative nature, so we can take one step together at a time. All right. So sentence want to start a startup. Here's our combined sentence embedding. So now we have this embedding for what's up here. All right, cool. Well, now what I want to do is I want to add one more metric to it. And I know we keep on going here, but hopefully you're still following along. I want to add the distance between the first sentence and the second group of sentences. I want to add that to the first sentence so I can see how big is the jump with the next one. All right. So what we're going to do is we're going to get the embedding of the current thing. We're going to get the embedding of the second thing, the second group that it comes with. We're going to get the distance. We're going to append the distances because we're going to do something with this later, but then we're going to start, we're going to get a distance to next. So how far is the distance between the current embedding with the next one? Let's go ahead and let's run this. And so now we just got that and this is added to our sentences, but we have our distances here too. So let's just take a look at the first three distances. Awesome. So this is 0.08. So this means that sentence number group number one is 0.08 distance away from group number two. And group number two is 0.2 distance away from group number three. Hmm. It's kind of interesting. Why is group one further away from group two than group two is further away from group three? I don't know, but we're going to do something with this in a second. Let me show you what these sentences look like one more time, just because we're doing this iteratively. Oh boy. I added a whole bunch here. There's too many. I should have just did the first three. Okay. Um, we'll go through this. Let's scroll all the way down to the bottom. We got a long ways to go. Okay. Now we finally have a distance to next because this is the first one, you can see that it's 0.08. That's what we just saw above. Cool. Let's close that up. We have our distances here, but now we're all data people. We're all having fun. We all want to see some visuals. I want to see some visuals. Let's do that. Any data scientists out there will laugh at this because I've typed this more in my life than I think I've ever should. Import matplotlib.pyplot as PLT. That is just absolute muscle memory for me right now. All right. So now we plot our distances. Cool. So this is our distance. Now it looks kind of random, doesn't it? Well, I mean a little bit. You can see here that it looks like there's a little bit of some ebbs and flows. This one, there's a little bit more distance in between. So what this would mean in English is that for some reason, the chunks here are more dissimilar from each other than further down than chunks that are grouped together. But either way, what's interesting to me is that we have some outliers up at the top here. You can see here, there's these points up at the very top. And that tells me, hmm, maybe there's good break points there because two groups are so dissimilar that they should actually be chunked up and they shouldn't actually be together. Because if there's a long distance in their embedding space, maybe they're not talking about the same thing. All right. So I want to show you this one more time, but let's iteratively build another visualization to further emphasize the point that I'm trying to make here. All right. First thing I'm going to do is I'm just going to plot the distances. Let's go down. That's the exact same thing that we had beforehand. All right. Well, the next thing I want to do, I just want to do a little bit of formatting. Don't hate me for this. First thing I'm going to do is I'm going to do a Y upper bound. This means the bound of the upper Y limit. Because as you can see right here, there's not enough cushion up here for me. It's visually too off. I want to fix this. All right. And then we're going to do a Y limit of from zero to the Y upper bound, which means how long is your Y axis. And then we're going to have an X limb of how long you want this to be on the x limit because you see here there's these buffers in between. I don't want that. All right, let's get rid of that. Anyway, we go through this. We have more space at the top. We got rid of the sides. All right, what's next? Let's see what we have here. I don't even know what's going on here. I wrote this code. I still don't. I'm just kidding. I do. Breakpoint percentile threshold. So what we need to do is we need to somehow identify which outliers do we want to split on. Now, there's a million ways you could go about doing this. And I'm really excited to hear alternative methods for you that you may have in the back of your mind. For me, what I ended up doing was I just did a percentile base. I wanted to identify the outliers. So using these different points as a distribution, I wanted to find out, Hey, which points are in the top 5%? Because if it's in the top 5%, well, those are probably going to be outliers for us. So I only want to take these, um, upper, these upper distances right here. And the way I'm going to do that is I'm going to specify the percentile I want to take, and then I'm going to use them by, and I'm going to go MP dot percentile. I'm going to pass it my distribution of distances, and I'm going to get the break point percentile threshold. That's going to be 95. And then what I want to do is I want to draw a line on the graph showing where that threshold is. So I'm going to draw a horizontal line right across. And the Y is going to be the break point distance threshold. This will be a number that says, hey, what is the 95th percentile of all these? All right, let's take a look at what that looks like. And here we go. So now anything above this line is going to be in the 95th percentile. These will be my outliers where I'm going to eventually make my chunks. So everything that's in right here up until this one point, that'll be chunk one. Then we have chunk two, then we have chunk three, chunk four, chunk five, blah, blah, blah, and going from there. Because again, one last time, I know I keep on repeating this, but I really want to hammer this home. The hypothesis is that if there's a big break point, then a chunk should be split up at that point. And so this is where we're going to end up doing that. All right. Then what we're going to do is we're going to see how many distances are actually above this one. And so I want to get the number of distances that we have, meaning the number of breakpoints, the number of things that are going to be above that threshold. And then I'm going to do plt.txt. This is just a fancy way to put some text on your visualization. Let me go ahead and do that. And so we can see that we have 17 chunks. I put that down in the corner right there. That's kind of interesting. All right. Then what we're going to do is we're going to get the indices of which points actually are above the breakpoint meaning which are the actual outliers because this break uh this breakpoint distance threshold this is just a single static number but i need to get a list of numbers to find out where these breakpoints and these chunks actually need to be met so for i in distances if x is above the breakpoint distance threshold so i'll get a bunch of indices there that doesn't do anything different for us but then what we're going to do actually let me just do this because i think this actually would be helpful i'm going to look at indices now what we have here these are 17 different chunks we can look at this it says 16 but that's because there's an extra one added to the front there should be a zero right here um or this could be the 317 at the end so what this means is between uh between sentence zero and sentence 23 we want to make our split and make our break all right because at just at number 23 it says hey here the distance to the next one was quite big so we want to include number 23 on this one okay either way let's go from there let's go do some more uh let's do some fancy colors on here so what i want to do is i want to add some colors i just set my own custom ones right here and then what i'm going to do is i'm going to go do a vertical span meaning you're going to have a vertical shading in the background of your um the background of your graph here and you're going to do a start index and an end index the reason why i do a for loop here is because you add them one at a time but the start index and the ending end index will be what is in our indices above thresholds anyway let's go through there and you can see here i cheated a little bit let's take away this text we can see that we have our different chunks right here so now we have chunk zero chunk one chunk two blah blah and go all the way through there but of course we want some text on here to really make it more explicit and you can see here chunk blah blah blah blah this last one was giving me a hard time so i actually had to do just a little bit of custom code for that one that splits it up uh that's just a little bit of band-aid don't don't add me for that one either I was too lazy to figure that one out. Okay, cool. So we go through there. So here's all of our different chunks. Now this wouldn't be a good chart unless we put on some graphics or some titles as well. So now we'll do a title, a Y label and an X label. And then there we go. That's kind of cool. Paul Graham essay chunks based off embedding break points. That's pretty interesting, but a good visualization doesn't do anything for us. We can't really pass this to the language model and it's not going to know what to do with it. So what we're going to do is we're going to actually get the sentences and actually combine them. So here's a bunch of code here. I won't go through it too much, but the TLDR is that you're going to append all these different pieces in your chunks. So like I said beforehand, you're going to go from chunk zero to chunk 23, and you're going to combine those, and that'll be your first chunk. Let's go through there. Then let's take a look at what this actually looks like. Let's go through this. And so we have the chunk number zero, about a month of the defining cycle. We had something called a prototype day. You might think they wouldn't need any more motivation. Cool. They're working on their cool new idea. They have funding for an immediate future and they're playing the long game with only two outcomes, wealth or failure. You think motivation might be enough. So the hypothesis here is that these two are actually getting split up at a spot where there's a big break point with it. So it's kind of interesting to see where the semantic splitting actually happened. There you go. And now I want to reemphasize that this isn't perfect, of course, but I think this is an interesting step towards doing semantic chunking. Because if I were to think, if I'm going to hypothesize out in the future, what is chunking going to be like? Well, as compute gets better, as language models get better, there's no way we're going to do physical-based chunking anymore, unless the structure of our documents is we can make those big assumptions on it. We're probably going to do a smart chunking. And I think this is a really cool way to go towards that. All right. So that's level four. Again, experimental. Please let me know what you think, please give me other ideas for how we could make this a little bit better. And this is going to be a little plug. If you want to see more of these experimental methods that I do, I share this out on Twitter. So other people got to see this early. Now let's move on to level five, agentic chunking. So if we went off the deep end with level four, we're going into the ocean here. We're going into the Mariana Trench and we're going to the very bottom and we're going to do some cool things. So my motivation for this side is I asked myself, Hey Greg, what if a human were to do chunking? How would I do chunking in the first place? And I thought, well, I would go get myself a piece of scratch paper because I can't do all that in my head. I'd start at the top of the essay and assume the first part will be in a chunk. Well, because the first little bit, it needs to go somewhere. We don't have any chunks yet. So of course it's going to go in the first chunk. Then I would keep going down the essay and evaluate if a new sentence or piece of the essay should be a part of the first chunk. If not, then create a new one. Then keep doing that all the way down on the essay until we got to the end. And I thought to myself, wait, wait a minute. This is pseudocode. We can make something like an agent to do this for us. Now, I don't like the word agent quite yet because we're not quite there yet. A lot of people like using it. I think there's a lot of marketing around it. So yes, I call this agentic chunking, but I'm going to call this an agent-like system. The way that I like to define what an agent is, is there's some decision making that needs to go on in there and you use the language model to make decisions which you're going to have an unbounded path but the language model will help you guide that via your decision making that you do and so i thought man this is pretty cool i'm going to try this out all right so now let's go into level five and talk about what i found here so one first design decision that i need to make is how do i want to give different pieces of my essay to the language model and i thought man well there's still that problem with the short sentences you know so So around this time, there was a cool paper that came out all about propositions. What is a proposition? Well, a proposition is a sentence that can stand on its own. So there's another agent-like system. It's kind of just a prompt. We'll go over that in a second here. But it's going to take a sentence, and it's going to pull out propositions, which are little itty-bitty Legos that can stand on their own. Let's talk about what that means. Greg went to the park. He likes walking. If you were to pass he likes walking as a chunk to the language model, language model is going to be like, WTF? Who is he? However, if we change this into propositions, it will make less sense from a reader's perspective, meaning it doesn't look great for us to read, but it makes a lot more sense for a language model. Greg went to the park. Greg likes walking. That's pretty interesting. If you want to take a look at more proposition work, and I might do a whole nother video on this. Let me know if you want me to, because I think this is really cool. Langchain came out with proposition based retrieval. So a new paper by Tom Chen. I haven't met Tom Chen yet, but Tom, I'm a big fan of your work. If you want to chat, I'm very down to talk. In the image that they showed, prior to the restoration work performed between blah, blah, blah, blah, blah, blah, you have this big long paragraph and then it's going to split up into different propositions. And then you use that for your retrieval because these sentences can stand a bit more on their own. Let's go ahead and do it. I'm importing a whole bunch of Langchain stuff here. I'm not going to go over each one except for the cool parts. Now one of the cool parts here is going to be this Langchain import hub. Hey, Greg, what's the Langchain hub? I should probably do a whole another video on this either. But they have this thing or Langchain came out with this thing called the Lang Hub. This is within their Lang Smith suite, but a Lang Hub is just going to be where they share prompts around. So this person, I don't know who this is, looks like it's been viewed a lot. YouTube transcript to article, act as a expert copywriter specializing blah, blah, blah. So Langchain will help host prompt templates that you can use for your own. So it's an easy way to just go share prompt templates. So here we have this whole entire prompt and here we have a token where you can go and grab it. And then if you wanted to grab this prompt yourself, then you can go and just grab it like this. The advantage of this is you can share prompts a whole lot easier, but then two, if you want prompts to be updated with the latest and greatest thinking, well, you can just keep on pulling from there, which is nice. So I'm going to pull a hub.poll, WHF proposal indexing. So let's go view the proposition prompt here. WFH proposal indexing. Here's a citation that comes with it. And here's the chat prompt template. I won't go through all this, but the interesting part is split the compound sentence into simple sentences. Maintain the original phrasing from the input whenever possible. And then they actually give one example here. So they have an input one here, and then they have an output about what they'd want the language model to output. Hmm, interesting. I just went and copied this code. I put that in right here. I'm also going to get my language model going. And we're going to use GPT for a preview because I want the long context and good processing power. All right. So within this object is going to be the prompt. Then I'm going to create a runnable which combines the prompt and the language model. And this is via the Langchain expression language. And with that language, you can do just this pipe operator and you can combine those right there. And the next thing I'm going to do, I tried doing some extraction of these propositions via the recommended way, but I found it was giving me a hard time. So I just made my own extractor from it. and the way i'm going to do that is i'm going to just do the pidantic extraction method so i'm going to create a pidantic class here and then i'm going to do extraction chain create extraction chain with pidantic and i'm going to pass it in the sentences pass in the language model cool we have that and then i'm going to create a small little function which is this the get propositions which is going to be hey go and get this pro go and get the list of propositions from the thing that i give you because it takes a runnable and then it's going to do the extractions and it's going to return the propositions for us. All right. So now I'm going to do Paul Graham's super linear essay. Cool. We have that. I'm going to split the essay into paragraphs. Now, this is a design choice. Hey, Greg, aren't we doing just chunking again? Not really, because this is like a very loose chunk that doesn't really matter. I could pass it a sentence at a time. I could pass it too. I could pass the paragraphs. See how many paragraphs we have. We have 53 different paragraphs. Nice. Let's take a look at one of the paragraphs just because I like being super explicit here. Let's do number two. Nice, one of the most important things. I didn't know, et cetera, et cetera. We can look at number five. Cool, those are just paragraphs for us. All right. Then what we're gonna do is we're gonna go get our propositions. And so we're gonna do essay propositions. And so I'm just gonna go through this, go through each paragraph. I'm just gonna do the first five because there's kind of a lot of data here. I'm gonna get the propositions. Let me come back to you when this is done. One minute later. Awesome, so we have our propositions. Let's take a look at a few here. Again, because we're doing this iteratively with each other. All right. I'm going to take a look at a few. You have 26 propositions. Cool. So our five paragraphs resulted in 26 propositions. The month is October. The year is 2023. At the time past, I did not understand something about the world. Cool. So now we're starting to pull out individual facts about what the paragraph is about. Lovely. So now what I want to do is I want to use an agent-like system that is going to go through each one of these iteratively and decide, hey, should this be a part of a chunk that we already have or should it not? There's no package that I saw that did this for us yet because this is a quite experimental method. But what I did is I ended up making a agentic chunker. This isn't a package yet, so you can't go import this anywhere, but I'll show you the code that's powering this. Awesome. So here's the code on how this works. Now, I'm not going to go through this in detail because that's not the point of this tutorial, but I'm going to go through the high level pieces here. The way that it works is you're going to have AC equals agentic chunker. Okay, cool. And then you're going to have your list of propositions. This could be sentences, but propositions will be best because that's how I designed it to work. And then what you're going to do is you're going to add your propositions to the class. And then what it's going to do is it's going to start to form chunks for you. Then we're going to pretty print the chunks. All right. So let's go through and see how this works. The real magic happens in the add propositions. So if we go up to the top here. So we're going to add a proposition. Now, if it's your first chunk, meaning your first proposition, your first chunk, then you're not going to have any chunks and chunks is going to be a property of this class. If chunk size equals zero, meaning you don't have any yet, then create a new chunk. Totally makes sense. Well, what do you want the chunk to be about? Well, let's go find out where create new chunk is. And on create new chunk, what we're going to do is we're going to create a chunk ID, which is going to be a random UUID or a subset of one. then we're going to get a chunk summary and a chunk title. The reason why we do this is because when we add a future thing, we need to know, well, what are our chunks already about, right? Because that'll tell us whether or not we need to add it. So we're going to have a summary, which is going to have a lot of good detailed information, and we're going to have a title, right? So in get new chunk summary, all that this is doing is looking at the propositions that are currently in the chunk, and then it's generating a summary about what that chunk is about. And then the chunk title, this is just a few words that kind of give you a quick glance on what it actually is. Now, there is a parameter you can set when you do this. Do you want to update your summary and title? Because as I was going through this, as I added, say, proposition number one, it was about one thing. But then once you added proposition number two and three and four to the chunk, you may need to update the summary or update the chunk because now the chunk is kind of just slightly different, right? It's kind of as if you had a centroid and it's moving just slightly, but you want to capture that essence, all right? And then what we do is with each one of the chunks we have a chunk id we have a proposition or we have a list of propositions a title a summary and then the chunk index is just um when was this chunk made its number cool uh let's go back to where we were um okay so that's if if you add uh your first proposition to your empty list of chunks and then you're going to return here meaning you're going to stop you're not going to go any further because well you've already added it to a chunk it's all good cool but let's say that wasn't the case well let's go find a relevant chunk this part i thought was actually kind of cool too in this relevant chunk you're going to have a proposition now you want a proposition to go in and you want a chunk to come out right and so we have a simple little prompt here determine whether or not the proposition should belong to any of the existing chunks and then i do an example here i have some other good stuff but what i'm going to do here is i'm going to pass it a string of what our current chunks actually look like and those current chunks are going to be groups of three things the id of the chunk the name of the chunk and the summary of the chunk because what i want it to do is if it deems that yes this proposition should be part of a chunk well then what i want you to do is i want you to output the chunk id for me and this is how i can then extract which chunk it should actually be a part of all right so we're going to go through there we got all of that and let's just say that a chunk id does come out, meaning it should be added to another chunk. Well, what I'm going to do is I'm just going to add that proposition to the chunk. Nice. So yes, I found a chunk it should be a part of. Cool. I'm going to add this proposition to it. Then when you add this proposition to it, this is where I was talking about beforehand. If you want to generate new metadata, then it will go and generate a new summary and title. If you don't want to do it, then it doesn't have to. So let's say you didn't find a new chunk ID, meaning you wanted to add a proposition. It didn't find a new chunk. So you need to actually make a new one. No chunks are found, create a new chunk. And that's the same thing that we had up above. And you go from there. So that's really the meat of the entire thing. And if we're going to go back to our repo here, let's go do this for our essay that we had. All right. So I'm going to do the agentic chunker. I'm going to say from AC or make AC. And then what I want to do is I want to add each one of the first 20 propositions. But you know, we only had 23. So let me just do this whole thing here. And then what I did was, is I have a lot of print logging. If you don't want that because you think it's annoying, just go print logging equals false. But let's step through this. So now it's adding our first chunk, which is the month is October. No chunks, duh, because you don't have any. So it's created a new chunk, which is 51322. It's called date and times. Yeah, cool. Makes sense. That's probably where I'd want this to go. The year is 2023. Chunk found. Date and times. Oh, duh because what it did is it saw there's a chunk there's a date and times chunk i'm going to add this one to it too okay cool i was a child at some past time no chunks are found because there's only one at this time and it doesn't think it's part of that one and so created a new chunk personal history nice okay at the past time i did not understand something important about the world it's adding it to personal history cool makes sense the important thing i did not understand is that the degree in which returns for performance are super linear. It didn't find any chunks, and so it doesn't think it's part of date and times or personal history. It made a new one called performance and returns relationship. Cool. Teachers and coaches implicitly told us returns were linear. Chunk found. It's adding it to the returns chunk. Nice. Teachers and coaches meant well, no chunks, blah, blah, blah, and we go all the way through here, and so it's kind of interesting, but what I want to show you is an instance where a chunk name was updated. So adding, you get out what you put in was heard a thousand times by the speaker. It's adding it to the misconceptions in performance of returns in relationships. Wait, where did that title come from? Oh, wait, it's the same chunk ID, but the name has been updated because you're actually updating the chunk as the chunk title as you go along here. So as we go through here, we're going all the way through. It did this a whole bunch of times. I want to see what comes out the other end. So you can pretty print the chunks. Cool, so it looks like we have five chunks. Chunk number zero has this chunk ID. This chunk contains information about the specific dates and times, cool. And here are the propositions that were added to that chunk, nice. So this is the content of our chunk that we wanna actually pull in. This chunk contains reflections on someone's childhood, blah, blah, blah, okay, cool. Oh, dang, oh, this is a big one. So now we have a bunch of statements that were pulled from the essay that are all about super linear returns across different fields. All right, cool. And let's look at this last one, or second to last one, teachers and coaches. Okay, cool. Consequences of inferior product quality on business viability and customer base. Nice, all right. So that's kind of interesting. Now we're starting to group similar items together. So if we have a question that pops up about product quality and business viability, well, here's the chunk that you gotta look at. Now, is this perfect? Well, not quite yet, because you could get some complicated questions where you may want multiple things from different chunks, But I think this is a really interesting direction on how we'd start to move towards there. And if we actually want to get these chunks because we want to go do something interesting with them, maybe proper index them, then you can go and get those and get the list of strings. So there you go. That's agentic chunking. Now, again, it's slow and it's expensive. But if you bet that language models will speed up and they'll get cheaper, which I'm guessing they will, then this type of method starts to come into play. Finally, what I want to do is I want to congratulate you on finishing the five levels of text chunking and text splitting. We are almost done, but I wanted to throw in a bonus level in there, right? So this bonus level is going to be through alternative representations. Now, much like before, like our chunking commandment, we need to think about how we're going to get our data ready for our language model, right? Chunking is only one part of that story. It's how you're actually going to split your texts. But the next step that you're going to take in front of that is you're going to get embeddings of your text and you're going to go throw those in your knowledge base, right? Well, there's different ways you can get embeddings of your text. And there's different questions that will come up. Like, should you get embeddings of your raw text? Or should you get embeddings of an alternative representation of your raw text? That's what we're going to talk about on this bonus level. Let's go through this very briefly. The first thing that we're going to talk about is multi-vector indexing. This is when you do a semantic search, but instead of doing semantic search over the embeddings of your raw text, you get it off of something else, like a summary of raw text or hypothetical questions of your raw text all right let's do this first one which is summaries and how we'd actually do that so we're going to get our super linear essay one more time i'm going to blow through this part since we already talked about it but what i'm going to do is i'm going to get my six chunks from our document and i'm going to say summarize the following documents and that's going to be a chain that we have there and then we're going to do this but we're going to do this in batch which is one of the other nice things about lang chain expression language and now we have all of our summaries and here's the first one so this is a summary of our first chunk that we have right now what i want to do is i want to get an embedding of this summary instead of the embedding of the chunk like i would have with the old method and the way that we're going to do this is we're going to get our vector store ready we're going to use chroma today and we're going to get just a dock store ready which is just going to be the in-memory byte store that link chain has and then we're going to pass in a multi-vector retriever this is a cool link chain abstraction where you can basically go semantic search off one thing but then return another thing for your final language model all right we're going to go through this there's a whole tutorial that they have on this in fact i have a whole tutorial on this as well at fullstack retrieval.com so i'm going quick through it but that's because i want to show you a whole bunch here we're going to get our summary docs we're going to add those and we're going to change those into proper lang chain documents instead of the actual strings that they were and then here's where the cool part happens we're going to add them to the vector store which is also going to get the embeddings for us we're going to add the summary docs and then we're going to add our normal docs to our normal doc store in our retriever nice cool and then what we could do here is we could go to our retriever and we could go get the relevant documents and so now what i'm going to do is with my query this is going to go match on the summary embedding rather than the raw document embedding but when these docs are passed back to us these are the raw documents so there's a little happening behind the hood here um but again i encourage you to go to fullstackretrieval.com and go check out the tutorial tutorial on this all right there's one method for us the other method you could do is well instead of summaries i want hypothetical questions this one's really nice if you anticipate a q a bot that you're making because then you can start to anticipate well which questions are people going to ask, the language model will generate questions for you. And there's a, your hypothesis is that there's a higher likelihood that these questions will end up matching for you and you'll get better document matching. Cool. Another one you can do is you can do a parent document retriever. So now this case, the hypothesis with this method is that if you subset your document even more, you'll get a better semantic search. However, in order to answer the question of whatever hypothetical question you could have, you actually want what's around that small document. So yes, that small document will do good semantic search, but you actually want the buffer around it. So what comes before and after it? And another way to say that is you want the parent document that that small chunk actually comes from. And in order to show you this one, I'm going to do a tutorial from Llama Index. Now, I also have another tutorial on this on fullstackretrieval.com if you want to go check that out. So with Lama Index, we're going to do their hierarchical node parser. And what you're going to do is you're going to give a list of chunk sizes. So they're going to get chunks that are 2048, 512, and 128 here. And let's go do our Paul Graham essay. Let's see how many nodes we actually have. This is 119 nodes. That's because 128 is pretty small for us here. And if we take a look at one of these relationships here, and this is one of the smaller ones because this is something at the end, and that's the 128. you can see that these chunks are quite small, and we are just looking at the relationships here. You can get a source, a previous, a next, and then the important part here is the parent. So if you were to do this via the Lama index way, yes, you matched on the 128 chunk because that has good semantic search, but you actually got the 512 or the 2048, right? I won't show you how to go all the way through that because again, that's a whole separate tutorial. And then the last thing that I'll show here is around the graph structure. So sometimes you're going to go over your raw text and instead of chunking your raw text, you actually want to extract a graph structure from that text because there's a lot of entities within your text. I'm going to do that via the diff bot. And this is the diff bot transformer. We're going to go, I'm going to say Greg lives in New York. Greg is friends with Bobby. San Francisco is a great city, but New York is amazing. Greg lives in New York. And let's see what actually pops out of over here. so now we have this so now we have actually have a graph document and one of our nodes is node id greg type person properties k greg we have another bobby node which is cool we have another node here which is an entity location it's entity new york we have another relationship node so this is kind of be like an edge and this is a social relationship between greg and bobby so i won't go through all these but either way you can start to see how now you can use a graph structure to answer questions about a person or your specific text. But that's a little outside the chunking side. Either way, that is actually the rest of the tutorial. And I want to congratulate you on what is quite a long video. I'm not sure how long this is. Probably one of my longer ones that we have yet. Either way, I'm excited that you're here. My name is Greg Kamrad, and I am on a mission to figure out how AI and business are going to overlap the Venn diagrams with each other. This has been a lot of fun. Thank you for joining me. We will see you later.\", 'chunks': [{'timestamp': (0.16, 4.56), 'text': ' One of the most effective strategies to improve the performance of your language model applications'}, {'timestamp': (4.56, 10.4), 'text': ' is to split your large data into smaller chunks. The goal is to give the language model only the'}, {'timestamp': (10.4, 15.12), 'text': ' information that it needs for your task and nothing more. This practice is the art and'}, {'timestamp': (15.12, 20.56), 'text': ' science of text splitting. It is one of the first and most foundational decisions a language model'}, {'timestamp': (20.56, 24.08), 'text': ' practitioner will need to make. Text splitting takes a minute to learn, but in this video,'}, {'timestamp': (24.08, 28.48), 'text': \" you're going to learn the five levels of text splitting that squeeze out more performance from\"}, {'timestamp': (0.0, 1.88), 'text': ' from your language model applications'}, {'timestamp': (1.88, 4.04), 'text': ' using the same data that you already have.'}, {'timestamp': (4.04, 6.72), 'text': \" Now, there's something for everyone in this video.\"}, {'timestamp': (6.72, 9.26), 'text': \" For the beginners, we're gonna start from the very basics.\"}, {'timestamp': (9.26, 12.08), 'text': \" And for the advanced devs, I'm gonna give you plenty\"}, {'timestamp': (12.08, 13.76), 'text': \" that you're gonna wanna argue with me on.\"}, {'timestamp': (13.76, 15.7), 'text': \" But either way, I guarantee you're gonna learn\"}, {'timestamp': (15.7, 16.68), 'text': ' something along the way.'}, {'timestamp': (16.68, 17.9), 'text': ' This is gonna be a longer video'}, {'timestamp': (17.9, 20.76), 'text': \" and we're gonna cover a lot, but that's on purpose.\"}, {'timestamp': (20.76, 22.88), 'text': ' I wanna take our time and I guarantee'}, {'timestamp': (22.88, 24.48), 'text': ' that if you make it to the end,'}, {'timestamp': (24.48, 27.78), 'text': \" you're gonna have a solid grasp on chunking theory,\"}, {'timestamp': (0.0, 2.62), 'text': ' strategies, and resources to go learn more.'}, {'timestamp': (2.62, 3.82), 'text': ' For those that are just joining us,'}, {'timestamp': (3.82, 6.18), 'text': \" my name is Greg and I'm exploring the AI space\"}, {'timestamp': (6.18, 8.2), 'text': ' through the lens of business value.'}, {'timestamp': (8.2, 11.52), 'text': ' You see, models are cool, stats are cool,'}, {'timestamp': (11.52, 13.22), 'text': ' but I wanna find out how businesses'}, {'timestamp': (13.22, 16.44), 'text': ' will actually be taking advantage of AI and language models.'}, {'timestamp': (16.44, 19.4), 'text': ' This video will be split up into six different sections.'}, {'timestamp': (19.4, 21.12), 'text': \" First, we're gonna talk about theory.\"}, {'timestamp': (21.12, 23.46), 'text': \" We'll talk about what splitting and chunking are,\"}, {'timestamp': (23.46, 25.76), 'text': \" why we need them, and why they're important.\"}, {'timestamp': (25.76, 29.14), 'text': ' I even made a cool tool called chunkviz.com'}, {'timestamp': (0.0, 1.76), 'text': ' to help us visualize along the way.'}, {'timestamp': (1.76, 3.44), 'text': \" Then we're gonna jump into the five levels\"}, {'timestamp': (3.44, 4.82), 'text': ' of text splitting.'}, {'timestamp': (4.82, 7.62), 'text': \" For each level, we're gonna progressively get more complex\"}, {'timestamp': (7.62, 10.42), 'text': ' and introduce topics along the way for you to consider'}, {'timestamp': (10.42, 12.76), 'text': \" when you're building your own language model applications.\"}, {'timestamp': (12.76, 15.34), 'text': \" For level one, we're gonna talk about character splitting.\"}, {'timestamp': (15.34, 16.74), 'text': ' This is when you split your documents'}, {'timestamp': (16.74, 19.24), 'text': ' by a static character limit.'}, {'timestamp': (19.24, 20.7), 'text': \" For level two, we're gonna talk about\"}, {'timestamp': (20.7, 23.0), 'text': ' recursive character text splitting.'}, {'timestamp': (23.0, 24.76), 'text': ' This is when you start with your long document'}, {'timestamp': (24.76, 26.42), 'text': ' and then recursively go through it'}, {'timestamp': (26.42, 29.3), 'text': ' and split it by a different list of separators.'}, {'timestamp': (0.0, 3.4), 'text': \" For level three, we're going to talk about document specific text splitting.\"}, {'timestamp': (3.6, 10.88), 'text': \" So if you have Python docs or JavaScript docs, or maybe PDFs with images, we're going to include multimodal in this level as well.\"}, {'timestamp': (10.88, 12.98), 'text': ' For level four, this is where it gets interesting.'}, {'timestamp': (13.18, 15.16), 'text': \" We're going to talk about semantic splitting.\"}, {'timestamp': (15.4, 18.46), 'text': ' So the first three levels were all naive ways of splitting.'}, {'timestamp': (18.78, 23.38), 'text': ' These levels focused on the physical positioning and structure of the text chunks.'}, {'timestamp': (23.78, 29.16), 'text': \" These first three levels, it's a bit like sorting a library based off the book sizes and shelf space,\"}, {'timestamp': (0.0, 2.3), 'text': ' rather than the actual content of the books.'}, {'timestamp': (2.6, 3.94), 'text': ' But in level four here,'}, {'timestamp': (4.1, 6.58), 'text': \" we're not just gonna look at where the text sits\"}, {'timestamp': (6.58, 7.54), 'text': ' or its structure.'}, {'timestamp': (7.96, 9.38), 'text': \" Instead, we're gonna start to delve\"}, {'timestamp': (9.38, 11.6), 'text': ' into the what and the why of the text,'}, {'timestamp': (11.94, 14.54), 'text': ' the actual meaning and context of these chunks.'}, {'timestamp': (15.08, 17.06), 'text': \" It's like understanding and categorizing the books\"}, {'timestamp': (17.06, 19.02), 'text': ' by their genre and themes instead.'}, {'timestamp': (19.28, 20.26), 'text': ' And then with level five,'}, {'timestamp': (20.36, 22.28), 'text': \" we're gonna talk about agentic splitting.\"}, {'timestamp': (22.8, 24.94), 'text': \" So we're gonna look at an experimental method\"}, {'timestamp': (24.94, 27.34), 'text': ' where you actually build an agent-like system'}, {'timestamp': (0.0, 3.72), 'text': \" that's going to review our text and split it for us.\"}, {'timestamp': (3.98, 6.88), 'text': \" And then to finish it off, we're going to end with some dessert.\"}, {'timestamp': (7.38, 12.1), 'text': ' A bonus level that shows the advanced tactics that start to creep a little beyond text splitting,'}, {'timestamp': (12.36, 16.36), 'text': ' but are going to be important for your overall knowledge about how to do retrieval in general.'}, {'timestamp': (16.66, 19.82), 'text': \" My goal isn't to prescribe the best or most powerful method.\"}, {'timestamp': (20.14, 22.18), 'text': \" You'll see why that's actually not possible.\"}, {'timestamp': (22.32, 26.46), 'text': ' My goal is to expose you to the different strategies and considerations of splitting,'}, {'timestamp': (26.82, 29.88), 'text': \" your own data, so you're able to make a more informed decision when you're building.\"}, {'timestamp': (0.0, 2.56), 'text': ' This is part of a larger series on retrieval I have going.'}, {'timestamp': (2.86, 5.56), 'text': ' And if you want to check out more or get the code for this content,'}, {'timestamp': (5.68, 8.92), 'text': ' head over to fullstackretrieval.com and I can go send it to you.'}, {'timestamp': (9.04, 11.74), 'text': ' Lastly, I do a lot of workshops with individuals and teams.'}, {'timestamp': (11.96, 16.72), 'text': ' If you, your team, or your company want to chat live or do a custom workshop,'}, {'timestamp': (16.88, 17.92), 'text': ' just feel free to reach out.'}, {'timestamp': (18.1, 21.1), 'text': \" So without further ado, let's jump into it.\"}, {'timestamp': (21.5, 24.5), 'text': \" First, we're going to start off with a theory behind text splitting.\"}, {'timestamp': (24.76, 27.18), 'text': ' What is it and why do we even need to do it in the first place?'}, {'timestamp': (0.0, 4.52), 'text': \" You see, well, applications are better when you give it your own data or maybe your user's data,\"}, {'timestamp': (4.68, 9.98), 'text': \" but you can't pass unlimited data to your language model. And there's two main reasons for this.\"}, {'timestamp': (10.4, 15.52), 'text': ' Number one, applications have a context limit. This is an upper bound on the amount of data'}, {'timestamp': (15.52, 19.92), 'text': \" that you can actually give to a language model. You can see the context windows on OpenAI's\"}, {'timestamp': (19.92, 25.1), 'text': ' websites for their own models. And number two, language models do better when you increase the'}, {'timestamp': (25.1, 29.94), 'text': \" signal to noise ratio. Let's see what Anton, co-founder of Chroma, has to say about this.\"}, {'timestamp': (0.0, 7.44), 'text': \" distracting information in the model's context window does tend to measurably destroy the\"}, {'timestamp': (7.44, 12.4), 'text': ' performance of the overall application. So instead of giving your language model the kitchen sink'}, {'timestamp': (12.4, 17.28), 'text': ' and hoping the language model can figure it out, you want to prune the fluff from your data whenever'}, {'timestamp': (17.28, 23.12), 'text': ' possible. Now text splitting or chunking is the process of splitting your data into smaller pieces'}, {'timestamp': (23.12, 28.8), 'text': ' so you can make it optimal for your task and your language model. Now I really want to emphasize'}, {'timestamp': (0.0, 5.88), 'text': ' this point. The whole goal of splitting your text is to best prepare it for the task that you'}, {'timestamp': (5.88, 10.88), 'text': ' actually have at hand. So rather than starting with, Hey, how should I chunk my data? Your'}, {'timestamp': (10.88, 16.24), 'text': \" question should really be, what's the optimal way for me to pass the data that my language model\"}, {'timestamp': (16.24, 22.52), 'text': ' needs for my task. Our goal is not to chunk just for chunking sake. Our goal is to get the data in'}, {'timestamp': (22.52, 27.3), 'text': \" a format where it can be retrieved for value later. So let's talk about retrieval in general.\"}, {'timestamp': (0.0, 4.4), 'text': ' So in the bigger picture, the act of gathering the right information for your language models'}, {'timestamp': (4.4, 9.84), 'text': ' is called retrieval. This is the orchestration of tools and techniques to surface up what your'}, {'timestamp': (9.84, 14.42), 'text': \" language model actually needs to complete its task. Let's take a look at where chunking fits\"}, {'timestamp': (14.42, 18.26), 'text': \" into the retrieval process. So here we're taking a look at the full stack retrieval process.\"}, {'timestamp': (18.48, 22.06), 'text': ' We have everything from your raw data sources to your response here. If you want an overview'}, {'timestamp': (22.06, 25.74), 'text': ' about this entire process, head over to fullstackretrieval.com where I do a separate'}, {'timestamp': (25.74, 29.58), 'text': \" tutorial on this. Now, the important part is we're all going to have our raw data sources\"}, {'timestamp': (0.0, 3.66), 'text': ' down at the bottom here, and they eventually need to make it into our knowledge base, right?'}, {'timestamp': (4.14, 7.94), 'text': \" However, we can't just put our raw data sources, we're going to need to chunk them, which is what\"}, {'timestamp': (7.94, 12.4), 'text': ' this video is about. Now, right when you do your data loading, this is where your chunking strategy'}, {'timestamp': (12.4, 16.76), 'text': ' is going to come into play. How you choose to split up your documents is a very important'}, {'timestamp': (16.76, 20.94), 'text': \" decision. As you go through this, you'll see that there isn't one right way to do your chunking\"}, {'timestamp': (20.94, 25.32), 'text': ' strategy, or really your retrieval strategy for that matter. For example, take a look at this'}, {'timestamp': (25.32, 29.94), 'text': ' tweet from Robert Hamir. For those that need a translation, Robert is basically saying that he'}, {'timestamp': (0.0, 6.12), 'text': ' employs many alternative strategies across his retrieval stack. What works for him may not work'}, {'timestamp': (6.12, 10.76), 'text': \" for you. The last thing I'll comment on is the topic of evaluations. Evaluations are super\"}, {'timestamp': (10.76, 14.36), 'text': \" important when you're developing your language model applications. You won't know if your\"}, {'timestamp': (14.36, 19.0), 'text': ' performance is improving without rigorous testing. One of the most popular retrieval evaluation'}, {'timestamp': (19.0, 23.38), 'text': \" frameworks out there is Ragus. I encourage you to go check it out. I won't be covering those today\"}, {'timestamp': (23.38, 27.4), 'text': \" because it's more of a retrieval topic rather than this narrow niche that we're going to be\"}, {'timestamp': (0.0, 4.98), 'text': \" covering today. Plus, they're very domain-specific and application-specific. If you want my taken\"}, {'timestamp': (4.98, 9.52), 'text': \" evals, please head over to fullstackretrieval.com and you'll get a notice when I start to cover it.\"}, {'timestamp': (9.92, 14.98), 'text': \" All right, now that is enough talking for now. I finally want to get into some code. Let's move\"}, {'timestamp': (14.98, 19.44), 'text': ' on to level one character split. All right, so level one character splitting. Before we jump'}, {'timestamp': (19.44, 24.58), 'text': ' into that, I want to talk about the chunking commandment. Your goal is not to chunk for'}, {'timestamp': (24.58, 29.98), 'text': \" chunking's sake. Your goal is to get our data in a format where it can be retrieved for value later.\"}, {'timestamp': (0.0, 6.6), 'text': \" I'm placing so much emphasis on this point because it doesn't matter what your chunking strategy is if it doesn't serve your downstream task.\"}, {'timestamp': (6.72, 10.4), 'text': ' Keep that in mind as we keep going here. So level one character splitting.'}, {'timestamp': (10.4, 17.22), 'text': \" This is the most basic form of splitting and this is when you're gonna chunk up your text by a fixed static character length.\"}, {'timestamp': (17.22, 26.08), 'text': \" Let's talk about what that means. First the pros it's extremely simple and easy. The cons it's very rigid and doesn't take into account the structure of your text.\"}, {'timestamp': (0.0, 4.32), 'text': \" and to be honest, I don't know anybody that does this in production. I don't. Let me know if you\"}, {'timestamp': (4.32, 8.22), 'text': \" do because I'm curious. The two concepts I want to talk about for this one, we're going to see\"}, {'timestamp': (8.22, 13.78), 'text': \" what chunk size is and we're going to talk about what chunk overlap is, but let's use examples to\"}, {'timestamp': (13.78, 18.42), 'text': ' explain those. For our text, this is the text I would like to chunk up. It is an example text for'}, {'timestamp': (18.42, 23.06), 'text': ' this exercise. Cool, we got that one. So before we talk about packages that help us do this'}, {'timestamp': (23.06, 27.52), 'text': ' automatically, I want to show you how we do this manually first just so you can appreciate the'}, {'timestamp': (0.0, 4.02), 'text': \" nuances for how cool some of this stuff is. So in order to create our chunks, I'm going to first\"}, {'timestamp': (4.02, 10.38), 'text': ' create an empty list of chunks. Now my chunk size is going to be 35. This stands for 35 characters.'}, {'timestamp': (10.68, 16.36), 'text': \" So I'm going to count 35 characters in, count that as chunk one. Next 35 characters is chunk two.\"}, {'timestamp': (16.74, 20.96), 'text': \" I'm going to run through this. I'm going to create a range and the range length is going to be the\"}, {'timestamp': (20.96, 25.86), 'text': \" length of my text that I have up above here. And then the iteration step or the step we're going\"}, {'timestamp': (0.0, 4.46), 'text': \" to take is the chunk size. So we're going to skip ahead every 35 characters. I'm going to get the\"}, {'timestamp': (4.46, 9.08), 'text': \" chunk. I'm going to unpend it and let's see what our chunks are. This is the text I would like to\"}, {'timestamp': (9.08, 17.06), 'text': ' unk up. It is an example text for this exercise. First off, congratulations. You just did your'}, {'timestamp': (17.06, 22.1), 'text': ' first chunking exercise. Do you feel like a language model practitioner yet? I sure do.'}, {'timestamp': (22.42, 26.04), 'text': \" Let's keep on going here. There's a couple of problems with this. This is the text I would\"}, {'timestamp': (0.0, 6.04), 'text': \" like to chuck, well, it's stuck in the middle of the word. That's no good. How are we supposed to\"}, {'timestamp': (6.04, 10.44), 'text': ' know how long this 35 character length? We could change this to 40. And then all of a sudden,'}, {'timestamp': (10.52, 14.02), 'text': \" this goes a little bit further. But dang, again, we have it one more time. That's giving us a hard\"}, {'timestamp': (14.02, 19.2), 'text': \" time. That's not too good. So yes, it's quick. Yes, it's simple. But we need to fix this problem.\"}, {'timestamp': (19.9, 23.92), 'text': \" Now, before we move on to level two, I want to talk about Langchain's character splitter.\"}, {'timestamp': (24.24, 28.0), 'text': \" So their character splitter, that's going to do the exact same thing for us. But it's going to\"}, {'timestamp': (0.0, 4.02), 'text': \" be through a Langchain one-liner. And the way that you do that is you're going to initialize\"}, {'timestamp': (4.02, 8.32), 'text': \" a character text splitter. You're going to tell it how much of a chunk size you want. You're going\"}, {'timestamp': (8.32, 12.34), 'text': \" to tell it how much of a chunk overlap you want. We'll talk about that in a second. And when you\"}, {'timestamp': (12.34, 17.46), 'text': \" do a blank or an empty string as a separator, we'll talk about that in a second too, that means\"}, {'timestamp': (17.46, 21.6), 'text': \" it's just going to split by character. And Langchain by default, they will strip the white space. And\"}, {'timestamp': (21.6, 26.16), 'text': \" so they'll remove the spaces on the end of your chunks. I don't want them to do that quite yet.\"}, {'timestamp': (0.0, 0.84), 'text': \" So I'm going to say false.\"}, {'timestamp': (1.12, 3.4), 'text': ' So this is, we just made our character splitter.'}, {'timestamp': (3.58, 5.58), 'text': \" Now we're actually going to go and split the documents.\"}, {'timestamp': (5.7, 10.76), 'text': \" And so I'm going to say dot create documents and this create documents function, it expects\"}, {'timestamp': (10.76, 11.5), 'text': ' a list.'}, {'timestamp': (11.72, 16.2), 'text': ' And because our string up above is just a plain old string, I need to wrap it in a list'}, {'timestamp': (16.2, 16.52), 'text': ' right here.'}, {'timestamp': (16.76, 18.1), 'text': \" Let's go do that text splitter.\"}, {'timestamp': (18.6, 21.24), 'text': ' So now what we get returned is we get three chunks.'}, {'timestamp': (21.4, 23.64), 'text': ' However, they look a little bit different than plain strings.'}, {'timestamp': (23.82, 26.62), 'text': \" The reason why is because they're actually a document object.\"}, {'timestamp': (0.0, 6.7), 'text': ' Now in Langchain, a document object is, well, an object that holds a string, but it can also hold'}, {'timestamp': (6.7, 10.96), 'text': ' metadata, which is important for us to understand when we start doing more advanced techniques.'}, {'timestamp': (11.34, 15.96), 'text': \" So don't get scared. Documents still have our string and they're held within page content.\"}, {'timestamp': (16.5, 20.04), 'text': \" This is the text I would like to chunk. It's the same thing that we had up above.\"}, {'timestamp': (20.7, 24.94), 'text': \" Cool. That makes sense. So let's talk about overlaps and separators here. So I'm going to\"}, {'timestamp': (24.94, 28.52), 'text': \" make the character splitter again. It's going to be 35 characters, but this time we're going to\"}, {'timestamp': (0.0, 6.46), 'text': ' have a chunk overlap of four. Now what this means is that the tail end of chunk number one is going'}, {'timestamp': (6.46, 11.54), 'text': ' to overlap a little bit with the head or the beginning of chunk number two. And the overlap'}, {'timestamp': (11.54, 15.98), 'text': ' is going to be four characters. So the last four characters of chunk one will be the same four'}, {'timestamp': (15.98, 20.12), 'text': \" characters of chunk two. Let's see what this looks like here. And again, I'm just going to make these.\"}, {'timestamp': (20.7, 25.58), 'text': ' And what we have is this is the chunk or this is the text that I would like to, we still have the'}, {'timestamp': (0.0, 5.22), 'text': ' first same split, the chunk size is the same, but check this out. Now the first four characters of'}, {'timestamp': (5.22, 10.06), 'text': ' chunk number two are the same four characters as chunk number one because we have that chunk'}, {'timestamp': (10.06, 14.28), 'text': ' overlap. All right. Now, when I was getting ready for this exercise, I thought I remember a tool'}, {'timestamp': (14.28, 18.74), 'text': ' that actually visually showed you different chunking techniques with highlights of the'}, {'timestamp': (18.74, 22.94), 'text': \" different chunks, but I couldn't find it. So I ended up making a tool and that tool is called\"}, {'timestamp': (22.94, 26.98), 'text': \" chunkviz.com. And this is just a quick snippet of it, but I want to show you this while we're on\"}, {'timestamp': (0.0, 4.38), 'text': ' the topic. So chunk number one was this first beginning part, and then we have the overlap,'}, {'timestamp': (4.68, 7.72), 'text': ' and then we have chunk number two, and then the overlap, and then chunk number three.'}, {'timestamp': (8.2, 12.46), 'text': \" But if you want to try this out for yourself, it's kind of cool. You can go to chunkviz.com,\"}, {'timestamp': (12.78, 19.1), 'text': \" and then what you'll get here is you'll get a tool where you can input different text and play with\"}, {'timestamp': (19.1, 23.22), 'text': \" your different chunk sizes that we'd get out. So let's go back up and let's grab the text that we\"}, {'timestamp': (23.22, 28.24), 'text': \" had. I'm going to bring this over here. I'm just going to replace this. And so you can see here\"}, {'timestamp': (0.0, 2.7), 'text': ' that right now our chunk size is one with no overlap.'}, {'timestamp': (2.84, 3.94), 'text': \" That doesn't make any sense.\"}, {'timestamp': (4.08, 6.58), 'text': \" Although it's visually cool, it won't do any good for us\"}, {'timestamp': (6.58, 8.74), 'text': ' because we have 83 chunks here.'}, {'timestamp': (9.1, 10.98), 'text': ' What are you going to do with 83 one-character chunks?'}, {'timestamp': (11.06, 11.4), 'text': \" I don't know.\"}, {'timestamp': (11.44, 12.32), 'text': \" I'm not going to do much with that.\"}, {'timestamp': (12.62, 14.24), 'text': ' But as you start to increase this number,'}, {'timestamp': (14.5, 16.08), 'text': ' you can see that these different chunk sizes'}, {'timestamp': (16.08, 17.2), 'text': ' are going to start to get bigger.'}, {'timestamp': (17.48, 19.12), 'text': \" So we're going to take this all the way up,\"}, {'timestamp': (19.38, 20.52), 'text': ' and as we go through it,'}, {'timestamp': (20.52, 22.84), 'text': \" I'm going to put it at what we had it before, which is 35.\"}, {'timestamp': (23.22, 25.18), 'text': ' So this is a text that I would like to,'}, {'timestamp': (25.58, 27.28), 'text': ' and you see it ends right in the middle of the CH,'}, {'timestamp': (27.42, 28.42), 'text': ' just like we had beforehand.'}, {'timestamp': (0.0, 1.86), 'text': ' Let me zoom in just a little bit more here.'}, {'timestamp': (1.86, 4.0), 'text': ' It ends just in the CH that we had beforehand.'}, {'timestamp': (4.0, 6.04), 'text': ' Now, if I start to introduce overlap,'}, {'timestamp': (6.04, 7.34), 'text': ' you can see that now we have a little bit'}, {'timestamp': (7.34, 8.52), 'text': ' of an overlapping section.'}, {'timestamp': (8.52, 11.94), 'text': ' So we had chunk over size or chunk overlap before.'}, {'timestamp': (11.94, 13.88), 'text': ' This is the text that I would like to.'}, {'timestamp': (13.88, 15.82), 'text': ' So with chunk one still ends here,'}, {'timestamp': (15.82, 17.86), 'text': \" but now there's the overlap that comes with it.\"}, {'timestamp': (17.86, 19.92), 'text': \" So what's cool is you can switch this around yourself.\"}, {'timestamp': (19.92, 22.72), 'text': ' You see above a certain mark, it ends up being,'}, {'timestamp': (22.72, 24.12), 'text': ' I wanna get rid of this overlap.'}, {'timestamp': (24.12, 26.46), 'text': ' Over a certain mark, well, this chunk size is bigger'}, {'timestamp': (26.46, 27.84), 'text': ' than the document that we have.'}, {'timestamp': (27.84, 29.24), 'text': \" So it's just gonna encapsulate everything,\"}, {'timestamp': (0.0, 3.06), 'text': ' but you can go to chunkviz.com and go play around with this.'}, {'timestamp': (3.06, 5.32), 'text': \" We'll take a look at one more of these sections\"}, {'timestamp': (5.32, 6.62), 'text': ' in a minute here.'}, {'timestamp': (6.62, 8.02), 'text': \" Cool, so let's go down there.\"}, {'timestamp': (8.02, 10.58), 'text': ' Those are characters, those are separators, fabulous.'}, {'timestamp': (10.58, 13.12), 'text': ' And so the next thing I wanna talk about is separator.'}, {'timestamp': (13.12, 16.28), 'text': ' So beforehand, we just had a blank string as a separator,'}, {'timestamp': (16.28, 18.62), 'text': \" which means you're gonna split by character, right?\"}, {'timestamp': (18.62, 21.26), 'text': ' However, if we specify another separator,'}, {'timestamp': (21.26, 23.38), 'text': \" in this case, I'm gonna do ch,\"}, {'timestamp': (23.38, 25.28), 'text': \" well, let's see what that does here.\"}, {'timestamp': (25.28, 27.5), 'text': ' This is the text that I would like to,'}, {'timestamp': (0.0, 2.58), 'text': ' And you see here that the CH is missing'}, {'timestamp': (2.58, 4.38), 'text': ' and the space is missing because I removed'}, {'timestamp': (4.38, 6.7), 'text': \" the strip white space and so it's default true.\"}, {'timestamp': (6.7, 7.54), 'text': \" So that's gone.\"}, {'timestamp': (7.54, 10.2), 'text': ' And you see that this word is supposed to be chunk up,'}, {'timestamp': (10.2, 12.92), 'text': \" but it's not anymore because we removed the CH.\"}, {'timestamp': (12.92, 14.92), 'text': \" It's not too helpful when we do CH.\"}, {'timestamp': (14.92, 16.38), 'text': ' You could do the letter E if you wanted'}, {'timestamp': (16.38, 17.62), 'text': \" and it's gonna be a little bit different.\"}, {'timestamp': (17.62, 19.94), 'text': \" Either way, unless you know exactly what you're doing,\"}, {'timestamp': (19.94, 21.98), 'text': \" I wouldn't suggest messing around with the separator\"}, {'timestamp': (21.98, 24.04), 'text': ' to try to get better results here.'}, {'timestamp': (24.04, 26.48), 'text': \" All right, so that's the Lang chain side of the house.\"}, {'timestamp': (26.48, 27.34), 'text': ' The next one I wanna show you'}, {'timestamp': (27.34, 29.62), 'text': ' is the Lama index side of the house.'}, {'timestamp': (0.0, 2.56), 'text': ' So they have what they call a sentence splitter,'}, {'timestamp': (2.56, 4.94), 'text': \" but also I'm gonna use their simple directory reader\"}, {'timestamp': (4.94, 7.98), 'text': ' because this time, instead of just using a static string'}, {'timestamp': (7.98, 9.0), 'text': ' that I put in the code,'}, {'timestamp': (9.0, 11.52), 'text': \" we're actually gonna load some essays from a directory.\"}, {'timestamp': (11.52, 13.28), 'text': \" So I'm gonna make the sentence splitter,\"}, {'timestamp': (13.28, 15.44), 'text': \" and this time I'm gonna have a chunk size of 200\"}, {'timestamp': (15.44, 17.64), 'text': ' and a chunk overlap of 15.'}, {'timestamp': (17.64, 19.46), 'text': \" And then I'm gonna load up some essays.\"}, {'timestamp': (19.46, 22.2), 'text': \" So my input files, I'm just gonna load up one essay.\"}, {'timestamp': (22.2, 23.88), 'text': ' This data is also in the repo.'}, {'timestamp': (23.88, 25.04), 'text': ' So if you go and clone this repo,'}, {'timestamp': (25.04, 26.92), 'text': ' you can also get this data pretty easily.'}, {'timestamp': (26.92, 28.18), 'text': ' This is gonna be a Paul Graham essay'}, {'timestamp': (0.0, 2.2), 'text': ' and this is gonna be his MIT essay.'}, {'timestamp': (2.2, 5.06), 'text': \" So we can go check out Paul Graham's MIT essay,\"}, {'timestamp': (5.06, 6.4), 'text': ' and we can see what it is right here.'}, {'timestamp': (6.4, 7.38), 'text': ' You can go and read it.'}, {'timestamp': (7.38, 9.16), 'text': ' I have it loaded up for you.'}, {'timestamp': (9.16, 10.18), 'text': \" So let's go ahead and load this.\"}, {'timestamp': (10.18, 11.42), 'text': ' Now we have our documents,'}, {'timestamp': (11.42, 14.32), 'text': ' but this document is just going to be,'}, {'timestamp': (14.32, 15.8), 'text': ' let me show you here.'}, {'timestamp': (16.62, 17.46), 'text': \" Let's check out,\"}, {'timestamp': (20.06, 21.48), 'text': \" let's check how long this document is,\"}, {'timestamp': (21.48, 23.0), 'text': \" and it's just one big long document\"}, {'timestamp': (23.0, 25.76), 'text': ' because the entire essay was loaded into this variable here.'}, {'timestamp': (25.76, 26.96), 'text': ' However, we wanna chunk it up.'}, {'timestamp': (26.96, 29.22), 'text': \" And the way we're gonna chunk it up is with our splitter,\"}, {'timestamp': (0.0, 2.38), 'text': \" We're gonna say get nodes from documents.\"}, {'timestamp': (2.38, 5.06), 'text': \" Now you may be asking, hey Greg, wait, what's a node?\"}, {'timestamp': (5.06, 7.34), 'text': \" Well, Llama Index's nomenclature for a chunk\"}, {'timestamp': (7.34, 10.1), 'text': ' or a subsection of a doc is gonna be nodes.'}, {'timestamp': (10.1, 12.22), 'text': \" And so that's what we're gonna get here, same thing.\"}, {'timestamp': (12.22, 14.8), 'text': ' So now that we have our nodes, I wanna take a look at one.'}, {'timestamp': (14.8, 16.56), 'text': ' So as you can see, this node is quite long'}, {'timestamp': (16.56, 18.66), 'text': \" based off of the amount of text that's in here,\"}, {'timestamp': (18.66, 20.02), 'text': \" but there's some really cool information\"}, {'timestamp': (20.02, 21.22), 'text': ' that comes out of the box.'}, {'timestamp': (21.22, 23.34), 'text': ' So first of all, this node has an ID,'}, {'timestamp': (23.34, 24.94), 'text': \" and we can see here that it's a text node\"}, {'timestamp': (24.94, 26.56), 'text': ' because they delineate from other nodes.'}, {'timestamp': (26.56, 29.94), 'text': ' So we have a node ID, so we can go and use it later'}, {'timestamp': (0.0, 4.08), 'text': ' deal with its uniqueness and then we also have some metadata so we can tell where it came from'}, {'timestamp': (5.04, 10.8), 'text': ' we can have last modified day etc but then one of the other parts i like a lot is going to be around'}, {'timestamp': (10.8, 15.84), 'text': ' node relationships so here we have a relationships key and we can take a look at other relationships'}, {'timestamp': (15.84, 21.6), 'text': ' this node has so node relationship we can see the source node that it came from but then also we can'}, {'timestamp': (21.6, 27.36), 'text': ' take a look at the next node so what node actually comes next and this is really helpful for when you'}, {'timestamp': (0.0, 5.34), 'text': \" start doing some traversing across your documents. But either way, I won't go too far into that one.\"}, {'timestamp': (5.5, 10.28), 'text': \" Well, congratulations. We just finished level number one. Let's head off to level number two,\"}, {'timestamp': (10.6, 15.68), 'text': \" recursive character text splitting. So you'll notice that in our previous level one, we split\"}, {'timestamp': (15.68, 21.44), 'text': ' by a static number of chunk sizes each time. So 35 characters by 35 characters. However,'}, {'timestamp': (21.48, 25.78), 'text': \" there's other chunking mechanisms that will actually look at the physical structure of your\"}, {'timestamp': (0.0, 3.62), 'text': ' text and it will infer what type of chunk sizes you should have.'}, {'timestamp': (3.86, 8.12), 'text': ' So instead of specifying by 35 characters, you can then specify, well,'}, {'timestamp': (8.22, 11.24), 'text': ' give me every new line or give me every double new line.'}, {'timestamp': (11.5, 13.5), 'text': \" And that's what recursive character text splitter does.\"}, {'timestamp': (13.82, 17.32), 'text': \" So what it's going to do is it's actually going to have a series of separators\"}, {'timestamp': (17.56, 20.84), 'text': \" and it's going to recursively go through these documents and it's going to start\"}, {'timestamp': (20.84, 24.36), 'text': \" at its first separator and it's going to first chunk up by every new double\"}, {'timestamp': (24.36, 27.26), 'text': ' new line that you have here for any chunks that are still too large.'}, {'timestamp': (0.0, 3.74), 'text': \" after that first iteration, it's going to go to its next separator.\"}, {'timestamp': (3.9, 5.18), 'text': \" And that's going to be just new lines.\"}, {'timestamp': (5.44, 6.52), 'text': \" And then it's going to go to spaces.\"}, {'timestamp': (6.52, 7.82), 'text': \" And then it's going to go to characters.\"}, {'timestamp': (8.3, 11.0), 'text': \" So now I don't need to specify 35 characters or 200.\"}, {'timestamp': (11.38, 13.22), 'text': ' All I can do is I can just pass it my text'}, {'timestamp': (13.22, 15.64), 'text': ' and it will infer what the structure should be.'}, {'timestamp': (15.94, 17.76), 'text': ' Now, the cool part about this one is'}, {'timestamp': (17.76, 20.5), 'text': ' if you think about how you write text,'}, {'timestamp': (20.68, 23.48), 'text': \" you're probably going to separate your ideas by paragraphs.\"}, {'timestamp': (23.9, 26.24), 'text': ' And those paragraphs are separated by double new lines.'}, {'timestamp': (26.58, 28.12), 'text': ' This method takes advantage of that fact.'}, {'timestamp': (0.0, 5.42), 'text': ' And so we can start to be smart about which separators that we use to take advantage of how humans naturally write a text.'}, {'timestamp': (5.62, 5.84), 'text': ' All right.'}, {'timestamp': (6.0, 7.58), 'text': \" So let's go ahead and let's check this out.\"}, {'timestamp': (7.76, 9.28), 'text': \" We're going to do Langchain TextSplitter.\"}, {'timestamp': (9.44, 11.48), 'text': \" We're going to do the recursive character text splitter.\"}, {'timestamp': (11.78, 15.5), 'text': \" Now, again, I'm going to take some text, but this one's going to be a little bit longer that we have here.\"}, {'timestamp': (15.66, 15.78), 'text': ' Right.'}, {'timestamp': (16.18, 17.58), 'text': \" And so let's do that text.\"}, {'timestamp': (17.68, 19.98), 'text': \" And then let's put it through our recursive character text splitter.\"}, {'timestamp': (20.24, 22.9), 'text': ' This is going to be a 65 character limit.'}, {'timestamp': (23.16, 24.1), 'text': \" We're going to pass it through.\"}, {'timestamp': (24.26, 27.3), 'text': ' And then all of a sudden you can see that we get a whole bunch of different chunks here.'}, {'timestamp': (27.6, 28.86), 'text': \" Now, I'm not even sure how many chunks.\"}, {'timestamp': (0.0, 1.34), 'text': \" Let's see how many we actually have.\"}, {'timestamp': (1.52, 3.7), 'text': ' We have 16 different chunks, all right?'}, {'timestamp': (4.38, 6.36), 'text': ' So with that, one of the most important things'}, {'timestamp': (6.36, 10.1), 'text': \" I didn't understand about the for end of sentence,\"}, {'timestamp': (10.18, 12.52), 'text': \" end of sentence day, you can see here that what's cool\"}, {'timestamp': (12.52, 15.1), 'text': \" is that we're ending on words quite often.\"}, {'timestamp': (15.1, 17.52), 'text': \" And that's because words have spaces in between them.\"}, {'timestamp': (17.68, 20.18), 'text': ' And that is one of the separators that we try out.'}, {'timestamp': (20.5, 21.48), 'text': ' So this is cool.'}, {'timestamp': (21.6, 23.34), 'text': \" Now we're not splitting in between words anymore.\"}, {'timestamp': (23.7, 26.38), 'text': ' However, we are starting to split in between sentences.'}, {'timestamp': (26.98, 28.18), 'text': \" That's not so good.\"}, {'timestamp': (28.26, 28.88), 'text': \" That's not so fun.\"}, {'timestamp': (0.56, 4.64), 'text': ' One of the ways that we can combat that is we can increase the chunk size'}, {'timestamp': (4.64, 8.54), 'text': ' because our hypothesis is that if we increase the chunk size,'}, {'timestamp': (8.68, 11.58), 'text': ' we can start to take advantage of the paragraph splits a little bit more.'}, {'timestamp': (11.86, 12.0), 'text': ' All right.'}, {'timestamp': (12.12, 15.06), 'text': \" So now what I'm going to do is I'm going to increase the chunk size to 450,\"}, {'timestamp': (15.28, 16.76), 'text': ' still chunk overlap of zero.'}, {'timestamp': (16.86, 17.78), 'text': \" And let's see what we have here.\"}, {'timestamp': (18.88, 21.64), 'text': \" One of the most important things I didn't understand about the world without a child\"}, {'timestamp': (21.64, 24.5), 'text': ' was the degree to which performances are super linear.'}, {'timestamp': (25.04, 25.3), 'text': ' Cool.'}, {'timestamp': (25.66, 26.58), 'text': \" So there's a period.\"}, {'timestamp': (27.46, 28.28), 'text': \" Here's a period.\"}, {'timestamp': (28.28, 29.38), 'text': \" And here's the end of it.\"}, {'timestamp': (0.0, 3.1), 'text': \" Now what's interesting is all three of those, let me scroll down to this vis again.\"}, {'timestamp': (3.44, 5.36), 'text': ' This is the same exact string that we have.'}, {'timestamp': (6.14, 8.24), 'text': ' Those are all three different paragraph breaks.'}, {'timestamp': (9.78, 10.78), 'text': \" That's pretty interesting.\"}, {'timestamp': (11.4, 15.04), 'text': ' So one of the important things to note here is look how these paragraphs are different lengths.'}, {'timestamp': (15.38, 20.42), 'text': \" If I use level one in the 35 character split or any character split, I'd start to cut in the middle of them.\"}, {'timestamp': (20.64, 23.02), 'text': ' But now I can get these paragraphs grouped together.'}, {'timestamp': (0.0, 5.46), 'text': ' and the hypothesis behind this method is that these paragraphs will hold semantically similar'}, {'timestamp': (5.46, 10.54), 'text': ' information that should be held together so the recursive character text splitter this is pretty'}, {'timestamp': (10.54, 15.22), 'text': \" awesome let's go take a look at what this looks like at chunkviz.com i'm just going to go ahead\"}, {'timestamp': (15.22, 20.8), 'text': ' and copy this go back to chunkviz.com let me put this text in there and you can see if we did the'}, {'timestamp': (20.8, 28.52), 'text': \" character splitter of 35 text we have 26 different chunks we're chunking all over the place this\"}, {'timestamp': (0.0, 3.48), 'text': \" doesn't make any sense but what I'm going to do is I'm going to actually scroll down I'm going to\"}, {'timestamp': (3.48, 9.6), 'text': \" select the recursive character text splitter and now I still have a chunk size of 35 but we're\"}, {'timestamp': (9.6, 13.52), 'text': ' going to increase this one so the first thing I want to show you is as I start to increase this'}, {'timestamp': (13.52, 20.02), 'text': \" notice how I go between 35 and 36 the split the first chunk here doesn't switch sizes that's\"}, {'timestamp': (20.02, 24.42), 'text': \" because it's looking for the space to actually split on and because there's a space here it\"}, {'timestamp': (24.42, 29.4), 'text': \" snaps to the nearest word this is why it's so cool so I'm going to increase this let's see when it\"}, {'timestamp': (0.0, 4.04), 'text': ' finally does split. And there it goes. It just jumped up to a degree. But either way, let me'}, {'timestamp': (4.04, 8.16), 'text': \" save you for this here. I'm going to select the chunk size. I think we wanted maybe like 450.\"}, {'timestamp': (8.86, 13.76), 'text': \" I forget what it was. Let me zoom out just a little bit. Let's go for maybe four, four. I mean,\"}, {'timestamp': (13.8, 17.78), 'text': \" either way, look at now we're splitting all these three different paragraphs and we can even increase\"}, {'timestamp': (17.78, 21.28), 'text': \" this size and it doesn't really do much for us. But all of a sudden if I go too big, well, it's\"}, {'timestamp': (21.28, 27.58), 'text': ' going to chunk the first two paragraphs together because this 493 is around the size of these two'}, {'timestamp': (0.0, 4.54), 'text': ' combined. Anyway, you can go and split this again. You can get so big that it finally takes over the'}, {'timestamp': (4.54, 9.12), 'text': \" third paragraph. All right. So that's it for level two. Congratulations, recursive character text\"}, {'timestamp': (9.12, 15.22), 'text': \" splitter. If I'm starting a project, this is my go-to splitter that I use each time. The ROI for\"}, {'timestamp': (15.22, 20.1), 'text': \" your energy to split up your docs is pretty awesome. It's a one-liner. It goes really quick.\"}, {'timestamp': (20.38, 24.26), 'text': \" There's no extra processing that's needed. So if you're looking for a go-to place to start,\"}, {'timestamp': (24.42, 28.22), 'text': \" I recommend with level two, the recursive character text splitter. Let's move on to level three,\"}, {'timestamp': (0.0, 5.4), 'text': \" document-specific splitting. So up until now, we've been splitting just regular old prose.\"}, {'timestamp': (5.8, 10.0), 'text': \" We've had some static strings and we have had some Paul Graham essays. But what if you have\"}, {'timestamp': (10.0, 15.18), 'text': \" Markdown? What if you have Python docs? What if you have JavaScript docs? There's probably a better\"}, {'timestamp': (15.18, 20.2), 'text': ' way to split on those because we can infer more about the document structure from special characters'}, {'timestamp': (20.2, 24.28), 'text': ' within those documents. Because when you have code, you start to have some code formatters'}, {'timestamp': (24.28, 29.22), 'text': ' and we can take advantage of those. All right. So the first one I want to look at here is for'}, {'timestamp': (0.0, 4.26), 'text': \" markdown. So we're still going to have something that's like the recursive character text splitter.\"}, {'timestamp': (4.58, 9.38), 'text': \" However, we have a lot more separators now. The reason why this is so cool is because let's take\"}, {'timestamp': (9.38, 14.34), 'text': \" a look at this first separator. It's a new line, and then it's going to be a pound symbol, which\"}, {'timestamp': (14.34, 20.38), 'text': ' indicates a heading within markdown. And this regex here means one pound symbol between one and'}, {'timestamp': (20.38, 26.06), 'text': \" six times. So it's a new line followed by a header, h1 through h6. Why would we do this? Well,\"}, {'timestamp': (0.0, 3.48), 'text': \" headers usually denote what you're gonna be talking about.\"}, {'timestamp': (3.48, 7.18), 'text': ' So this is a cool way to try to group similar items together.'}, {'timestamp': (7.18, 8.64), 'text': ' So these are the Langchain splitters.'}, {'timestamp': (8.64, 9.9), 'text': ' If you have your own different package,'}, {'timestamp': (9.9, 11.3), 'text': ' you might see other splitters.'}, {'timestamp': (11.3, 13.34), 'text': ' But if you wanna see the Langchain side of the house,'}, {'timestamp': (13.34, 15.28), 'text': ' you can head over to their GitHub'}, {'timestamp': (15.28, 17.64), 'text': ' and you can go find on LangchainLibs,'}, {'timestamp': (17.64, 20.18), 'text': ' LangchainLangchain, textsplitter.py,'}, {'timestamp': (20.18, 21.78), 'text': ' you can see that they have a markdown language'}, {'timestamp': (21.78, 24.2), 'text': ' and here are the splitters that they actually end up using.'}, {'timestamp': (24.2, 25.66), 'text': \" All right, so let's go ahead and load up\"}, {'timestamp': (25.66, 28.28), 'text': ' our Langchain markdown splitter.'}, {'timestamp': (0.0, 1.78), 'text': \" I'm gonna do a chunk size of 40,\"}, {'timestamp': (1.78, 3.76), 'text': ' which is, again, this is really, really small.'}, {'timestamp': (3.76, 5.38), 'text': ' My first go-to for chunk sizes'}, {'timestamp': (5.38, 8.76), 'text': ' is gonna be anywhere between 2,000, 4,000, 5,000,'}, {'timestamp': (8.76, 10.0), 'text': ' 6,000 different chunks.'}, {'timestamp': (10.0, 12.94), 'text': ' And as context lengths for language models get better,'}, {'timestamp': (12.94, 16.24), 'text': ' and their performance with large context gets even better,'}, {'timestamp': (16.24, 18.18), 'text': \" well, you're gonna start to increase this a whole lot\"}, {'timestamp': (18.18, 20.46), 'text': ' because it can infer what you want there, all right?'}, {'timestamp': (20.46, 21.52), 'text': \" So let's go ahead and do that.\"}, {'timestamp': (21.52, 23.52), 'text': \" And then, so here's some markdown text,\"}, {'timestamp': (23.52, 27.82), 'text': ' fun in California, H2 driving, blah, blah, blah, blah, blah.'}, {'timestamp': (27.82, 28.66), 'text': ' Cool.'}, {'timestamp': (0.0, 1.0), 'text': \" And let's split these up.\"}, {'timestamp': (1.5, 5.1), 'text': ' And so we can see here, the first document is fun in California driving.'}, {'timestamp': (5.5, 7.46), 'text': \" And what's cool is that it split it on these headers.\"}, {'timestamp': (7.96, 11.88), 'text': ' And so split on header here, et cetera, et cetera, split on header here, which is nice.'}, {'timestamp': (11.98, 12.48), 'text': \" That's markdown.\"}, {'timestamp': (12.78, 16.5), 'text': \" You can do this also for Python, but instead of using the markdown splitters, you're going\"}, {'timestamp': (16.5, 17.74), 'text': ' to want to have your own Python splitters.'}, {'timestamp': (18.18, 23.3), 'text': ' So again, Langchain is going to split on classes, functions, indent and functions.'}, {'timestamp': (23.3, 27.68), 'text': ' So these might be methods within your class, or you might have double new lines, new lines,'}, {'timestamp': (27.86, 28.78), 'text': ' spaces, and characters.'}, {'timestamp': (0.0, 2.82), 'text': ' They have a Python code text splitter.'}, {'timestamp': (2.82, 3.92), 'text': \" Let's go ahead and run this.\"}, {'timestamp': (3.92, 6.2), 'text': \" Let's see what we got, chunk size of 100.\"}, {'timestamp': (6.2, 9.3), 'text': ' And we scroll and we can see that we have,'}, {'timestamp': (10.5, 13.9), 'text': ' this whole class is enveloped in within one document,'}, {'timestamp': (13.9, 15.68), 'text': \" which is cool, because that's what we'd want.\"}, {'timestamp': (15.68, 18.2), 'text': ' But then we have p1 equals john equals person.'}, {'timestamp': (18.2, 21.82), 'text': ' We have this stuff and we have the ranges right here.'}, {'timestamp': (21.82, 25.74), 'text': ' I put this over on the chunkfiz.com'}, {'timestamp': (25.74, 27.44), 'text': ' and you can go ahead and you can throw this in there.'}, {'timestamp': (27.44, 28.94), 'text': ' You can go to Python splitters'}, {'timestamp': (0.0, 7.84), 'text': \" let's see how long this that was 100 let's go ahead and bump this up to 100. we have it at 100\"}, {'timestamp': (7.84, 13.2), 'text': \" right here and langchain i couldn't figure out how to undo the strip white space with them within\"}, {'timestamp': (13.2, 16.88), 'text': \" the javascript version to make this tool which is why i'm a little hesitant to show up but either\"}, {'timestamp': (16.88, 21.44), 'text': \" way you can see that we're splitting right there that's what we'd want nice all right same thing\"}, {'timestamp': (21.44, 27.2), 'text': ' we have javascript we also have just a bunch of different separators that we have here this is'}, {'timestamp': (0.0, 4.4), 'text': \" very similar but in this case we're going to do our recursive character text splitter again but\"}, {'timestamp': (4.4, 8.32), 'text': \" now we're going to specify which language we wanted to split by so here's our text\"}, {'timestamp': (8.32, 13.52), 'text': \" recursive character text splitter dot from language this time and we're going to do a language which\"}, {'timestamp': (13.52, 18.56), 'text': \" is going to be language.js chunk size 65 let's go through there let's do it and then all of a sudden\"}, {'timestamp': (18.56, 25.6), 'text': ' we split up our javascript code as well cool those are all strings those are all pretty easy because'}, {'timestamp': (0.0, 3.2), 'text': ' because they might just be in.txt files,'}, {'timestamp': (3.2, 5.6), 'text': ' which is simple enough for us to work with.'}, {'timestamp': (5.6, 7.98), 'text': ' However, what if you have PDFs?'}, {'timestamp': (7.98, 10.58), 'text': ' Everyone loves talking about PDFs.'}, {'timestamp': (10.58, 13.92), 'text': ' They especially love talking about pulling tables from PDFs.'}, {'timestamp': (13.92, 15.82), 'text': \" This is because there's a lot of old school industries\"}, {'timestamp': (15.82, 18.12), 'text': ' that still put information inside of PDF.'}, {'timestamp': (18.12, 19.18), 'text': ' So when it comes to chunking,'}, {'timestamp': (19.18, 21.54), 'text': \" you're not only just gonna split text,\"}, {'timestamp': (21.54, 24.14), 'text': ' but you just wanna pull out all the different elements'}, {'timestamp': (24.14, 25.28), 'text': ' within your documents.'}, {'timestamp': (25.28, 27.58), 'text': ' And some of those might actually be tables.'}, {'timestamp': (27.58, 29.84), 'text': ' It might be pictures, it might be graphs.'}, {'timestamp': (0.0, 1.24), 'text': \" Let's take a look at how we do this here.\"}, {'timestamp': (1.7, 4.12), 'text': \" So the way I'm going to do this is I'm going to load up a PDF right here.\"}, {'timestamp': (4.18, 6.32), 'text': ' And this is just going to be a Salesforce financial PDF.'}, {'timestamp': (6.78, 11.96), 'text': ' I went over and I just pulled one of their random PDFs that we have here so I can have a table.'}, {'timestamp': (12.24, 12.44), 'text': ' All right.'}, {'timestamp': (12.98, 13.82), 'text': ' So we have this.'}, {'timestamp': (14.04, 16.74), 'text': \" And the way I'm going to do it is I'm actually going to do it via unstructured.\"}, {'timestamp': (17.06, 19.02), 'text': ' Now unstructured is another library.'}, {'timestamp': (19.38, 20.56), 'text': ' We can go check them out.'}, {'timestamp': (20.64, 22.34), 'text': \" They're at unstructured.io.\"}, {'timestamp': (22.8, 24.34), 'text': ' We get your data LLM ready.'}, {'timestamp': (24.34, 28.56), 'text': ' So they have some really cool parsers that you can use, which is going to be advantageous'}, {'timestamp': (0.0, 2.12), 'text': ' for when you start to get more complicated data types'}, {'timestamp': (2.12, 4.2), 'text': ' or your data gets a whole lot more messier.'}, {'timestamp': (4.5, 6.92), 'text': ' So for example, if you had a million PDFs'}, {'timestamp': (6.92, 9.3), 'text': ' that you somehow needed to get into a structured form,'}, {'timestamp': (9.66, 11.14), 'text': ' unstructured would be who you went with that.'}, {'timestamp': (11.22, 12.78), 'text': ' Not a sponsored video by them at all.'}, {'timestamp': (13.46, 15.74), 'text': \" All right, so we're gonna load up two elements by them.\"}, {'timestamp': (15.86, 18.44), 'text': \" It's gonna be partition PDF and then elements to JSON.\"}, {'timestamp': (18.72, 19.96), 'text': \" We're gonna get our PDF.\"}, {'timestamp': (20.32, 22.36), 'text': ' And if we take a look at this Salesforce PDF here,'}, {'timestamp': (22.64, 23.74), 'text': ' you can see that we have a few'}, {'timestamp': (23.74, 25.18), 'text': ' kind of just like notes or whatever.'}, {'timestamp': (25.3, 26.14), 'text': ' Then we have some paragraphs,'}, {'timestamp': (26.36, 27.22), 'text': ' but then we have this table'}, {'timestamp': (27.22, 29.52), 'text': \" and this is where I'm gonna start to place more emphasis on.\"}, {'timestamp': (0.0, 2.0), 'text': \" And then we're going to do partition PDF.\"}, {'timestamp': (2.0, 5.0), 'text': \" We're going to give it the file name and then we're going to give it some unstructured\"}, {'timestamp': (5.0, 6.0), 'text': ' helpers.'}, {'timestamp': (6.0, 7.76), 'text': ' This is just a little bit of config for them.'}, {'timestamp': (7.76, 9.24), 'text': \" So if we take a look here, let's load it up.\"}, {'timestamp': (9.24, 10.88), 'text': \" Let's see what elements I actually found.\"}, {'timestamp': (10.88, 12.12), 'text': ' And so I found a whole bunch of them here.'}, {'timestamp': (12.12, 14.0), 'text': ' It looks like we just have some narrative text.'}, {'timestamp': (14.0, 16.94), 'text': \" This is going to be your regular text, but then we have this table and that's actually\"}, {'timestamp': (16.94, 18.54), 'text': ' what I want to double click on.'}, {'timestamp': (18.54, 22.18), 'text': \" And so what I'm going to do is I'm just going to go grab the fourth from the last element,\"}, {'timestamp': (22.18, 25.14), 'text': ' which is going to be this table right here at one, two, three, four.'}, {'timestamp': (25.14, 27.64), 'text': \" And let's look at what the HTML looks like.\"}, {'timestamp': (27.64, 29.32), 'text': ' So here we have the HTML.'}, {'timestamp': (0.0, 2.94), 'text': ' And now you might be saying, well, Greg, how come the HTML is important?'}, {'timestamp': (2.94, 5.4), 'text': \" Why won't you just want to read it like a table?\"}, {'timestamp': (5.4, 8.28), 'text': ' Well, tables are easy for us to read.'}, {'timestamp': (8.28, 10.44), 'text': \" They're not so easy for the language model to read.\"}, {'timestamp': (10.44, 16.26), 'text': ' However, the language model has been trained on HTML tables, not only HTML, but also Markdown.'}, {'timestamp': (16.26, 19.82), 'text': ' But in this case, I want to pull out the HTML table because the language model is going'}, {'timestamp': (19.82, 22.52), 'text': ' to be able to make more sense of this than I can.'}, {'timestamp': (22.52, 27.7), 'text': \" So when I pass my data to the language model, I'm going to pass the HTML or you can pass\"}, {'timestamp': (27.7, 29.48), 'text': ' Markdown, whatever works for you.'}, {'timestamp': (0.0, 1.24), 'text': ' If you want to see what this actually looks like,'}, {'timestamp': (1.24, 3.08), 'text': ' you can head over to an HTML viewer'}, {'timestamp': (3.08, 5.64), 'text': ' and you can see what the unstructured actually pulled out,'}, {'timestamp': (5.64, 6.94), 'text': ' which is pretty cool.'}, {'timestamp': (6.94, 7.82), 'text': ' Nice.'}, {'timestamp': (7.82, 10.06), 'text': \" So that's how you do tables within PDFs.\"}, {'timestamp': (10.06, 13.76), 'text': \" But now let's say you have images within PDFs\"}, {'timestamp': (13.76, 15.4), 'text': ' or maybe you have images elsewhere.'}, {'timestamp': (15.4, 17.14), 'text': ' How are you gonna take advantage of those?'}, {'timestamp': (17.14, 18.48), 'text': ' How are you gonna extract those?'}, {'timestamp': (18.48, 20.18), 'text': \" Well, let's take a look at how you do that.\"}, {'timestamp': (20.18, 22.38), 'text': \" And I'm gonna use unstructured once more this time.\"}, {'timestamp': (22.38, 24.54), 'text': \" I'm gonna do their partition PDF,\"}, {'timestamp': (24.54, 25.68), 'text': ' which is the same as last time.'}, {'timestamp': (25.68, 29.76), 'text': ' And here I have a visual fine tuning paper.'}, {'timestamp': (0.0, 0.94), 'text': ' You can go and check this out.'}, {'timestamp': (0.94, 2.58), 'text': \" Let's go look at the archive one.\"}, {'timestamp': (2.58, 4.82), 'text': \" I'm gonna download the PDF just so you can see it\"}, {'timestamp': (4.82, 7.7), 'text': \" because there's this wacky photo up at the front.\"}, {'timestamp': (7.7, 9.12), 'text': \" All right, so here's the same one.\"}, {'timestamp': (9.12, 11.8), 'text': \" I'm gonna load up that page that I had,\"}, {'timestamp': (11.8, 15.54), 'text': \" and then I'm gonna get the partition PDF ready for me.\"}, {'timestamp': (15.54, 20.18), 'text': \" And this time I'm gonna do extract images in PDF equals true.\"}, {'timestamp': (20.18, 21.86), 'text': \" So now it's gonna extract all the parts for me,\"}, {'timestamp': (21.86, 23.28), 'text': ' which is the chunking process,'}, {'timestamp': (23.28, 25.7), 'text': \" but it's gonna treat the images separately, which is nice.\"}, {'timestamp': (25.7, 27.94), 'text': ' Also infer table structure, blah, blah, blah.'}, {'timestamp': (27.94, 29.04), 'text': \" Let's go from there.\"}, {'timestamp': (0.0, 2.94), 'text': ' image output directory path, this is in this repo.'}, {'timestamp': (2.94, 5.24), 'text': \" So you don't have to reload the images if you don't want to.\"}, {'timestamp': (5.24, 6.7), 'text': \" But either way, I'm gonna load those up\"}, {'timestamp': (6.7, 7.72), 'text': \" and I don't wanna make you wait.\"}, {'timestamp': (7.72, 9.54), 'text': ' This does take just a little bit of time.'}, {'timestamp': (9.54, 10.74), 'text': ' So let me come back to you.'}, {'timestamp': (10.74, 12.58), 'text': ' One minute later.'}, {'timestamp': (12.58, 14.24), 'text': ' Awesome, so that just finished uploading for us.'}, {'timestamp': (14.24, 15.2), 'text': \" Let's take a look here.\"}, {'timestamp': (15.2, 16.68), 'text': \" So I know it's kind of small on the screen,\"}, {'timestamp': (16.68, 20.2), 'text': ' but they found, looks like 15 or so different images'}, {'timestamp': (20.2, 21.76), 'text': ' or 16 different images on here.'}, {'timestamp': (21.76, 24.92), 'text': ' And that was all extracted from the PDF that I supplied.'}, {'timestamp': (24.92, 26.5), 'text': ' Now the interesting thing about this is'}, {'timestamp': (26.5, 28.94), 'text': ' how are we gonna make those images useful, right?'}, {'timestamp': (0.0, 1.4), 'text': ' we would need to take an embedding of them'}, {'timestamp': (1.4, 3.74), 'text': \" because we're probably gonna do semantic search later.\"}, {'timestamp': (4.18, 7.14), 'text': \" However, embedding models usually don't cross paths\"}, {'timestamp': (7.14, 8.82), 'text': ' between text and images,'}, {'timestamp': (9.22, 10.8), 'text': \" meaning there's embedding models for images\"}, {'timestamp': (10.8, 12.48), 'text': \" and there's embedding models for text,\"}, {'timestamp': (12.6, 15.62), 'text': \" but generally their vector length isn't gonna line up.\"}, {'timestamp': (15.88, 17.48), 'text': \" And if you don't have the same model for each one,\"}, {'timestamp': (17.74, 19.12), 'text': ' doing similarity search between the two'}, {'timestamp': (19.12, 20.44), 'text': ' may give you a hard time.'}, {'timestamp': (20.64, 22.22), 'text': ' Yes, I know for all the perfectionists out there,'}, {'timestamp': (22.42, 23.84), 'text': ' there is something called the clip model,'}, {'timestamp': (23.84, 26.5), 'text': ' which is gonna do embeddings for both images and text'}, {'timestamp': (26.5, 27.62), 'text': ' so you can take advantage of them.'}, {'timestamp': (0.0, 4.56), 'text': \" However, the tech is still not quite there and I haven't found the same performance with clip that I have with other ones\"}, {'timestamp': (4.56, 6.96), 'text': \" So I'm going to show you a different method here\"}, {'timestamp': (6.96, 12.22), 'text': \" I will note this to say that in the future and when you're watching this or maybe really good models that do both of these\"}, {'timestamp': (12.22, 17.48), 'text': \" If it's true, then I would take advantage of those instead of the method I show you but either way\"}, {'timestamp': (17.48, 18.64), 'text': \" Let's take a look here\"}, {'timestamp': (18.64, 23.56), 'text': ' So what I want to do actually is I want to generate a text summary of each image'}, {'timestamp': (23.56, 26.54), 'text': \" And then I'm going to do an embedding of that text summary\"}, {'timestamp': (0.0, 3.12), 'text': \" So now what I can do is I'm going to go do semantic search.\"}, {'timestamp': (3.46, 5.58), 'text': ' Maybe the text summary will get returned for me.'}, {'timestamp': (5.76, 9.22), 'text': ' If so, then I can pass that image to a multimodal LLM,'}, {'timestamp': (9.3, 13.12), 'text': ' or I can just use the text summary on its own to answer my question or do my task.'}, {'timestamp': (13.5, 13.84), 'text': ' Cool.'}, {'timestamp': (14.06, 16.02), 'text': \" The way we're going to do this is we're actually going to use LangChain.\"}, {'timestamp': (16.16, 16.72), 'text': ' And here we are.'}, {'timestamp': (16.78, 18.62), 'text': \" We're going to load up chat OpenAI.\"}, {'timestamp': (18.86, 21.68), 'text': \" And we're going to use the GPT-4 vision preview model.\"}, {'timestamp': (21.9, 22.12), 'text': ' All right.'}, {'timestamp': (22.2, 23.28), 'text': \" So I'm going to load that up.\"}, {'timestamp': (23.38, 24.68), 'text': ' And I made a quick function here.'}, {'timestamp': (24.84, 28.8), 'text': ' This is just going to convert the physical file on my local machine to Base64,'}, {'timestamp': (0.0, 2.76), 'text': ' which we then can go and pass to the language model.'}, {'timestamp': (3.18, 4.26), 'text': ' So string to base64.'}, {'timestamp': (4.54, 5.78), 'text': ' Now we have this image string.'}, {'timestamp': (6.02, 7.98), 'text': \" Let's go take a look at what this looks like.\"}, {'timestamp': (8.64, 10.1), 'text': ' It just looks like a bunch of gobbledygook,'}, {'timestamp': (10.24, 11.72), 'text': \" which that doesn't mean much to me,\"}, {'timestamp': (11.84, 13.6), 'text': ' but it will mean something to OpenAI,'}, {'timestamp': (13.78, 14.7), 'text': \" and I'm glad that it does.\"}, {'timestamp': (15.02, 16.62), 'text': \" All right, let's close this.\"}, {'timestamp': (16.7, 17.46), 'text': \" Let's get that out of there.\"}, {'timestamp': (17.74, 20.26), 'text': \" So what I'm going to do is I'm going to use the GPT-4 Vision 1 again,\"}, {'timestamp': (20.66, 23.16), 'text': \" and we're going to construct a human message.\"}, {'timestamp': (23.16, 26.46), 'text': \" This just means it acts as if it's coming from the human content type.\"}, {'timestamp': (26.8, 28.9), 'text': ' Please give me a summary of the image provided.'}, {'timestamp': (0.0, 5.82), 'text': \" be descriptive. And then we're going to pass it in image URL. And here we are, the URL is we're\"}, {'timestamp': (5.82, 10.36), 'text': \" going to pass in our image base 64, what we had there. Let's go ahead and pass that over. And\"}, {'timestamp': (10.36, 14.88), 'text': \" let's see what OpenAI thinks the image actually is. I haven't shown you what the image is, but\"}, {'timestamp': (14.88, 19.28), 'text': \" let's look at the summary. The image shows a baking tray with pieces of food, like cookies\"}, {'timestamp': (19.28, 23.82), 'text': ' or some baked goods arranged loosely to resemble the continents on earth as seen from space.'}, {'timestamp': (24.38, 29.56), 'text': ' Hmm. What do you know? There you go. Yeah, that makes sense. So now when I do my retrieval process,'}, {'timestamp': (0.0, 5.28), 'text': \" I can either just use this text in lieu of the picture if I don't want to work with a multimodal\"}, {'timestamp': (5.28, 12.4), 'text': ' LLM or I can do semantic search, have this summary get returned and then pass this image over to the'}, {'timestamp': (12.4, 17.52), 'text': \" language model, the LLM. All right. So that seems about right. So what I've done in level three here\"}, {'timestamp': (17.52, 23.76), 'text': ' is emphasizing that your chunking strategy really depends on your data types. So in this case, I was'}, {'timestamp': (23.76, 27.76), 'text': ' pretty explicit about that and I showed you what it would mean for Python and JavaScript and if you'}, {'timestamp': (0.0, 4.74), 'text': ' have images. But in your industry, in your vertical, you may have different data formats'}, {'timestamp': (4.74, 8.7), 'text': \" and you'll want to pick a chunking strategy that is going to adapt to those data formats.\"}, {'timestamp': (9.02, 13.64), 'text': ' Because remember, the ultimate goal is that you want to group similar items together so that you'}, {'timestamp': (13.64, 18.54), 'text': \" can get them ready and prepared for your language model task in the end. Now you'll see there that\"}, {'timestamp': (18.54, 23.08), 'text': \" I even made an assumption that you want to group similar items together. I'm just saying that\"}, {'timestamp': (23.08, 27.74), 'text': \" because generally you're doing question and answer and generally you want to combine similar items\"}, {'timestamp': (0.0, 5.26), 'text': \" together for context to answer a question. However, if you're not doing that, maybe for some reason\"}, {'timestamp': (5.26, 9.6), 'text': ' you want to combine opposite items together, in which case your chunking strategy would be a lot'}, {'timestamp': (9.6, 13.74), 'text': \" different. I don't know of anybody who actually do that, but let's get back to the tutorial here.\"}, {'timestamp': (13.96, 19.54), 'text': \" All right, so now we're moving on to level four, semantic chunking. Now the interesting part\"}, {'timestamp': (19.54, 25.76), 'text': \" about levels one through three here is we all took physical positioning into account. Doesn't\"}, {'timestamp': (25.76, 29.42), 'text': ' it seem kind of weird that we would split up a document with the intention of grouping similar'}, {'timestamp': (0.0, 4.34), 'text': ' items together, we just assume that paragraphs have similar information in there? What if they'}, {'timestamp': (4.34, 8.36), 'text': \" don't? What if we have a really messy information and doing recursive character text splitting\"}, {'timestamp': (8.36, 13.88), 'text': \" doesn't really do anything for us? You know, I saw this tweet from Linus. We can go take a look at\"}, {'timestamp': (13.88, 19.96), 'text': ' this one. He says, weird idea. Chunk size when doing this, when doing retrieval augmented'}, {'timestamp': (19.96, 24.44), 'text': ' generation is annoying hyperprim and feels naive to turn it into a global constant value.'}, {'timestamp': (24.9, 29.56), 'text': \" I totally agree. Now he recommends, could we train an end to end chunking model? I didn't want to go\"}, {'timestamp': (0.0, 4.58), 'text': \" quite that far because I think there's a little easier step that we could try beforehand and I\"}, {'timestamp': (4.58, 9.08), 'text': \" wanted to do an exploration. But now what I'm going to do is I'm going to do an embedding based\"}, {'timestamp': (9.08, 14.34), 'text': \" chunking method. It's a little bit more expensive and it's definitely more work and it's definitely\"}, {'timestamp': (14.34, 20.06), 'text': ' slower than what we talked about for the first three, but it starts to take the meaning and the'}, {'timestamp': (20.06, 25.48), 'text': ' content of the text into account to make our chunks. The analogy I looked up beforehand is'}, {'timestamp': (0.0, 4.96), 'text': \" imagine the first three levels that's like having a bunch of books and putting them on a bookshelf\"}, {'timestamp': (4.96, 9.92), 'text': ' depending on their size right and the bookshelf size but what if you want to group the books'}, {'timestamp': (9.92, 14.86), 'text': ' together by genre or by theme or by author well then you actually need to know what the books are'}, {'timestamp': (14.86, 19.08), 'text': \" about and that's what we're going to try doing level four here all right so when i thought about\"}, {'timestamp': (19.08, 24.3), 'text': ' level four what i wanted to do was obviously semantic chunking and i chose an embedding'}, {'timestamp': (24.3, 29.4), 'text': ' based way to do this so what i wanted to do was i wanted to take embeddings at certain positions'}, {'timestamp': (0.0, 5.1), 'text': ' of our document. And then I wanted to compare those embeddings together, right? So if two'}, {'timestamp': (5.1, 9.56), 'text': \" embeddings are close to each other distance wise, well, maybe they're talking about the same thing.\"}, {'timestamp': (9.66, 13.72), 'text': \" That's the assumption that we're going to make. If they're further from each other, that means that\"}, {'timestamp': (13.72, 18.56), 'text': \" they're maybe not talking about the same thing, right? So what I'd imagine is I would have a big\"}, {'timestamp': (18.56, 23.54), 'text': ' long essay. And then with those, I take an embedding of every single sentence that we have,'}, {'timestamp': (23.68, 28.12), 'text': ' right? And then I want to compare those embeddings together. Now the comparing the embeddings,'}, {'timestamp': (0.0, 3.7), 'text': \" that's going to be the important part and where all the magic is going to be for this.\"}, {'timestamp': (3.78, 8.18), 'text': ' And I did two different methods that I wanted to share with you. The first one is I did'}, {'timestamp': (8.18, 14.84), 'text': \" hierarchical, that's a mouthful, hierarchical clustering with positional reward. So my first\"}, {'timestamp': (14.84, 18.16), 'text': \" thought is, well, you know, let's just do a clustering algorithm and let's see which embeddings\"}, {'timestamp': (18.16, 21.4), 'text': \" are clustered together. And then let's assume that those are the chunks that we're going to have.\"}, {'timestamp': (21.4, 27.12), 'text': ' But one thing I wanted to do was take into account short sentences that appear after a long sentence,'}, {'timestamp': (0.0, 7.0), 'text': \" you know just like that i wanted the you know to be included with that long sentence because it's\"}, {'timestamp': (7.0, 12.2), 'text': ' likely needs to be relevant with it and so i added a little bit of a positional reward so hierarchical'}, {'timestamp': (12.2, 16.7), 'text': ' clustering generally is just going to be based off of distance but i had a little a little uh'}, {'timestamp': (16.7, 22.0), 'text': ' extra sauce to it and did some positional reward all right this one was okay but it was kind of'}, {'timestamp': (22.0, 27.1), 'text': \" messy to uh to work with and it wasn't as logical as i wanted it to be and i couldn't really tune\"}, {'timestamp': (0.0, 4.78), 'text': ' this intuitively like I wanted. So I wanted to find something just a little bit easier as an'}, {'timestamp': (4.78, 10.4), 'text': ' exploration for me. So what I did was, is the next method was to find break points between'}, {'timestamp': (10.4, 15.78), 'text': ' sequential sentences. So I got embedding number one of sentence number one, and I compared that'}, {'timestamp': (15.78, 20.5), 'text': \" to sentence number two's embedding. And I measured the distance between them. And then I got two,\"}, {'timestamp': (20.6, 25.1), 'text': ' compared it to three, and then three compared it to four, and so on and so forth. I do a visual'}, {'timestamp': (25.1, 28.24), 'text': \" with this, and so I guarantee it's going to make more sense in a second. We're going to use Paul\"}, {'timestamp': (0.0, 4.08), 'text': \" Graham's essay. And what I'm going to do is I'm first going to split all of my different sentences.\"}, {'timestamp': (4.48, 8.38), 'text': \" And I'm going to do that just via some regex with a period, a question mark, or an explanation\"}, {'timestamp': (8.38, 12.78), 'text': \" point. There's likely a lot of better ways to do this. Don't come at me with that. But either way,\"}, {'timestamp': (12.84, 18.9), 'text': ' we have 317 different sentences in this Paul Graham essay. All right. So what I want to do is I want'}, {'timestamp': (18.9, 23.84), 'text': \" to start adding more information to each one of these sentences. So it's like I have a Langchain\"}, {'timestamp': (23.84, 27.5), 'text': \" document, but I'm just going to do my own to show you how we're going to do this. So instead of\"}, {'timestamp': (0.0, 6.36), 'text': ' having a list of sentences, I want to have a list of dictionaries of which the sentence is placed in'}, {'timestamp': (6.36, 10.72), 'text': \" it. I'm going to add in the index just because it's fun. Why not? And let's take a look at these\"}, {'timestamp': (10.72, 15.92), 'text': ' first three after I do that. Now I have a list of dictionaries and one of the keys is sentence.'}, {'timestamp': (16.32, 19.74), 'text': ' And now we have these different sentences up here. Because if I were to go up here, let me just show'}, {'timestamp': (19.74, 23.52), 'text': ' you what this looks like. The single sentence list. I want to do this with this, the first three'}, {'timestamp': (23.52, 27.6), 'text': \" again. It's just a list of strings. Now these list of strings are a list of dictionaries. All right,\"}, {'timestamp': (0.0, 5.08), 'text': ' cool well now what I want to do is I actually want to do some combining of the sentences'}, {'timestamp': (5.08, 9.06), 'text': ' like I said if I just did sentence one compared to sentence two compared to sentence three'}, {'timestamp': (9.06, 12.18), 'text': \" it was a little noisy it was kind of all over the place and it didn't tell me much\"}, {'timestamp': (12.18, 16.6), 'text': \" I thought you know what if I combined the sentences so there's a little less movement\"}, {'timestamp': (16.6, 21.86), 'text': ' from each one because now what I want to do instead of comparing one to two comparing to'}, {'timestamp': (21.86, 27.08), 'text': \" three comparing to four etc I'm going to compare the embedding of sentence one two and three\"}, {'timestamp': (0.0, 7.6), 'text': ' combined with sentence two three and four combined then compare that with sentence three four and five'}, {'timestamp': (7.6, 11.92), 'text': \" combined so it's a little bit more of a group i did a just a small little function you could take\"}, {'timestamp': (11.92, 16.88), 'text': ' advantage of here i have a buffer size of one means one sentence before and one sentence afterwards'}, {'timestamp': (16.88, 20.24), 'text': \" you can do whatever you want go and switch around with this and go play with it i won't go through\"}, {'timestamp': (20.24, 24.96), 'text': \" this code but i've commented it so you can follow along if you want now let's take a look at what\"}, {'timestamp': (0.0, 5.52), 'text': ' that does so here we have our original sentence but now we have our combined sentence all right'}, {'timestamp': (5.52, 9.36), 'text': ' and this combined sentence is going to be what comes before and after it because this is the'}, {'timestamp': (9.36, 15.2), 'text': \" first one there's nothing before it's only after so get funded by y combinator is the sentence of\"}, {'timestamp': (15.2, 20.0), 'text': \" number two nice so we have a combined sentence here which is want to start our startup that's\"}, {'timestamp': (20.0, 25.6), 'text': \" what sentence number one is and then uh something in the grad school and that's what sentence number\"}, {'timestamp': (0.0, 6.16), 'text': ' three is cool now that we have those what i want to do is i want to get an embedding of the grouped'}, {'timestamp': (6.16, 11.16), 'text': \" sentences of this combined sentence key so i'm going to use open ai embeddings and let's go\"}, {'timestamp': (11.16, 15.56), 'text': \" through this and i'm going to get all the embeddings which is basically get the combined\"}, {'timestamp': (15.56, 20.44), 'text': \" sentence for x in each one of those sentences and this is going to be we're going to go grab all\"}, {'timestamp': (20.44, 25.28), 'text': ' those which is really nice we have our embeddings now i need to put those embeddings with its proper'}, {'timestamp': (0.0, 5.1), 'text': \" list. All right. So sentence with it. Now I'm going to make a new key, the combined sentence\"}, {'timestamp': (5.1, 9.16), 'text': \" embeddings, and I'm just going to go through and add those. And let's go take a look at what that\"}, {'timestamp': (9.16, 12.94), 'text': ' looks like now, because of course this is fun. And I like doing this in an iterative nature,'}, {'timestamp': (12.94, 17.24), 'text': ' so we can take one step together at a time. All right. So sentence want to start a startup.'}, {'timestamp': (17.46, 22.5), 'text': \" Here's our combined sentence embedding. So now we have this embedding for what's up here. All right,\"}, {'timestamp': (23.1, 28.4), 'text': ' cool. Well, now what I want to do is I want to add one more metric to it. And I know we keep'}, {'timestamp': (0.0, 5.2), 'text': \" on going here, but hopefully you're still following along. I want to add the distance between the first\"}, {'timestamp': (5.2, 11.02), 'text': ' sentence and the second group of sentences. I want to add that to the first sentence so I can see'}, {'timestamp': (11.02, 15.22), 'text': \" how big is the jump with the next one. All right. So what we're going to do is we're going to get\"}, {'timestamp': (15.22, 19.6), 'text': \" the embedding of the current thing. We're going to get the embedding of the second thing, the second\"}, {'timestamp': (19.6, 23.48), 'text': \" group that it comes with. We're going to get the distance. We're going to append the distances\"}, {'timestamp': (23.48, 26.56), 'text': \" because we're going to do something with this later, but then we're going to start, we're going\"}, {'timestamp': (0.0, 4.76), 'text': ' to get a distance to next. So how far is the distance between the current embedding with'}, {'timestamp': (4.76, 10.06), 'text': \" the next one? Let's go ahead and let's run this. And so now we just got that and this is added to\"}, {'timestamp': (10.06, 13.7), 'text': \" our sentences, but we have our distances here too. So let's just take a look at the first three\"}, {'timestamp': (13.7, 20.98), 'text': ' distances. Awesome. So this is 0.08. So this means that sentence number group number one is 0.08'}, {'timestamp': (20.98, 26.72), 'text': ' distance away from group number two. And group number two is 0.2 distance away from group number'}, {'timestamp': (0.0, 5.1), 'text': \" three. Hmm. It's kind of interesting. Why is group one further away from group two than group\"}, {'timestamp': (5.1, 9.38), 'text': \" two is further away from group three? I don't know, but we're going to do something with this\"}, {'timestamp': (9.38, 12.96), 'text': \" in a second. Let me show you what these sentences look like one more time, just because we're doing\"}, {'timestamp': (12.96, 16.96), 'text': \" this iteratively. Oh boy. I added a whole bunch here. There's too many. I should have just did\"}, {'timestamp': (16.96, 20.74), 'text': \" the first three. Okay. Um, we'll go through this. Let's scroll all the way down to the bottom.\"}, {'timestamp': (21.58, 26.64), 'text': ' We got a long ways to go. Okay. Now we finally have a distance to next because this is the first'}, {'timestamp': (0.0, 4.32), 'text': \" one, you can see that it's 0.08. That's what we just saw above. Cool. Let's close that up.\"}, {'timestamp': (4.7, 9.68), 'text': \" We have our distances here, but now we're all data people. We're all having fun. We all want\"}, {'timestamp': (9.68, 14.08), 'text': \" to see some visuals. I want to see some visuals. Let's do that. Any data scientists out there will\"}, {'timestamp': (14.08, 18.0), 'text': \" laugh at this because I've typed this more in my life than I think I've ever should. Import\"}, {'timestamp': (18.0, 23.4), 'text': ' matplotlib.pyplot as PLT. That is just absolute muscle memory for me right now. All right. So now'}, {'timestamp': (0.0, 7.16), 'text': \" we plot our distances. Cool. So this is our distance. Now it looks kind of random, doesn't\"}, {'timestamp': (7.16, 10.76), 'text': \" it? Well, I mean a little bit. You can see here that it looks like there's a little bit of some\"}, {'timestamp': (10.76, 14.74), 'text': \" ebbs and flows. This one, there's a little bit more distance in between. So what this would mean\"}, {'timestamp': (14.74, 19.74), 'text': ' in English is that for some reason, the chunks here are more dissimilar from each other than'}, {'timestamp': (19.74, 23.96), 'text': \" further down than chunks that are grouped together. But either way, what's interesting to me\"}, {'timestamp': (23.96, 28.28), 'text': \" is that we have some outliers up at the top here. You can see here, there's these points up at the\"}, {'timestamp': (0.0, 5.62), 'text': \" very top. And that tells me, hmm, maybe there's good break points there because two groups are\"}, {'timestamp': (5.62, 10.1), 'text': \" so dissimilar that they should actually be chunked up and they shouldn't actually be together.\"}, {'timestamp': (10.28, 13.44), 'text': \" Because if there's a long distance in their embedding space, maybe they're not talking\"}, {'timestamp': (13.44, 18.4), 'text': \" about the same thing. All right. So I want to show you this one more time, but let's iteratively\"}, {'timestamp': (18.4, 23.34), 'text': \" build another visualization to further emphasize the point that I'm trying to make here. All right.\"}, {'timestamp': (23.52, 27.06), 'text': \" First thing I'm going to do is I'm just going to plot the distances. Let's go down. That's the\"}, {'timestamp': (0.0, 3.28), 'text': ' exact same thing that we had beforehand. All right. Well, the next thing I want to do, I just'}, {'timestamp': (3.28, 6.5), 'text': \" want to do a little bit of formatting. Don't hate me for this. First thing I'm going to do is I'm\"}, {'timestamp': (6.5, 11.52), 'text': ' going to do a Y upper bound. This means the bound of the upper Y limit. Because as you can see right'}, {'timestamp': (11.52, 15.64), 'text': \" here, there's not enough cushion up here for me. It's visually too off. I want to fix this. All\"}, {'timestamp': (15.64, 20.72), 'text': \" right. And then we're going to do a Y limit of from zero to the Y upper bound, which means how\"}, {'timestamp': (20.72, 25.9), 'text': \" long is your Y axis. And then we're going to have an X limb of how long you want this to be on the\"}, {'timestamp': (0.0, 4.52), 'text': \" x limit because you see here there's these buffers in between. I don't want that. All right, let's\"}, {'timestamp': (4.52, 7.64), 'text': ' get rid of that. Anyway, we go through this. We have more space at the top. We got rid of the'}, {'timestamp': (7.64, 10.62), 'text': \" sides. All right, what's next? Let's see what we have here. I don't even know what's going on here.\"}, {'timestamp': (10.66, 15.44), 'text': \" I wrote this code. I still don't. I'm just kidding. I do. Breakpoint percentile threshold.\"}, {'timestamp': (15.9, 21.34), 'text': ' So what we need to do is we need to somehow identify which outliers do we want to split on.'}, {'timestamp': (21.58, 25.3), 'text': \" Now, there's a million ways you could go about doing this. And I'm really excited to hear\"}, {'timestamp': (25.3, 29.98), 'text': ' alternative methods for you that you may have in the back of your mind. For me, what I ended up'}, {'timestamp': (0.0, 5.42), 'text': ' doing was I just did a percentile base. I wanted to identify the outliers. So using these different'}, {'timestamp': (5.42, 10.88), 'text': ' points as a distribution, I wanted to find out, Hey, which points are in the top 5%? Because if'}, {'timestamp': (10.88, 15.22), 'text': \" it's in the top 5%, well, those are probably going to be outliers for us. So I only want to take\"}, {'timestamp': (15.22, 19.32), 'text': \" these, um, upper, these upper distances right here. And the way I'm going to do that is I'm\"}, {'timestamp': (19.32, 23.42), 'text': \" going to specify the percentile I want to take, and then I'm going to use them by, and I'm going\"}, {'timestamp': (23.42, 28.6), 'text': \" to go MP dot percentile. I'm going to pass it my distribution of distances, and I'm going to get\"}, {'timestamp': (0.0, 4.66), 'text': \" the break point percentile threshold. That's going to be 95. And then what I want to do is I want to\"}, {'timestamp': (4.66, 11.28), 'text': \" draw a line on the graph showing where that threshold is. So I'm going to draw a horizontal\"}, {'timestamp': (11.28, 17.1), 'text': ' line right across. And the Y is going to be the break point distance threshold. This will be a'}, {'timestamp': (17.1, 22.14), 'text': \" number that says, hey, what is the 95th percentile of all these? All right, let's take a look at what\"}, {'timestamp': (22.14, 28.24), 'text': ' that looks like. And here we go. So now anything above this line is going to be in the 95th'}, {'timestamp': (0.0, 4.76), 'text': \" percentile. These will be my outliers where I'm going to eventually make my chunks. So everything\"}, {'timestamp': (4.76, 9.08), 'text': \" that's in right here up until this one point, that'll be chunk one. Then we have chunk two,\"}, {'timestamp': (9.34, 12.38), 'text': ' then we have chunk three, chunk four, chunk five, blah, blah, blah, and going from there.'}, {'timestamp': (13.38, 17.54), 'text': ' Because again, one last time, I know I keep on repeating this, but I really want to hammer this'}, {'timestamp': (17.54, 22.38), 'text': \" home. The hypothesis is that if there's a big break point, then a chunk should be split up at\"}, {'timestamp': (22.38, 26.28), 'text': \" that point. And so this is where we're going to end up doing that. All right. Then what we're\"}, {'timestamp': (0.0, 3.72), 'text': \" going to do is we're going to see how many distances are actually above this one. And so\"}, {'timestamp': (3.72, 8.08), 'text': ' I want to get the number of distances that we have, meaning the number of breakpoints,'}, {'timestamp': (8.16, 12.56), 'text': \" the number of things that are going to be above that threshold. And then I'm going to do plt.txt.\"}, {'timestamp': (12.78, 17.0), 'text': ' This is just a fancy way to put some text on your visualization. Let me go ahead and do that.'}, {'timestamp': (17.0, 21.08), 'text': ' And so we can see that we have 17 chunks. I put that down in the corner right there.'}, {'timestamp': (21.84, 25.98), 'text': \" That's kind of interesting. All right. Then what we're going to do is we're going to get the\"}, {'timestamp': (0.0, 6.22), 'text': ' indices of which points actually are above the breakpoint meaning which are the actual outliers'}, {'timestamp': (6.22, 11.62), 'text': ' because this break uh this breakpoint distance threshold this is just a single static number'}, {'timestamp': (11.62, 14.96), 'text': ' but i need to get a list of numbers to find out where these breakpoints and these chunks actually'}, {'timestamp': (14.96, 21.32), 'text': \" need to be met so for i in distances if x is above the breakpoint distance threshold so i'll get a\"}, {'timestamp': (21.32, 25.8), 'text': \" bunch of indices there that doesn't do anything different for us but then what we're going to do\"}, {'timestamp': (25.8, 29.84), 'text': \" actually let me just do this because i think this actually would be helpful i'm going to look at\"}, {'timestamp': (0.0, 6.0), 'text': ' indices now what we have here these are 17 different chunks we can look at this it says 16'}, {'timestamp': (6.0, 10.4), 'text': \" but that's because there's an extra one added to the front there should be a zero right here um or\"}, {'timestamp': (10.4, 15.84), 'text': ' this could be the 317 at the end so what this means is between uh between sentence zero and'}, {'timestamp': (15.84, 22.88), 'text': ' sentence 23 we want to make our split and make our break all right because at just at number 23'}, {'timestamp': (23.44, 28.32), 'text': ' it says hey here the distance to the next one was quite big so we want to include number 23 on this'}, {'timestamp': (0.0, 5.76), 'text': \" one okay either way let's go from there let's go do some more uh let's do some fancy colors on here\"}, {'timestamp': (5.76, 9.28), 'text': ' so what i want to do is i want to add some colors i just set my own custom ones right here and then'}, {'timestamp': (9.28, 13.12), 'text': \" what i'm going to do is i'm going to go do a vertical span meaning you're going to have a\"}, {'timestamp': (13.12, 18.8), 'text': \" vertical shading in the background of your um the background of your graph here and you're going to\"}, {'timestamp': (18.8, 23.36), 'text': ' do a start index and an end index the reason why i do a for loop here is because you add them one'}, {'timestamp': (23.36, 28.0), 'text': ' at a time but the start index and the ending end index will be what is in our indices above'}, {'timestamp': (0.0, 4.0), 'text': \" thresholds anyway let's go through there and you can see here i cheated a little bit let's take\"}, {'timestamp': (4.0, 8.24), 'text': ' away this text we can see that we have our different chunks right here so now we have chunk'}, {'timestamp': (8.24, 12.72), 'text': ' zero chunk one chunk two blah blah and go all the way through there but of course we want some text'}, {'timestamp': (12.72, 17.68), 'text': ' on here to really make it more explicit and you can see here chunk blah blah blah blah this last'}, {'timestamp': (17.68, 21.04), 'text': ' one was giving me a hard time so i actually had to do just a little bit of custom code for that one'}, {'timestamp': (21.76, 26.0), 'text': \" that splits it up uh that's just a little bit of band-aid don't don't add me for that one either\"}, {'timestamp': (0.0, 4.68), 'text': \" I was too lazy to figure that one out. Okay, cool. So we go through there. So here's all of\"}, {'timestamp': (4.68, 8.66), 'text': \" our different chunks. Now this wouldn't be a good chart unless we put on some graphics or some\"}, {'timestamp': (8.66, 13.78), 'text': \" titles as well. So now we'll do a title, a Y label and an X label. And then there we go.\"}, {'timestamp': (14.36, 18.84), 'text': \" That's kind of cool. Paul Graham essay chunks based off embedding break points.\"}, {'timestamp': (20.14, 24.5), 'text': \" That's pretty interesting, but a good visualization doesn't do anything for us. We can't really pass\"}, {'timestamp': (24.5, 27.28), 'text': \" this to the language model and it's not going to know what to do with it. So what we're going to\"}, {'timestamp': (0.0, 4.5), 'text': \" do is we're going to actually get the sentences and actually combine them. So here's a bunch of\"}, {'timestamp': (4.5, 11.42), 'text': \" code here. I won't go through it too much, but the TLDR is that you're going to append all these\"}, {'timestamp': (11.42, 14.68), 'text': \" different pieces in your chunks. So like I said beforehand, you're going to go from chunk zero\"}, {'timestamp': (14.68, 17.96), 'text': \" to chunk 23, and you're going to combine those, and that'll be your first chunk.\"}, {'timestamp': (18.2, 21.64), 'text': \" Let's go through there. Then let's take a look at what this actually looks like. Let's go through\"}, {'timestamp': (21.64, 27.1), 'text': ' this. And so we have the chunk number zero, about a month of the defining cycle. We had something'}, {'timestamp': (0.0, 4.86), 'text': \" called a prototype day. You might think they wouldn't need any more motivation. Cool. They're\"}, {'timestamp': (4.86, 8.2), 'text': \" working on their cool new idea. They have funding for an immediate future and they're playing the\"}, {'timestamp': (8.2, 12.78), 'text': ' long game with only two outcomes, wealth or failure. You think motivation might be enough.'}, {'timestamp': (13.12, 16.94), 'text': \" So the hypothesis here is that these two are actually getting split up at a spot where there's\"}, {'timestamp': (16.94, 21.62), 'text': \" a big break point with it. So it's kind of interesting to see where the semantic splitting\"}, {'timestamp': (21.62, 25.88), 'text': \" actually happened. There you go. And now I want to reemphasize that this isn't perfect, of course,\"}, {'timestamp': (0.0, 5.06), 'text': ' but I think this is an interesting step towards doing semantic chunking. Because if I were to'}, {'timestamp': (5.06, 8.22), 'text': \" think, if I'm going to hypothesize out in the future, what is chunking going to be like?\"}, {'timestamp': (8.68, 13.02), 'text': \" Well, as compute gets better, as language models get better, there's no way we're going to do\"}, {'timestamp': (13.02, 18.62), 'text': ' physical-based chunking anymore, unless the structure of our documents is we can make those'}, {'timestamp': (18.62, 23.8), 'text': \" big assumptions on it. We're probably going to do a smart chunking. And I think this is a really cool\"}, {'timestamp': (23.8, 29.1), 'text': \" way to go towards that. All right. So that's level four. Again, experimental. Please let me\"}, {'timestamp': (0.0, 3.74), 'text': ' know what you think, please give me other ideas for how we could make this a little bit better.'}, {'timestamp': (3.98, 7.22), 'text': ' And this is going to be a little plug. If you want to see more of these experimental methods that I'}, {'timestamp': (7.22, 12.6), 'text': \" do, I share this out on Twitter. So other people got to see this early. Now let's move on to level\"}, {'timestamp': (12.6, 18.3), 'text': \" five, agentic chunking. So if we went off the deep end with level four, we're going into the ocean\"}, {'timestamp': (18.3, 22.28), 'text': \" here. We're going into the Mariana Trench and we're going to the very bottom and we're going\"}, {'timestamp': (22.28, 27.46), 'text': ' to do some cool things. So my motivation for this side is I asked myself, Hey Greg, what if a human'}, {'timestamp': (0.0, 4.96), 'text': ' were to do chunking? How would I do chunking in the first place? And I thought, well, I would go'}, {'timestamp': (4.96, 8.96), 'text': \" get myself a piece of scratch paper because I can't do all that in my head. I'd start at the\"}, {'timestamp': (8.96, 13.5), 'text': ' top of the essay and assume the first part will be in a chunk. Well, because the first little bit,'}, {'timestamp': (13.54, 16.32), 'text': \" it needs to go somewhere. We don't have any chunks yet. So of course it's going to go in the first\"}, {'timestamp': (16.32, 21.6), 'text': ' chunk. Then I would keep going down the essay and evaluate if a new sentence or piece of the essay'}, {'timestamp': (21.6, 28.82), 'text': ' should be a part of the first chunk. If not, then create a new one. Then keep doing that all the way'}, {'timestamp': (0.0, 5.66), 'text': ' down on the essay until we got to the end. And I thought to myself, wait, wait a minute.'}, {'timestamp': (6.36, 12.42), 'text': \" This is pseudocode. We can make something like an agent to do this for us. Now, I don't like the\"}, {'timestamp': (12.42, 16.0), 'text': \" word agent quite yet because we're not quite there yet. A lot of people like using it. I think\"}, {'timestamp': (16.0, 20.46), 'text': \" there's a lot of marketing around it. So yes, I call this agentic chunking, but I'm going to call\"}, {'timestamp': (20.46, 26.26), 'text': \" this an agent-like system. The way that I like to define what an agent is, is there's some decision\"}, {'timestamp': (0.0, 4.64), 'text': \" making that needs to go on in there and you use the language model to make decisions which you're\"}, {'timestamp': (4.64, 9.76), 'text': ' going to have an unbounded path but the language model will help you guide that via your decision'}, {'timestamp': (9.76, 14.72), 'text': \" making that you do and so i thought man this is pretty cool i'm going to try this out all right\"}, {'timestamp': (14.72, 19.2), 'text': \" so now let's go into level five and talk about what i found here so one first design decision\"}, {'timestamp': (19.2, 24.8), 'text': ' that i need to make is how do i want to give different pieces of my essay to the language'}, {'timestamp': (24.8, 28.96), 'text': \" model and i thought man well there's still that problem with the short sentences you know so\"}, {'timestamp': (0.0, 3.14), 'text': ' So around this time, there was a cool paper that came out all about propositions.'}, {'timestamp': (3.54, 4.26), 'text': ' What is a proposition?'}, {'timestamp': (4.74, 8.44), 'text': ' Well, a proposition is a sentence that can stand on its own.'}, {'timestamp': (8.78, 10.68), 'text': \" So there's another agent-like system.\"}, {'timestamp': (10.76, 11.52), 'text': \" It's kind of just a prompt.\"}, {'timestamp': (11.62, 12.54), 'text': \" We'll go over that in a second here.\"}, {'timestamp': (12.76, 16.26), 'text': \" But it's going to take a sentence, and it's going to pull out propositions,\"}, {'timestamp': (16.42, 19.3), 'text': ' which are little itty-bitty Legos that can stand on their own.'}, {'timestamp': (19.76, 20.82), 'text': \" Let's talk about what that means.\"}, {'timestamp': (21.34, 22.24), 'text': ' Greg went to the park.'}, {'timestamp': (22.62, 23.34), 'text': ' He likes walking.'}, {'timestamp': (23.5, 26.82), 'text': ' If you were to pass he likes walking as a chunk to the language model,'}, {'timestamp': (27.3, 29.12), 'text': ' language model is going to be like, WTF?'}, {'timestamp': (0.0, 1.0), 'text': ' Who is he?'}, {'timestamp': (1.0, 6.4), 'text': \" However, if we change this into propositions, it will make less sense from a reader's perspective,\"}, {'timestamp': (6.4, 9.94), 'text': \" meaning it doesn't look great for us to read, but it makes a lot more sense for a language\"}, {'timestamp': (9.94, 10.94), 'text': ' model.'}, {'timestamp': (10.94, 11.94), 'text': ' Greg went to the park.'}, {'timestamp': (11.94, 12.94), 'text': ' Greg likes walking.'}, {'timestamp': (12.94, 14.44), 'text': \" That's pretty interesting.\"}, {'timestamp': (14.44, 17.66), 'text': ' If you want to take a look at more proposition work, and I might do a whole nother video'}, {'timestamp': (17.66, 18.66), 'text': ' on this.'}, {'timestamp': (18.66, 20.72), 'text': ' Let me know if you want me to, because I think this is really cool.'}, {'timestamp': (20.72, 23.34), 'text': ' Langchain came out with proposition based retrieval.'}, {'timestamp': (23.34, 25.8), 'text': ' So a new paper by Tom Chen.'}, {'timestamp': (25.8, 28.78), 'text': \" I haven't met Tom Chen yet, but Tom, I'm a big fan of your work.\"}, {'timestamp': (0.0, 1.92), 'text': \" If you want to chat, I'm very down to talk.\"}, {'timestamp': (1.92, 5.32), 'text': ' In the image that they showed, prior to the restoration work performed between blah, blah,'}, {'timestamp': (5.32, 7.94), 'text': \" blah, blah, blah, blah, you have this big long paragraph and then it's going to split\"}, {'timestamp': (7.94, 9.92), 'text': ' up into different propositions.'}, {'timestamp': (9.92, 13.72), 'text': ' And then you use that for your retrieval because these sentences can stand a bit more on their'}, {'timestamp': (13.72, 14.72), 'text': ' own.'}, {'timestamp': (14.72, 15.72), 'text': \" Let's go ahead and do it.\"}, {'timestamp': (15.72, 16.72), 'text': \" I'm importing a whole bunch of Langchain stuff here.\"}, {'timestamp': (16.72, 20.68), 'text': \" I'm not going to go over each one except for the cool parts.\"}, {'timestamp': (20.68, 24.24), 'text': ' Now one of the cool parts here is going to be this Langchain import hub.'}, {'timestamp': (24.24, 26.32), 'text': \" Hey, Greg, what's the Langchain hub?\"}, {'timestamp': (26.32, 28.48), 'text': ' I should probably do a whole another video on this either.'}, {'timestamp': (0.0, 3.74), 'text': ' But they have this thing or Langchain came out with this thing called the Lang Hub.'}, {'timestamp': (3.74, 8.9), 'text': ' This is within their Lang Smith suite, but a Lang Hub is just going to be where they'}, {'timestamp': (8.9, 10.4), 'text': ' share prompts around.'}, {'timestamp': (10.4, 14.36), 'text': \" So this person, I don't know who this is, looks like it's been viewed a lot.\"}, {'timestamp': (14.36, 19.98), 'text': ' YouTube transcript to article, act as a expert copywriter specializing blah, blah, blah.'}, {'timestamp': (19.98, 25.06), 'text': ' So Langchain will help host prompt templates that you can use for your own.'}, {'timestamp': (25.06, 27.44), 'text': \" So it's an easy way to just go share prompt templates.\"}, {'timestamp': (0.0, 3.32), 'text': ' So here we have this whole entire prompt and here we have a token where you can go and'}, {'timestamp': (3.32, 4.32), 'text': ' grab it.'}, {'timestamp': (4.32, 7.74), 'text': ' And then if you wanted to grab this prompt yourself, then you can go and just grab it'}, {'timestamp': (7.74, 8.74), 'text': ' like this.'}, {'timestamp': (8.74, 13.26), 'text': ' The advantage of this is you can share prompts a whole lot easier, but then two, if you want'}, {'timestamp': (13.26, 17.18), 'text': ' prompts to be updated with the latest and greatest thinking, well, you can just keep'}, {'timestamp': (17.18, 18.92), 'text': ' on pulling from there, which is nice.'}, {'timestamp': (18.92, 25.14), 'text': \" So I'm going to pull a hub.poll, WHF proposal indexing.\"}, {'timestamp': (0.0, 2.24), 'text': \" So let's go view the proposition prompt here.\"}, {'timestamp': (3.26, 5.74), 'text': ' WFH proposal indexing.'}, {'timestamp': (5.9, 7.22), 'text': \" Here's a citation that comes with it.\"}, {'timestamp': (7.34, 8.78), 'text': \" And here's the chat prompt template.\"}, {'timestamp': (8.88, 9.62), 'text': \" I won't go through all this,\"}, {'timestamp': (9.7, 12.44), 'text': ' but the interesting part is split the compound sentence'}, {'timestamp': (12.44, 13.4), 'text': ' into simple sentences.'}, {'timestamp': (13.8, 16.08), 'text': ' Maintain the original phrasing from the input whenever possible.'}, {'timestamp': (16.42, 18.38), 'text': ' And then they actually give one example here.'}, {'timestamp': (18.48, 19.74), 'text': ' So they have an input one here,'}, {'timestamp': (19.74, 21.78), 'text': \" and then they have an output about what they'd want\"}, {'timestamp': (21.78, 22.92), 'text': ' the language model to output.'}, {'timestamp': (23.26, 23.84), 'text': ' Hmm, interesting.'}, {'timestamp': (24.2, 25.34), 'text': ' I just went and copied this code.'}, {'timestamp': (25.74, 26.88), 'text': ' I put that in right here.'}, {'timestamp': (27.08, 29.08), 'text': \" I'm also going to get my language model going.\"}, {'timestamp': (0.0, 4.26), 'text': \" And we're going to use GPT for a preview because I want the long context and good processing power.\"}, {'timestamp': (4.48, 4.62), 'text': ' All right.'}, {'timestamp': (4.96, 7.1), 'text': ' So within this object is going to be the prompt.'}, {'timestamp': (7.3, 10.76), 'text': \" Then I'm going to create a runnable which combines the prompt and the language model.\"}, {'timestamp': (11.08, 14.48), 'text': ' And this is via the Langchain expression language.'}, {'timestamp': (14.68, 18.64), 'text': ' And with that language, you can do just this pipe operator and you can combine those right there.'}, {'timestamp': (18.86, 24.72), 'text': \" And the next thing I'm going to do, I tried doing some extraction of these propositions via the recommended way,\"}, {'timestamp': (24.78, 26.0), 'text': ' but I found it was giving me a hard time.'}, {'timestamp': (26.06, 27.54), 'text': ' So I just made my own extractor from it.'}, {'timestamp': (0.0, 4.36), 'text': \" and the way i'm going to do that is i'm going to just do the pidantic extraction method so i'm\"}, {'timestamp': (4.36, 9.04), 'text': \" going to create a pidantic class here and then i'm going to do extraction chain create extraction\"}, {'timestamp': (9.04, 13.4), 'text': \" chain with pidantic and i'm going to pass it in the sentences pass in the language model\"}, {'timestamp': (13.4, 17.52), 'text': \" cool we have that and then i'm going to create a small little function which is this the get\"}, {'timestamp': (17.52, 22.18), 'text': ' propositions which is going to be hey go and get this pro go and get the list of propositions from'}, {'timestamp': (22.18, 26.5), 'text': \" the thing that i give you because it takes a runnable and then it's going to do the extractions\"}, {'timestamp': (0.0, 2.26), 'text': \" and it's going to return the propositions for us.\"}, {'timestamp': (2.42, 2.58), 'text': ' All right.'}, {'timestamp': (2.84, 5.12), 'text': \" So now I'm going to do Paul Graham's super linear essay.\"}, {'timestamp': (5.58, 5.86), 'text': ' Cool.'}, {'timestamp': (5.98, 6.54), 'text': ' We have that.'}, {'timestamp': (6.82, 8.5), 'text': \" I'm going to split the essay into paragraphs.\"}, {'timestamp': (8.7, 10.86), 'text': ' Now, this is a design choice.'}, {'timestamp': (11.52, 13.38), 'text': \" Hey, Greg, aren't we doing just chunking again?\"}, {'timestamp': (13.7, 16.46), 'text': \" Not really, because this is like a very loose chunk that doesn't really matter.\"}, {'timestamp': (16.54, 18.02), 'text': ' I could pass it a sentence at a time.'}, {'timestamp': (18.22, 18.94), 'text': ' I could pass it too.'}, {'timestamp': (19.02, 19.78), 'text': ' I could pass the paragraphs.'}, {'timestamp': (20.04, 21.4), 'text': ' See how many paragraphs we have.'}, {'timestamp': (22.3, 23.96), 'text': ' We have 53 different paragraphs.'}, {'timestamp': (24.44, 24.68), 'text': ' Nice.'}, {'timestamp': (24.96, 29.66), 'text': \" Let's take a look at one of the paragraphs just because I like being super explicit here.\"}, {'timestamp': (0.0, 2.6), 'text': \" Let's do number two.\"}, {'timestamp': (2.6, 4.94), 'text': ' Nice, one of the most important things.'}, {'timestamp': (4.94, 6.82), 'text': \" I didn't know, et cetera, et cetera.\"}, {'timestamp': (6.82, 7.9), 'text': ' We can look at number five.'}, {'timestamp': (7.9, 9.5), 'text': ' Cool, those are just paragraphs for us.'}, {'timestamp': (9.5, 10.5), 'text': ' All right.'}, {'timestamp': (10.5, 12.8), 'text': \" Then what we're gonna do is we're gonna go get our propositions.\"}, {'timestamp': (12.8, 15.0), 'text': \" And so we're gonna do essay propositions.\"}, {'timestamp': (15.0, 16.34), 'text': \" And so I'm just gonna go through this,\"}, {'timestamp': (16.34, 17.5), 'text': ' go through each paragraph.'}, {'timestamp': (17.5, 18.56), 'text': \" I'm just gonna do the first five\"}, {'timestamp': (18.56, 20.54), 'text': \" because there's kind of a lot of data here.\"}, {'timestamp': (20.54, 22.04), 'text': \" I'm gonna get the propositions.\"}, {'timestamp': (22.04, 23.58), 'text': ' Let me come back to you when this is done.'}, {'timestamp': (23.58, 25.44), 'text': ' One minute later.'}, {'timestamp': (25.44, 27.16), 'text': ' Awesome, so we have our propositions.'}, {'timestamp': (27.16, 28.32), 'text': \" Let's take a look at a few here.\"}, {'timestamp': (0.0, 1.7), 'text': \" Again, because we're doing this iteratively with each other.\"}, {'timestamp': (1.86, 2.06), 'text': ' All right.'}, {'timestamp': (2.38, 3.4), 'text': \" I'm going to take a look at a few.\"}, {'timestamp': (5.3, 6.82), 'text': ' You have 26 propositions.'}, {'timestamp': (6.98, 7.1), 'text': ' Cool.'}, {'timestamp': (7.2, 9.58), 'text': ' So our five paragraphs resulted in 26 propositions.'}, {'timestamp': (10.06, 10.82), 'text': ' The month is October.'}, {'timestamp': (10.96, 11.78), 'text': ' The year is 2023.'}, {'timestamp': (12.32, 14.56), 'text': ' At the time past, I did not understand something about the world.'}, {'timestamp': (14.7, 14.88), 'text': ' Cool.'}, {'timestamp': (14.96, 19.36), 'text': \" So now we're starting to pull out individual facts about what the paragraph is about.\"}, {'timestamp': (19.48, 19.66), 'text': ' Lovely.'}, {'timestamp': (19.98, 24.16), 'text': ' So now what I want to do is I want to use an agent-like system that is going to go through'}, {'timestamp': (0.0, 5.58), 'text': ' each one of these iteratively and decide, hey, should this be a part of a chunk that we already'}, {'timestamp': (5.58, 10.0), 'text': \" have or should it not? There's no package that I saw that did this for us yet because this is a\"}, {'timestamp': (10.0, 15.02), 'text': \" quite experimental method. But what I did is I ended up making a agentic chunker. This isn't a\"}, {'timestamp': (15.02, 19.06), 'text': \" package yet, so you can't go import this anywhere, but I'll show you the code that's powering this.\"}, {'timestamp': (19.44, 22.8), 'text': \" Awesome. So here's the code on how this works. Now, I'm not going to go through this in detail\"}, {'timestamp': (22.8, 26.38), 'text': \" because that's not the point of this tutorial, but I'm going to go through the high level pieces\"}, {'timestamp': (0.0, 5.06), 'text': \" here. The way that it works is you're going to have AC equals agentic chunker. Okay, cool. And\"}, {'timestamp': (5.06, 8.46), 'text': \" then you're going to have your list of propositions. This could be sentences, but propositions will be\"}, {'timestamp': (8.46, 11.32), 'text': \" best because that's how I designed it to work. And then what you're going to do is you're going\"}, {'timestamp': (11.32, 15.6), 'text': \" to add your propositions to the class. And then what it's going to do is it's going to start to\"}, {'timestamp': (15.6, 20.26), 'text': \" form chunks for you. Then we're going to pretty print the chunks. All right. So let's go through\"}, {'timestamp': (20.26, 24.78), 'text': ' and see how this works. The real magic happens in the add propositions. So if we go up to the top'}, {'timestamp': (0.0, 4.88), 'text': \" here. So we're going to add a proposition. Now, if it's your first chunk, meaning your first\"}, {'timestamp': (4.88, 8.6), 'text': \" proposition, your first chunk, then you're not going to have any chunks and chunks is going to\"}, {'timestamp': (8.6, 12.5), 'text': \" be a property of this class. If chunk size equals zero, meaning you don't have any yet,\"}, {'timestamp': (12.92, 18.3), 'text': ' then create a new chunk. Totally makes sense. Well, what do you want the chunk to be about?'}, {'timestamp': (18.62, 23.54), 'text': \" Well, let's go find out where create new chunk is. And on create new chunk, what we're going to do\"}, {'timestamp': (23.54, 28.4), 'text': \" is we're going to create a chunk ID, which is going to be a random UUID or a subset of one.\"}, {'timestamp': (0.0, 4.48), 'text': \" then we're going to get a chunk summary and a chunk title. The reason why we do this is because\"}, {'timestamp': (4.48, 9.9), 'text': ' when we add a future thing, we need to know, well, what are our chunks already about, right?'}, {'timestamp': (10.02, 13.18), 'text': \" Because that'll tell us whether or not we need to add it. So we're going to have a summary,\"}, {'timestamp': (13.36, 16.44), 'text': \" which is going to have a lot of good detailed information, and we're going to have a title,\"}, {'timestamp': (16.66, 22.32), 'text': ' right? So in get new chunk summary, all that this is doing is looking at the propositions that are'}, {'timestamp': (22.32, 26.12), 'text': \" currently in the chunk, and then it's generating a summary about what that chunk is about.\"}, {'timestamp': (26.24, 29.72), 'text': ' And then the chunk title, this is just a few words that kind of give you a quick glance'}, {'timestamp': (0.0, 5.12), 'text': ' on what it actually is. Now, there is a parameter you can set when you do this. Do you want to'}, {'timestamp': (5.12, 10.72), 'text': ' update your summary and title? Because as I was going through this, as I added, say, proposition'}, {'timestamp': (10.72, 14.92), 'text': ' number one, it was about one thing. But then once you added proposition number two and three and'}, {'timestamp': (14.92, 18.88), 'text': ' four to the chunk, you may need to update the summary or update the chunk because now the chunk'}, {'timestamp': (18.88, 24.06), 'text': \" is kind of just slightly different, right? It's kind of as if you had a centroid and it's moving\"}, {'timestamp': (24.06, 27.96), 'text': ' just slightly, but you want to capture that essence, all right? And then what we do is with'}, {'timestamp': (0.0, 4.8), 'text': ' each one of the chunks we have a chunk id we have a proposition or we have a list of propositions a'}, {'timestamp': (4.8, 11.04), 'text': ' title a summary and then the chunk index is just um when was this chunk made its number cool uh'}, {'timestamp': (11.04, 16.8), 'text': \" let's go back to where we were um okay so that's if if you add uh your first proposition to your\"}, {'timestamp': (16.8, 20.4), 'text': \" empty list of chunks and then you're going to return here meaning you're going to stop you're\"}, {'timestamp': (20.4, 24.24), 'text': \" not going to go any further because well you've already added it to a chunk it's all good cool\"}, {'timestamp': (24.24, 29.12), 'text': \" but let's say that wasn't the case well let's go find a relevant chunk this part i thought was\"}, {'timestamp': (0.0, 4.64), 'text': \" actually kind of cool too in this relevant chunk you're going to have a proposition now you want a\"}, {'timestamp': (4.64, 11.04), 'text': ' proposition to go in and you want a chunk to come out right and so we have a simple little prompt'}, {'timestamp': (11.04, 15.36), 'text': ' here determine whether or not the proposition should belong to any of the existing chunks'}, {'timestamp': (15.92, 19.76), 'text': \" and then i do an example here i have some other good stuff but what i'm going to do here is i'm\"}, {'timestamp': (19.76, 25.44), 'text': ' going to pass it a string of what our current chunks actually look like and those current'}, {'timestamp': (0.0, 5.44), 'text': ' chunks are going to be groups of three things the id of the chunk the name of the chunk and'}, {'timestamp': (5.44, 11.92), 'text': ' the summary of the chunk because what i want it to do is if it deems that yes this proposition'}, {'timestamp': (11.92, 17.2), 'text': ' should be part of a chunk well then what i want you to do is i want you to output the chunk id'}, {'timestamp': (17.2, 21.92), 'text': ' for me and this is how i can then extract which chunk it should actually be a part of all right'}, {'timestamp': (22.64, 26.64), 'text': \" so we're going to go through there we got all of that and let's just say that a chunk id\"}, {'timestamp': (0.0, 5.06), 'text': \" does come out, meaning it should be added to another chunk. Well, what I'm going to do is\"}, {'timestamp': (5.06, 9.52), 'text': \" I'm just going to add that proposition to the chunk. Nice. So yes, I found a chunk it should\"}, {'timestamp': (9.52, 13.12), 'text': \" be a part of. Cool. I'm going to add this proposition to it. Then when you add this\"}, {'timestamp': (13.12, 18.26), 'text': ' proposition to it, this is where I was talking about beforehand. If you want to generate new'}, {'timestamp': (18.26, 22.04), 'text': \" metadata, then it will go and generate a new summary and title. If you don't want to do it,\"}, {'timestamp': (22.04, 29.26), 'text': \" then it doesn't have to. So let's say you didn't find a new chunk ID, meaning you wanted to add a\"}, {'timestamp': (0.0, 4.1), 'text': \" proposition. It didn't find a new chunk. So you need to actually make a new one. No chunks are\"}, {'timestamp': (4.1, 9.08), 'text': \" found, create a new chunk. And that's the same thing that we had up above. And you go from there.\"}, {'timestamp': (9.4, 13.64), 'text': \" So that's really the meat of the entire thing. And if we're going to go back to our repo here,\"}, {'timestamp': (13.88, 18.84), 'text': \" let's go do this for our essay that we had. All right. So I'm going to do the agentic chunker.\"}, {'timestamp': (18.84, 24.98), 'text': \" I'm going to say from AC or make AC. And then what I want to do is I want to add each one of\"}, {'timestamp': (0.0, 5.06), 'text': ' the first 20 propositions. But you know, we only had 23. So let me just do this whole thing here.'}, {'timestamp': (5.36, 9.64), 'text': \" And then what I did was, is I have a lot of print logging. If you don't want that because you think\"}, {'timestamp': (9.64, 14.02), 'text': \" it's annoying, just go print logging equals false. But let's step through this. So now it's adding\"}, {'timestamp': (14.02, 18.46), 'text': \" our first chunk, which is the month is October. No chunks, duh, because you don't have any. So\"}, {'timestamp': (18.46, 24.04), 'text': \" it's created a new chunk, which is 51322. It's called date and times. Yeah, cool. Makes sense.\"}, {'timestamp': (24.28, 29.94), 'text': \" That's probably where I'd want this to go. The year is 2023. Chunk found. Date and times. Oh,\"}, {'timestamp': (0.0, 3.54), 'text': \" duh because what it did is it saw there's a chunk there's a date and times chunk i'm going to add\"}, {'timestamp': (3.54, 9.16), 'text': \" this one to it too okay cool i was a child at some past time no chunks are found because there's\"}, {'timestamp': (9.16, 12.72), 'text': \" only one at this time and it doesn't think it's part of that one and so created a new chunk\"}, {'timestamp': (12.72, 19.5), 'text': ' personal history nice okay at the past time i did not understand something important about the world'}, {'timestamp': (19.5, 25.24), 'text': \" it's adding it to personal history cool makes sense the important thing i did not understand\"}, {'timestamp': (0.0, 2.42), 'text': ' is that the degree in which returns for performance'}, {'timestamp': (2.42, 3.22), 'text': ' are super linear.'}, {'timestamp': (3.72, 5.16), 'text': \" It didn't find any chunks,\"}, {'timestamp': (5.22, 6.82), 'text': \" and so it doesn't think it's part of date and times\"}, {'timestamp': (6.82, 7.56), 'text': ' or personal history.'}, {'timestamp': (7.88, 10.36), 'text': ' It made a new one called performance'}, {'timestamp': (10.36, 11.58), 'text': ' and returns relationship.'}, {'timestamp': (12.08, 12.26), 'text': ' Cool.'}, {'timestamp': (13.44, 15.9), 'text': ' Teachers and coaches implicitly told us returns were linear.'}, {'timestamp': (16.54, 17.18), 'text': ' Chunk found.'}, {'timestamp': (17.6, 19.98), 'text': \" It's adding it to the returns chunk.\"}, {'timestamp': (20.42, 20.62), 'text': ' Nice.'}, {'timestamp': (21.26, 23.56), 'text': ' Teachers and coaches meant well, no chunks, blah, blah, blah,'}, {'timestamp': (23.6, 24.48), 'text': ' and we go all the way through here,'}, {'timestamp': (24.74, 25.54), 'text': \" and so it's kind of interesting,\"}, {'timestamp': (25.68, 28.32), 'text': ' but what I want to show you is an instance'}, {'timestamp': (28.32, 29.64), 'text': ' where a chunk name was updated.'}, {'timestamp': (0.0, 6.74), 'text': ' So adding, you get out what you put in was heard a thousand times by the speaker.'}, {'timestamp': (7.7, 11.86), 'text': \" It's adding it to the misconceptions in performance of returns in relationships.\"}, {'timestamp': (12.38, 13.74), 'text': ' Wait, where did that title come from?'}, {'timestamp': (14.38, 23.08), 'text': \" Oh, wait, it's the same chunk ID, but the name has been updated because you're actually updating the chunk as the chunk title as you go along here.\"}, {'timestamp': (23.34, 25.26), 'text': \" So as we go through here, we're going all the way through.\"}, {'timestamp': (25.36, 26.44), 'text': ' It did this a whole bunch of times.'}, {'timestamp': (26.76, 28.06), 'text': ' I want to see what comes out the other end.'}, {'timestamp': (0.0, 2.76), 'text': ' So you can pretty print the chunks.'}, {'timestamp': (2.76, 5.32), 'text': ' Cool, so it looks like we have five chunks.'}, {'timestamp': (5.32, 7.48), 'text': ' Chunk number zero has this chunk ID.'}, {'timestamp': (8.52, 9.7), 'text': ' This chunk contains information'}, {'timestamp': (9.7, 11.4), 'text': ' about the specific dates and times, cool.'}, {'timestamp': (11.4, 12.72), 'text': ' And here are the propositions'}, {'timestamp': (12.72, 14.42), 'text': ' that were added to that chunk, nice.'}, {'timestamp': (14.42, 16.2), 'text': ' So this is the content of our chunk'}, {'timestamp': (16.2, 18.1), 'text': ' that we wanna actually pull in.'}, {'timestamp': (18.1, 20.34), 'text': \" This chunk contains reflections on someone's childhood,\"}, {'timestamp': (20.34, 21.96), 'text': ' blah, blah, blah, okay, cool.'}, {'timestamp': (21.96, 23.86), 'text': ' Oh, dang, oh, this is a big one.'}, {'timestamp': (23.86, 25.02), 'text': ' So now we have a bunch of statements'}, {'timestamp': (25.02, 26.72), 'text': ' that were pulled from the essay'}, {'timestamp': (26.72, 28.84), 'text': ' that are all about super linear returns'}, {'timestamp': (0.0, 1.58), 'text': ' across different fields.'}, {'timestamp': (1.58, 2.36), 'text': ' All right, cool.'}, {'timestamp': (2.36, 3.86), 'text': \" And let's look at this last one,\"}, {'timestamp': (3.86, 5.92), 'text': ' or second to last one, teachers and coaches.'}, {'timestamp': (5.92, 6.76), 'text': ' Okay, cool.'}, {'timestamp': (6.76, 8.7), 'text': ' Consequences of inferior product quality'}, {'timestamp': (8.7, 11.1), 'text': ' on business viability and customer base.'}, {'timestamp': (11.1, 12.74), 'text': ' Nice, all right.'}, {'timestamp': (12.74, 13.86), 'text': \" So that's kind of interesting.\"}, {'timestamp': (13.86, 15.7), 'text': \" Now we're starting to group similar items together.\"}, {'timestamp': (15.7, 18.04), 'text': ' So if we have a question that pops up'}, {'timestamp': (18.04, 20.36), 'text': ' about product quality and business viability,'}, {'timestamp': (20.36, 22.22), 'text': \" well, here's the chunk that you gotta look at.\"}, {'timestamp': (22.22, 23.38), 'text': ' Now, is this perfect?'}, {'timestamp': (23.38, 24.66), 'text': ' Well, not quite yet,'}, {'timestamp': (24.66, 26.56), 'text': ' because you could get some complicated questions'}, {'timestamp': (26.56, 28.72), 'text': ' where you may want multiple things from different chunks,'}, {'timestamp': (0.0, 4.02), 'text': \" But I think this is a really interesting direction on how we'd start to move towards there.\"}, {'timestamp': (4.44, 7.16), 'text': ' And if we actually want to get these chunks because we want to go do something interesting'}, {'timestamp': (7.16, 10.68), 'text': ' with them, maybe proper index them, then you can go and get those and get the list of strings.'}, {'timestamp': (11.28, 11.88), 'text': ' So there you go.'}, {'timestamp': (11.98, 13.54), 'text': \" That's agentic chunking.\"}, {'timestamp': (13.86, 16.34), 'text': \" Now, again, it's slow and it's expensive.\"}, {'timestamp': (16.5, 21.3), 'text': \" But if you bet that language models will speed up and they'll get cheaper, which I'm guessing\"}, {'timestamp': (21.3, 24.48), 'text': ' they will, then this type of method starts to come into play.'}, {'timestamp': (24.62, 29.14), 'text': ' Finally, what I want to do is I want to congratulate you on finishing the five levels of text'}, {'timestamp': (0.0, 5.14), 'text': ' chunking and text splitting. We are almost done, but I wanted to throw in a bonus level in there,'}, {'timestamp': (5.34, 11.36), 'text': ' right? So this bonus level is going to be through alternative representations. Now, much like before,'}, {'timestamp': (11.46, 15.62), 'text': \" like our chunking commandment, we need to think about how we're going to get our data ready for\"}, {'timestamp': (15.62, 20.2), 'text': \" our language model, right? Chunking is only one part of that story. It's how you're actually going\"}, {'timestamp': (20.2, 24.48), 'text': \" to split your texts. But the next step that you're going to take in front of that is you're going to\"}, {'timestamp': (24.48, 29.0), 'text': \" get embeddings of your text and you're going to go throw those in your knowledge base, right? Well,\"}, {'timestamp': (0.0, 3.94), 'text': \" there's different ways you can get embeddings of your text. And there's different questions that\"}, {'timestamp': (3.94, 9.02), 'text': ' will come up. Like, should you get embeddings of your raw text? Or should you get embeddings of an'}, {'timestamp': (9.02, 12.98), 'text': \" alternative representation of your raw text? That's what we're going to talk about on this\"}, {'timestamp': (12.98, 16.84), 'text': \" bonus level. Let's go through this very briefly. The first thing that we're going to talk about is\"}, {'timestamp': (16.84, 21.5), 'text': ' multi-vector indexing. This is when you do a semantic search, but instead of doing semantic'}, {'timestamp': (21.5, 26.58), 'text': ' search over the embeddings of your raw text, you get it off of something else, like a summary of'}, {'timestamp': (0.0, 4.96), 'text': \" raw text or hypothetical questions of your raw text all right let's do this first one which is\"}, {'timestamp': (4.96, 8.8), 'text': \" summaries and how we'd actually do that so we're going to get our super linear essay one more time\"}, {'timestamp': (8.8, 11.92), 'text': \" i'm going to blow through this part since we already talked about it but what i'm going to do\"}, {'timestamp': (11.92, 16.48), 'text': \" is i'm going to get my six chunks from our document and i'm going to say summarize the following\"}, {'timestamp': (16.48, 21.2), 'text': \" documents and that's going to be a chain that we have there and then we're going to do this but\"}, {'timestamp': (21.2, 25.36), 'text': \" we're going to do this in batch which is one of the other nice things about lang chain expression\"}, {'timestamp': (0.0, 4.8), 'text': \" language and now we have all of our summaries and here's the first one so this is a summary of our\"}, {'timestamp': (4.8, 10.64), 'text': ' first chunk that we have right now what i want to do is i want to get an embedding of this summary'}, {'timestamp': (10.64, 14.48), 'text': \" instead of the embedding of the chunk like i would have with the old method and the way that we're\"}, {'timestamp': (14.48, 18.56), 'text': \" going to do this is we're going to get our vector store ready we're going to use chroma today and\"}, {'timestamp': (18.56, 22.56), 'text': \" we're going to get just a dock store ready which is just going to be the in-memory byte store that\"}, {'timestamp': (22.56, 28.32), 'text': \" link chain has and then we're going to pass in a multi-vector retriever this is a cool link chain\"}, {'timestamp': (0.0, 7.44), 'text': ' abstraction where you can basically go semantic search off one thing but then return another'}, {'timestamp': (7.44, 10.8), 'text': \" thing for your final language model all right we're going to go through this there's a whole\"}, {'timestamp': (10.8, 14.4), 'text': ' tutorial that they have on this in fact i have a whole tutorial on this as well at fullstack'}, {'timestamp': (14.4, 17.92), 'text': \" retrieval.com so i'm going quick through it but that's because i want to show you a whole bunch\"}, {'timestamp': (17.92, 21.68), 'text': \" here we're going to get our summary docs we're going to add those and we're going to change\"}, {'timestamp': (21.68, 26.0), 'text': \" those into proper lang chain documents instead of the actual strings that they were and then here's\"}, {'timestamp': (0.0, 3.84), 'text': \" where the cool part happens we're going to add them to the vector store which is also going to\"}, {'timestamp': (3.84, 7.52), 'text': \" get the embeddings for us we're going to add the summary docs and then we're going to add our normal\"}, {'timestamp': (7.52, 14.56), 'text': ' docs to our normal doc store in our retriever nice cool and then what we could do here is we could go'}, {'timestamp': (14.56, 19.68), 'text': \" to our retriever and we could go get the relevant documents and so now what i'm going to do is with\"}, {'timestamp': (19.68, 25.04), 'text': ' my query this is going to go match on the summary embedding rather than the raw document embedding'}, {'timestamp': (0.0, 5.84), 'text': \" but when these docs are passed back to us these are the raw documents so there's a little happening\"}, {'timestamp': (5.84, 10.08), 'text': ' behind the hood here um but again i encourage you to go to fullstackretrieval.com and go check out'}, {'timestamp': (10.08, 14.08), 'text': \" the tutorial tutorial on this all right there's one method for us the other method you could do\"}, {'timestamp': (14.08, 20.16), 'text': \" is well instead of summaries i want hypothetical questions this one's really nice if you anticipate\"}, {'timestamp': (20.16, 25.12), 'text': \" a q a bot that you're making because then you can start to anticipate well which questions are people\"}, {'timestamp': (0.0, 5.06), 'text': \" going to ask, the language model will generate questions for you. And there's a, your hypothesis\"}, {'timestamp': (5.06, 9.92), 'text': \" is that there's a higher likelihood that these questions will end up matching for you and you'll\"}, {'timestamp': (9.92, 15.7), 'text': ' get better document matching. Cool. Another one you can do is you can do a parent document retriever.'}, {'timestamp': (15.92, 21.82), 'text': ' So now this case, the hypothesis with this method is that if you subset your document even more,'}, {'timestamp': (22.08, 27.86), 'text': \" you'll get a better semantic search. However, in order to answer the question of whatever\"}, {'timestamp': (0.0, 4.4), 'text': \" hypothetical question you could have, you actually want what's around that small document.\"}, {'timestamp': (4.84, 9.86), 'text': ' So yes, that small document will do good semantic search, but you actually want the buffer around'}, {'timestamp': (9.86, 14.4), 'text': ' it. So what comes before and after it? And another way to say that is you want the parent document'}, {'timestamp': (14.4, 19.88), 'text': \" that that small chunk actually comes from. And in order to show you this one, I'm going to do a\"}, {'timestamp': (19.88, 26.16), 'text': ' tutorial from Llama Index. Now, I also have another tutorial on this on fullstackretrieval.com'}, {'timestamp': (0.0, 1.12), 'text': ' if you want to go check that out.'}, {'timestamp': (1.66, 6.04), 'text': \" So with Lama Index, we're going to do their hierarchical node parser.\"}, {'timestamp': (6.5, 8.96), 'text': \" And what you're going to do is you're going to give a list of chunk sizes.\"}, {'timestamp': (9.28, 13.94), 'text': \" So they're going to get chunks that are 2048, 512, and 128 here.\"}, {'timestamp': (14.32, 15.62), 'text': \" And let's go do our Paul Graham essay.\"}, {'timestamp': (15.92, 17.44), 'text': \" Let's see how many nodes we actually have.\"}, {'timestamp': (17.76, 19.2), 'text': ' This is 119 nodes.'}, {'timestamp': (19.46, 21.84), 'text': \" That's because 128 is pretty small for us here.\"}, {'timestamp': (22.26, 24.22), 'text': ' And if we take a look at one of these relationships here,'}, {'timestamp': (25.22, 27.68), 'text': ' and this is one of the smaller ones because this is something at the end,'}, {'timestamp': (27.68, 28.4), 'text': \" and that's the 128.\"}, {'timestamp': (0.0, 3.64), 'text': ' you can see that these chunks are quite small,'}, {'timestamp': (4.38, 6.46), 'text': ' and we are just looking at the relationships here.'}, {'timestamp': (6.66, 9.24), 'text': ' You can get a source, a previous, a next,'}, {'timestamp': (9.42, 11.38), 'text': ' and then the important part here is the parent.'}, {'timestamp': (11.82, 13.82), 'text': ' So if you were to do this via the Lama index way,'}, {'timestamp': (14.28, 18.12), 'text': ' yes, you matched on the 128 chunk'}, {'timestamp': (18.12, 19.74), 'text': ' because that has good semantic search,'}, {'timestamp': (19.84, 23.18), 'text': ' but you actually got the 512 or the 2048, right?'}, {'timestamp': (23.3, 24.88), 'text': \" I won't show you how to go all the way through that\"}, {'timestamp': (24.88, 26.26), 'text': \" because again, that's a whole separate tutorial.\"}, {'timestamp': (27.12, 28.5), 'text': \" And then the last thing that I'll show here\"}, {'timestamp': (0.0, 1.56), 'text': ' is around the graph structure.'}, {'timestamp': (1.98, 4.26), 'text': \" So sometimes you're going to go over your raw text\"}, {'timestamp': (4.26, 5.82), 'text': ' and instead of chunking your raw text,'}, {'timestamp': (5.96, 9.1), 'text': ' you actually want to extract a graph structure from that text'}, {'timestamp': (9.1, 12.06), 'text': \" because there's a lot of entities within your text.\"}, {'timestamp': (12.34, 13.96), 'text': \" I'm going to do that via the diff bot.\"}, {'timestamp': (14.06, 15.42), 'text': ' And this is the diff bot transformer.'}, {'timestamp': (15.86, 19.8), 'text': \" We're going to go, I'm going to say Greg lives in New York.\"}, {'timestamp': (20.18, 21.0), 'text': ' Greg is friends with Bobby.'}, {'timestamp': (21.1, 22.88), 'text': ' San Francisco is a great city, but New York is amazing.'}, {'timestamp': (23.0, 23.76), 'text': ' Greg lives in New York.'}, {'timestamp': (23.76, 26.38), 'text': \" And let's see what actually pops out of over here.\"}, {'timestamp': (0.0, 6.96), 'text': ' so now we have this so now we have actually have a graph document and one of our nodes'}, {'timestamp': (6.96, 12.82), 'text': ' is node id greg type person properties k greg we have another bobby node which is cool we have'}, {'timestamp': (12.82, 18.5), 'text': \" another node here which is an entity location it's entity new york we have another relationship\"}, {'timestamp': (18.5, 22.8), 'text': ' node so this is kind of be like an edge and this is a social relationship between greg and bobby'}, {'timestamp': (22.8, 27.22), 'text': \" so i won't go through all these but either way you can start to see how now you can use a graph\"}, {'timestamp': (0.0, 5.56), 'text': \" structure to answer questions about a person or your specific text. But that's a little outside\"}, {'timestamp': (5.56, 11.62), 'text': ' the chunking side. Either way, that is actually the rest of the tutorial. And I want to congratulate'}, {'timestamp': (11.62, 17.38), 'text': \" you on what is quite a long video. I'm not sure how long this is. Probably one of my longer ones\"}, {'timestamp': (17.38, 21.82), 'text': \" that we have yet. Either way, I'm excited that you're here. My name is Greg Kamrad, and I am on\"}, {'timestamp': (21.82, 25.8), 'text': ' a mission to figure out how AI and business are going to overlap the Venn diagrams with each other.'}, {'timestamp': (25.8, 28.68), 'text': ' This has been a lot of fun. Thank you for joining me. We will see you later.'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract only the transcribed text from the result\n",
        "transcription = result['text']\n",
        "\n",
        "# Save transcription to a .txt file\n",
        "with open(\"/content/transcription_output.txt\", \"w\") as file:\n",
        "    file.write(transcription)\n",
        "\n",
        "print(\"Transcription saved to transcription_output.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpFS6HaZd5Xv",
        "outputId": "402ff5bc-bc83-40d0-b9f5-dfe6b9a7f26d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription saved to transcription_output.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Access with gradio"
      ],
      "metadata": {
        "id": "SHFexWzgnxkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def transcribe(inputs):\n",
        "    if inputs is None:\n",
        "        raise gr.Error(\"No audio file\")\n",
        "\n",
        "    text = pipe(inputs, generate_kwargs={\"task\": \"transcribe\"}, return_timestamps=True)[\"text\"]\n",
        "    return text"
      ],
      "metadata": {
        "id": "FJNj4-8QX4dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(\n",
        "    fn=transcribe,\n",
        "    inputs=[\n",
        "        gr.Audio(sources=[\"microphone\", \"upload\"], type=\"filepath\"),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Whisper Large V3 Turbo: Transcribe Audio\",\n",
        "    description=(\n",
        "        \"Transcribe long-form microphone or audio inputs. Thanks to HuggingFace\"\n",
        "    ),\n",
        "    allow_flagging=\"never\",\n",
        ")"
      ],
      "metadata": {
        "id": "62Bbfa4bX5Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "fXQm2UpxX8lL",
        "outputId": "113222ae-2b18-461c-c71b-155ba1724c84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://36b3a05895a278d6ae.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://36b3a05895a278d6ae.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "DCk7xM0-njuS",
        "outputId": "69790ad5-4009-41b3-f08b-6e10708fc361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
            "----\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://36b3a05895a278d6ae.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://36b3a05895a278d6ae.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zq5TTwZVpdHu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}