{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1PPxYEBa6eu__LquGoFFJZkhYgWVYE6kh","timestamp":1729438018538}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["* https://docs.confident-ai.com/docs/getting-started"],"metadata":{"id":"derI9HZt1Go5"}},{"cell_type":"markdown","source":["# ðŸ¤— Welcome to DeepEval!\n","\n","Thanks for trying us out, we're here to provide you with the best LLM evaluation experience you can dream of ðŸ˜Š any questions or concerns you may have, [come talk to us on discord,](https://discord.com/invite/a3K9c8GRGt) we're always here to help!\n","\n","# What is DeepEval?\n","\n","DeepEval is the evaluation framework for LLMs. It takes the latest research (eg., G-Eval, SelfCheckGPT, RAGAS) in LLM evaluation and implement it as metrics for anyone to easily plug and use.\n","\n","Our strength lies in the simplicity and ease of use, while being a full evaluation suite for LLMs. Hope you enjoy trying us out!\n","\n","# Installation\n","\n","Install deepeval, you can ignore the warnings at the end of the installation."],"metadata":{"id":"bDgDprF_xbkS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hz58Byh2m60V"},"outputs":[],"source":["!pip install -U deepeval"]},{"cell_type":"markdown","source":["^^ Please ignore the warnings after installation, but if you're concerned please run `!pip install cohere` as well."],"metadata":{"id":"8weaJmIDr1PF"}},{"cell_type":"markdown","source":["# Login (recommended step)\n","\n","Login to Confident AI to log evaluation results on the cloud. Later, you can use Confident AI to centralize evaluation datasets, perform real-time evaluations in production, and much more."],"metadata":{"id":"FMwb2vPYoA2Y"}},{"cell_type":"code","source":["!deepeval login"],"metadata":{"id":"E1Xh8ybXn7dJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" # ðŸ˜‡ Create Your First Test Case\n","\n"," A test case in `deepeval` mimics a user interaction with your LLM (application)Here:\n"," - `input` is what a user would input\n"," - `actual_output` is the output of yoru LLM (application)\n"," - `retrieval_context` is the retrieved context in your RAG pipeline.\n","\n","Note that only RAG metrics require `retrieval_context` when creating a test case. Visit the [test cases section in our docs](https://docs.confident-ai.com/docs/evaluation-test-cases) to learn about how a test case work in `deepeval`."],"metadata":{"id":"MoyW1ntQoQbv"}},{"cell_type":"code","source":["from deepeval.test_case import LLMTestCase\n","\n","test_case = LLMTestCase(\n","  input=\"What if these shoes don't fit?\",\n","  # Replace this with the actual output of your LLM application\n","  actual_output=\"We offer a 30-day full refund at no extra cost.\",\n","  # Replace this with the retrieval context (in the RAG pipeline) of your LLM application\n","  retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra cost.\"]\n",")"],"metadata":{"id":"nGjEshHFm8tD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ðŸ¤— Create Your First Metric\n","\n","A metric in `deepeval` evaluates based on the values of the parameters in a particular test case. `deepeval` incorporates the latest research into its metrics implementation so you don't have to.\n","\n","## Set OpenAI API Key\n","\n","Most of `deepeval`'s metrics are evaluated using LLMs. To begin, set your `OPENAI_API_KEY` below (IMPORTANT: don't include quotation \"\" marks!)"],"metadata":{"id":"xwSlhH0KpGk-"}},{"cell_type":"code","source":["%env OPENAI_API_KEY=<your-openai-api-key>"],"metadata":{"id":"MMwsf9mfsZWU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(Note that you don't have to use OpenAI, although we highly highly highly recommend it, since evaluation requires a high level of reasoning. That being said, if you want to use a custom model like Azure OpenAI, visit the [metrics section in our docs](https://docs.confident-ai.com/docs/metrics-introduction#using-a-custom-llm))\n","\n","\n","## Create Your Metric\n","\n","In this example, we create an `AnswerRelevancyMetric`, which measures the answer relevancy of a RAG based LLM application. Not all metrics are RAG metrics. For a list of full metrics and an explanation for each, visit [the metrics section in our docs](https://docs.confident-ai.com/docs/metrics-introduction)"],"metadata":{"id":"cksQJRtrskG3"}},{"cell_type":"code","source":["from deepeval.metrics import AnswerRelevancyMetric\n","\n","answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)"],"metadata":{"id":"X9HpXp6sngFu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ðŸš€ Run Your First Evaluation\n","\n","With a test case and metric ready, you can start using our `evaluate()` function to evaluate your LLM (application).\n","\n","The `evaluate()` function accepts a list of test cases, and a list of metrics. Under the hood, it evaluates each individual test case using the list of provided metrics. A test case only passess if all the metrics are passing. For more information, including how to use our Pytest integration for evaluation, visit the evaluation [section in our docs.](https://docs.confident-ai.com/docs/evaluation-introduction)"],"metadata":{"id":"-v2tKr8Yp8Ry"}},{"cell_type":"code","source":["from deepeval import evaluate\n","\n","evaluate([test_case], [answer_relevancy_metric])"],"metadata":{"id":"C0yBxUBHnzWF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Using Standalone Metrics\n","\n","`deepeval` offers a simple way for anyone to plug and use our metrics. This is especially useful if you're looking to build your own evaluation pipelines. Using the previous example:"],"metadata":{"id":"hhU0L9uWrWk1"}},{"cell_type":"code","source":["from deepeval.test_case import LLMTestCase\n","from deepeval.metrics import AnswerRelevancyMetric\n","\n","\n","test_case = LLMTestCase(\n","  input=\"What if these shoes don't fit?\",\n","  # Replace this with the actual output of your LLM application\n","  actual_output=\"We offer a 30-day full refund at no extra cost.\",\n","  # Replace this with the retrieval context (in the RAG pipeline) of your LLM application\n","  retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra cost.\"]\n",")\n","answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n","\n","answer_relevancy_metric.measure(test_case)\n","print(answer_relevancy_metric.score)\n","print(answer_relevancy_metric.reason)"],"metadata":{"id":"EaWouoOUrkgj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create Your First Evaluation Dataset\n","\n","An evaluation dataset in `deepeval` is a collection of test cases. It offers a rich set of features for you to manipulate evaluation data. For more information, visit the [dataset section in our docs.](https://docs.confident-ai.com/docs/evaluation-datasets)"],"metadata":{"id":"QPh31S8QuCam"}},{"cell_type":"code","source":["from deepeval import evaluate\n","from deepeval.dataset import EvaluationDataset\n","from deepeval.test_case import LLMTestCase\n","from deepeval.metrics import AnswerRelevancyMetric\n","\n","test_case_1 = LLMTestCase(\n","  input=\"What if these shoes don't fit?\",\n","  actual_output=\"We offer a 30-day full refund at no extra cost.\",\n","  retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra cost.\"]\n",")\n","test_case_2 = LLMTestCase(\n","  input=\"What should you do if there's a fire?\",\n","  actual_output=\"Drop and roll.\",\n","  retrieval_context=[\"Don't use the elevator in the case of a fire.\"]\n",")\n","answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n","\n","dataset = EvaluationDataset(test_cases=[test_case_1, test_case_2])\n","\n","# Same as before, using the evaluate function\n","evaluate(dataset, [answer_relevancy_metric])\n","\n","# Or, use the evaluate method directly, they're exactly the same\n","# dataset.evaluate([answer_relevancy_metric])"],"metadata":{"id":"MxFMHQZOucxs"},"execution_count":null,"outputs":[]}]}