{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Run Ollama\n",
        "Modified by Jon Chun 7 Oct 2024"
      ],
      "metadata": {
        "id": "rC58lwR84bAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install"
      ],
      "metadata": {
        "id": "NSxMGv3u4udm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y pciutils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmQZbNa03s10",
        "outputId": "f9ba54bc-6473-4362-8565-12b1bce5cf17"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpci3 pci.ids\n",
            "The following NEW packages will be installed:\n",
            "  libpci3 pci.ids pciutils\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 343 kB of archives.\n",
            "After this operation, 1,581 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 pci.ids all 0.0~2022.01.22-1 [251 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpci3 amd64 1:3.7.0-6 [28.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 pciutils amd64 1:3.7.0-6 [63.6 kB]\n",
            "Fetched 343 kB in 2s (214 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package pci.ids.\n",
            "(Reading database ... 123620 files and directories currently installed.)\n",
            "Preparing to unpack .../pci.ids_0.0~2022.01.22-1_all.deb ...\n",
            "Unpacking pci.ids (0.0~2022.01.22-1) ...\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "Preparing to unpack .../libpci3_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.7.0-6) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking pciutils (1:3.7.0-6) ...\n",
            "Setting up pci.ids (0.0~2022.01.22-1) ...\n",
            "Setting up libpci3:amd64 (1:3.7.0-6) ...\n",
            "Setting up pciutils (1:3.7.0-6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh # download ollama api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bYsG8t53syf",
        "outputId": "c37aba47-9614-4ec9-c277-aa344f8fbafa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "l9y9I-ZC4w41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "IWDJ5_ZS3svB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "oVICLymo7b2w"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "EEy2eC0w3srd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ollama"
      ],
      "metadata": {
        "id": "ScI1NAQ84ynx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Python script to start the Ollama API server in a separate thread\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()"
      ],
      "metadata": {
        "id": "Q0plDC9X35xc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ollama Model"
      ],
      "metadata": {
        "id": "bJHIH9lM40UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:\n",
        "\n",
        "from IPython.display import clear_output\n",
        "!ollama pull llama3.1:8b\n",
        "clear_output()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e7Ru6A635un",
        "outputId": "d26a1fe7-9c68-4bd3-be26-d2cfe11c6360"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.08 s, sys: 143 ms, total: 1.22 s\n",
            "Wall time: 1min 33s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Serve Ollama in Colab"
      ],
      "metadata": {
        "id": "X-Iuxjm543c1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U lightrag[ollama]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxwqB8yW35ri",
        "outputId": "92632cc1-4686-4883-9d00-a0badd23374c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightrag[ollama]\n",
            "  Downloading lightrag-0.1.0b6-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting backoff<3.0.0,>=2.2.1 (from lightrag[ollama])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (3.1.4)\n",
            "Collecting jsonlines<5.0.0,>=4.0.0 (from lightrag[ollama])\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (1.6.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.4 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (1.26.4)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from lightrag[ollama])\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (6.0.2)\n",
            "Collecting tiktoken<0.8.0,>=0.7.0 (from lightrag[ollama])\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.4 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (4.66.5)\n",
            "Collecting ollama<0.3.0,>=0.2.1 (from lightrag[ollama])\n",
            "  Downloading ollama-0.2.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.3->lightrag[ollama]) (2.1.5)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines<5.0.0,>=4.0.0->lightrag[ollama]) (24.2.0)\n",
            "Collecting httpx<0.28.0,>=0.27.0 (from ollama<0.3.0,>=0.2.1->lightrag[ollama])\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2.32.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama])\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2.2.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.2.2)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading ollama-0.2.1-py3-none-any.whl (9.7 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightrag-0.1.0b6-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.1/159.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, jsonlines, h11, backoff, tiktoken, httpcore, lightrag, httpx, ollama\n",
            "Successfully installed backoff-2.2.1 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jsonlines-4.0.0 lightrag-0.1.0b6 ollama-0.2.1 python-dotenv-1.0.1 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lightrag.core.generator import Generator\n",
        "from lightrag.core.component import Component\n",
        "from lightrag.core.model_client import ModelClient\n",
        "from lightrag.components.model_client import OllamaClient, GroqAPIClient\n",
        "\n",
        "import time"
      ],
      "metadata": {
        "id": "VWXatdD139yC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Queries"
      ],
      "metadata": {
        "id": "wOk_ACTq47VN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_template = r\"\"\"<SYS>\n",
        "You are a helpful assistant.\n",
        "</SYS>\n",
        "User: {{input_str}}\n",
        "You:\"\"\"\n",
        "\n",
        "class SimpleQA(Component):\n",
        "    def __init__(self, model_client: ModelClient, model_kwargs: dict):\n",
        "        super().__init__()\n",
        "        self.generator = Generator(\n",
        "            model_client=model_client,\n",
        "            model_kwargs=model_kwargs,\n",
        "            template=qa_template,\n",
        "        )\n",
        "\n",
        "    def call(self, input: dict) -> str:\n",
        "        return self.generator.call({\"input_str\": str(input)})\n",
        "\n",
        "    async def acall(self, input: dict) -> str:\n",
        "        return await self.generator.acall({\"input_str\": str(input)})"
      ],
      "metadata": {
        "id": "MvLn1Cm939r3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YLUSc9LP3kcz"
      },
      "outputs": [],
      "source": [
        "from lightrag.components.model_client import OllamaClient\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "model = {\n",
        "    \"model_client\": OllamaClient(),\n",
        "    \"model_kwargs\": {\"model\": \"llama3.1:8b\"}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt-Response"
      ],
      "metadata": {
        "id": "T679fEjh4qUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa = SimpleQA(**model)\n",
        "output=qa(\"what is happiness\")\n",
        "display(Markdown(f\"**Answer:** {output.data}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "-BR_AVxf4TD2",
        "outputId": "41a5efde-8ce1-428d-af9a-dfb490ef834c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Answer:** A profound question!\n\nHappiness is a complex and multifaceted concept that can be subjective and personal, but I'll try to break it down in simple terms.\n\nHappiness is often described as a positive emotional state characterized by feelings of joy, contentment, and well-being. It's a sense of satisfaction with one's life, circumstances, and experiences. People who experience happiness tend to feel good about themselves, their relationships, and the world around them.\n\nSome common aspects that contribute to happiness include:\n\n1. **Positive emotions**: Feelings of joy, love, excitement, gratitude, and wonder.\n2. **Life satisfaction**: A sense of contentment with one's life, including achievements, relationships, and personal growth.\n3. **Well-being**: Good physical and mental health, as well as a strong sense of purpose and meaning in life.\n4. **Relationships**: Strong connections with others, built on trust, empathy, and love.\n5. **Personal growth**: A sense of progress, learning, and self-improvement.\n\nHappiness can manifest in different ways, such as:\n\n1. **Intrinsic happiness**: Feeling happy from within, often due to personal values, relationships, or activities that bring joy.\n2. **Extrinsic happiness**: Experiencing happiness through external factors, like travel, achievements, or material possessions.\n3. **Flow experiences**: Engaging in activities that fully absorb and challenge one's attention, leading to a sense of complete absorption and enjoyment.\n\nUltimately, happiness is unique to each individual, and what brings happiness to one person might not be the same for another. Do you have any specific thoughts on happiness?"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa = SimpleQA(**model)\n",
        "output=qa(\"Why do fools fall in love?\")\n",
        "display(Markdown(f\"**Answer:** {output.data}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "L7vznB8O4psS",
        "outputId": "211e44c1-9149-409c-b271-19a620dbbcf3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Answer:** A classic question!\n\nAccording to the 1956 song \"Fools Rush In\" by Tommie Connor, made famous by Elvis Presley and other artists, the lyrics say:\n\n\"Why do fools fall in love? That's what I'd like to know. Some things about that fool you can't explain, But there they go again...\"\n\nIn a more literal sense, people often ask this question when it seems like two individuals who may not be the best match are still drawn to each other.\n\nWould you like me to elaborate on why people might fall in love with someone who's not necessarily \"the best\" match? Or is there something else I can help with?"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa = SimpleQA(**model)\n",
        "output=qa(\"Who invented the lightbulb?\")\n",
        "display(Markdown(f\"**Answer:** {output.data}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "bjjVd66V5Czy",
        "outputId": "f4ee871d-5462-4ff8-d299-556068069c85"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Answer:** The invention of the lightbulb is a bit more complex than just one person. While Thomas Edison is often credited with inventing the lightbulb, he actually improved upon an existing design.\n\nThe first incandescent lightbulb was actually developed by Humphry Davy in 1802, using a battery and a thin strip of carbon. Later, scientists like Warren de la Rue and Frederick de Moleyns worked on similar designs.\n\nHowever, Thomas Edison's contributions were significant. He developed the first commercially practical incandescent lightbulb in 1879, which used a carbon filament and lasted for hours. His design improved upon earlier versions by using a longer-lasting bamboo filament and a more efficient vacuum pump to remove air from the bulb.\n\nSo while it's not entirely accurate to say that Thomas Edison invented the lightbulb, his improvements made it a practical reality, paving the way for widespread use of electric lighting in homes and businesses. Would you like to know more about the history of lightbulbs?"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa = SimpleQA(**model)\n",
        "output=qa(\"What were the main causes of the US Civil War?\")\n",
        "display(Markdown(f\"**Answer:** {output.data}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "rLJkliI-5HUQ",
        "outputId": "72b5a120-90bd-40e6-d17d-c93e25f82fac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Answer:** The US Civil War, also known as the American Civil War, was a pivotal event in American history that occurred from 1861 to 1865. The main causes of the war can be attributed to several key factors:\n\n1. **Slavery**: The disagreement over slavery and its expansion into new territories was a primary cause of the conflict. Southern states, which relied heavily on agriculture and slave labor, wanted to protect and expand the institution of slavery, while Northern states, which were more industrialized, wanted it abolished.\n2. **States' rights vs. Federal power**: The debate over states' rights versus federal power was also a significant factor in the lead-up to the war. Southern states felt that the federal government was infringing on their rights as sovereign states and sought greater autonomy.\n3. **Economic differences**: The North and South had distinct economies, with the North being more industrialized and the South relying heavily on agriculture. This led to disagreements over tariffs, trade policies, and other economic issues.\n4. **Westward expansion**: As the United States expanded westward, conflicts arose between Southern states, which wanted to expand slavery into new territories, and Northern states, which opposed this expansion.\n5. **The Kansas-Nebraska Act (1854)**: This law allowed new states to decide for themselves whether to allow slavery, effectively repealing the Missouri Compromise of 1820, which had prohibited slavery in certain territories.\n6. **Abraham Lincoln's election**: Abraham Lincoln's victory in the 1860 presidential election was seen as a threat by Southern states, who feared he would restrict their rights and abolish slavery.\n\nThese factors ultimately led to the secession of 11 Southern states from the Union and the formation of the Confederacy. The war that ensued resulted in the deaths of an estimated 620,000 to 750,000 soldiers and civilians and led to a profound transformation in American society.\n\nWould you like me to elaborate on any of these points or provide more information?"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBIA4dxn6HSX",
        "outputId": "5f70b102-b295-44a0-f21d-8d5b0a4e4378"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lightrag.core.types.GeneratorOutput"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMNdtwVE6Ji0",
        "outputId": "f5780d65-b12c-4b6d-9778-1c7e1722bdd5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__annotations__',\n",
              " '__class__',\n",
              " '__class_getitem__',\n",
              " '__dataclass_fields__',\n",
              " '__dataclass_params__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__match_args__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__orig_bases__',\n",
              " '__parameters__',\n",
              " '__post_init__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__slots__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_is_protocol',\n",
              " 'data',\n",
              " 'error',\n",
              " 'format_class_str',\n",
              " 'format_example_str',\n",
              " 'from_dict',\n",
              " 'from_json',\n",
              " 'from_yaml',\n",
              " 'metadata',\n",
              " 'raw_response',\n",
              " 'to_dict',\n",
              " 'to_dict_class',\n",
              " 'to_json',\n",
              " 'to_json_obj',\n",
              " 'to_json_signature',\n",
              " 'to_schema',\n",
              " 'to_schema_str',\n",
              " 'to_yaml',\n",
              " 'to_yaml_obj',\n",
              " 'to_yaml_signature',\n",
              " 'usage']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(output.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLqaZx1t6JfS",
        "outputId": "7454c7e9-fc37-47fc-d368-d388bfbd85c6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Synthetic Data"
      ],
      "metadata": {
        "id": "31l7MKpD5bG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_llama_3_prompt(user, system=\"\", assistant=\"\"):\n",
        "    system_prompt = \"\"\n",
        "    if system:\n",
        "        system_prompt = (\n",
        "            f\"<|start_header_id|>system<|end_header_id|>\\n\\n{system}<|eot_id|>\"\n",
        "        )\n",
        "\n",
        "    user_prompt = f\"<|start_header_id|>user<|end_header_id|>\\n\\n{user}<|eot_id|>\"\n",
        "    assistant_prompt = f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{assistant}<|eot_id|>\" if assistant else \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "\n",
        "    return f\"<|begin_of_text|>{system_prompt}{user_prompt}{assistant_prompt}\"\n",
        "\n",
        "def get_movie_schema():\n",
        "    return \"\"\"\\\n",
        "    0|Title|TEXT eg. \"Inception\"\n",
        "    1|Director|TEXT eg. \"Christopher Nolan\"\n",
        "    2|Year|INT eg. \"2010\"\n",
        "    3|Rating|TEXT eg. \"PG-13\"\n",
        "    4|Runtime|TEXT eg. \"148 min\" castable to int\n",
        "    5|Genre|TEXT eg. \"Sci-Fi\"\n",
        "    6|Box_Office|TEXT eg. \"$829,895,144\" and when null has a value \"N/A\"\n",
        "    \"\"\"\n",
        "\n",
        "def generate_question_and_query():\n",
        "    system = \"You are a data analyst with 10 years of experience writing complex SQL queries.\\n\"\n",
        "    system += (\n",
        "        \"Consider a table called 'movies' with the following schema (columns)\\n\"\n",
        "    )\n",
        "    system += get_movie_schema()\n",
        "    system += \"Consider the following questions, and queries used to answer them:\\n\"\n",
        "\n",
        "    question = \"\"\"What is the highest-grossing movie of all time?\"\"\"\n",
        "\n",
        "    sql = \"SELECT Title, Box_Office FROM movies WHERE Box_Office != 'N/A' ORDER BY CAST(REPLACE(Box_Office, ',', '') AS INTEGER) DESC LIMIT 1;\"\n",
        "\n",
        "    system += \"Question: \" + question + \"\\n\"\n",
        "    system += \"Query: \" + sql + \"\\n\"\n",
        "\n",
        "    user = \"Write a question and a query that are similar but different to those above.\\n\"\n",
        "    user += \"Format the question and query as a JSON object, i.e.\\n\"\n",
        "    user += '{\"question\" : str, \"sql_query\": str }.\\n'\n",
        "\n",
        "    user += \"Make sure to only return me valid sqlite SQL query generated as response to the question. Don't give me any comments. Just return question and query as JSON objects. Make sure query is relevant to the question. Make sure each query is complete and ends with a ;\\n\"\n",
        "\n",
        "    prompt = make_llama_3_prompt(user, system)\n",
        "\n",
        "    # Generate the result from the model\n",
        "    # result = ollama.generate(model='llama3.1', prompt=prompt)\n",
        "    result = qa(prompt)\n",
        "\n",
        "    # Inspect and parse the result['response']\n",
        "    response_str = result.data # result['response']\n",
        "    try:\n",
        "        response_dict = json.loads(response_str)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Failed to parse response as JSON:\", e)\n",
        "        response_dict = {}\n",
        "\n",
        "    return response_dict\n",
        "\n",
        "def save_to_jsonl(data, file_path):\n",
        "    with open(file_path, 'a') as f:\n",
        "        for entry in data:\n",
        "            f.write(json.dumps(entry) + '\\n')"
      ],
      "metadata": {
        "id": "nQDfv61d5NlD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file_path = 'questions_queries.jsonl'\n",
        "num_iterations = 10  # Define how many questions and queries you want to generate\n",
        "all_questions_queries = []\n",
        "\n",
        "for _ in tqdm(range(num_iterations), desc=\"Generating Questions\", unit=\"question\"):\n",
        "    question_and_query = generate_question_and_query()\n",
        "    all_questions_queries.append(question_and_query)\n",
        "\n",
        "save_to_jsonl(all_questions_queries, output_file_path)\n",
        "files.download(output_file_path)\n",
        "\n",
        "print(f\"Saved {num_iterations} questions and queries to {output_file_path}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "qUn4m3TI5gIY",
        "outputId": "29240506-8bd5-4610-db4a-bc60c4ca203d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Questions: 100%|██████████| 10/10 [00:18<00:00,  1.84s/question]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f65e88a9-5e1b-4002-8655-e19b4d1af097\", \"questions_queries.jsonl\", 4579)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 10 questions and queries to questions_queries.jsonl\n"
          ]
        }
      ]
    }
  ]
}