# AI Fine-Tuning

### FINE-TUNING

* [Open Source and In-House: How Uber Optimizes LLM Training (17 Oct 2024)](https://www.uber.com/blog/open-source-and-in-house-how-uber-optimizes-llm-training/?ref=dailydev)
* [Fine-tuning large language models (LLMs) in 2024 (23 Jul 2024)](https://www.superannotate.com/blog/llm-fine-tuning)
* [Fine Tune Llama 3.2 (3b) - On Custom Dataset 2X Faster | With Google Colab and 0$ (17:23) (2 Oct 2024)](https://www.youtube.com/watch?v=inT-m5Y9Pdo)

### Fine-Tuning in Detail

### Libraries

* Unsloth
  * [Llama 3.3 Unsloth (10 Dec 2024)](https://unsloth.ai/blog/llama3-3)
  * [Github](https://github.com/unslothai/unsloth)
  * [Colab](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing)
  * [Video (17:35) (29 Sep 2024)](https://www.youtube.com/watch?v=YZW3pkIR-YE)
* Videos
  * [Fine-Tune Llama3 using Synthetic Data (37:02) (6 May 2024)](https://www.youtube.com/watch?v=jAnVvLRPvJo&list=PLD7HrIBE_yqLv07dzDYmgmuRJRIdl0uQg&index=15) [Colab](https://colab.research.google.com/drive/1WzHVa1b6Zg862XloxY7KlvKb1gCBiMVT?usp=sharing)
  * [How to Fine-Tune Qwen-2 for Free | Open-Source LLM Guide (13:19) (30 Sep 2024)](https://www.youtube.com/watch?v=f9Fi9IKa-0M) [Colab](https://colab.research.google.com/drive/1evx24o1tN33HAb5eI-hFsQtez1VivdDo)
* [How-To Fine-Tune Llama 3.2 11B Vision Model on Custom Dataset Locally (17:07) (1 Oct 2024)](https://www.youtube.com/watch?v=zGqQGtmXFQ8) [Colab](https://colab.research.google.com/github/philschmid/deep-learning-pytorch-huggingface/blob/main/training/fine-tune-multimodal-llms-with-trl.ipynb)

### Fine-Tuning Frameworks

* [LlamaFactory Github](https://github.com/hiyouga/LLaMA-Factory)
* [TransformerLab Github](https://github.com/transformerlab/transformerlab-app)

### Uncensoring

* [Uncensor any LLM with abliteration (13 Jun 2024)](https://huggingface.co/blog/mlabonne/abliteration)


* PeFT and LoRA

* [Fine-tuning LLMs with PEFT and LoRA (15:34) (24 Apr 2023)](https://www.youtube.com/watch?v=Us5ZFp16PaU)

### Reasoning Videos

* [DeepSeekR1 - Full Breakdown (22:48) (27 Jan 2025)](https://www.youtube.com/watch?v=gzZihJ5miZE&t=504s)
  * [Colab](https://colab.research.google.com/drive/1JGvU1lM2fwfEtmNKERQGh7L0t42ip15q?usp=sharing)
* [I Reverse Engineered Deepseek R1: Here Is The Code and Explanation Of The Method (18:42) (27 Jan 2025)](https://www.youtube.com/watch?v=eRi3rr4Y1as)
  * [Colab](https://colab.research.google.com/drive/1iG1EMTyA2YRgZv00WmyxUNVxERDLB60e#scrollTo=QFne6zWuvR7S)
* [5 Easy Ways to help LLMs to Reason (50:36) (Jul 2024)](https://www.youtube.com/watch?v=YrKieg1rqe0)
* [Datacamp: Fine Tune DeepSeek R1 | Build a Medical Chatbot (48:51) (1 Feb 2025)](https://www.youtube.com/watch?v=qcNmOItRw4U&t=797s)
  * [Huggingface: medical-o1-reasoning-SFT Dataset](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT?row=0)
* [AIAnytime Github: Distill Model FT(3 Feb 2025)](https://github.com/AIAnytime/Fine-Tune-DeepSeek/blob/main/Distill_Model_FT.ipynb)
* [OPEN DeepSeek R1: SECRETS Uncovered (18:39) (29 Jan 2025)](https://www.youtube.com/watch?v=2ENvGkkK36E)

### Unsloth

* [Unsloth Github Colab](https://github.com/unslothai/unsloth)
* [Unsloth Reddit](https://www.reddit.com/r/unsloth)
  * [Unsloth Docker](https://www.reddit.com/r/unsloth/comments/1iggb1u/i_built_a_docker_tool_that_has_unsloth_ollama/)
* [Fine-Tuning LLMs for Smarter AI: Build a Reasoning Agent! (4:50) (10 Feb 2025)](https://www.youtube.com/watch?v=6tkV4XJ91Bg)
  * [Blog](https://ubiai.tools/fine-tune-llm-for-agentic-reasoning-to-demonstrate-better-performance-compared-to-vanilla-llms/)

GRPO

* [Unsloth Blog: Train your own R1 reasoning model with Unsloth (GRPO) (6 Feb 2025) ](https://unsloth.ai/blog/r1-reasoning)
* [Train Your Own Reasoning Model Like DeepSeek R1 in Free Google Colab (19:33) (6 Feb 2025)](https://www.youtube.com/watch?v=nBQ-vP5X2c0)
  * [Colab for Llama 3.1 8B](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)
* (!)[Turn ANY LLM into a Mini Deepseek R1 ðŸ’¥Fine-Tuning with GRPO!!! (15:45) (5 Feb 2025)](https://www.youtube.com/watch?v=i5CXUNCM1Ug)
  * [Colab Synthesis](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb) (2hr on A100 math finetuning)
* [Fine Tune Phi 4 For Free! (8:55) (13 Jan 2025)](https://www.youtube.com/watch?v=EwmrFIUAE4c) Colab upon request

Create Reasoning Dataset 
* [Camel-AI](https://www.camel-ai.org/)
* [Create Reasoning Dataset with DeepSeek R1 and Camel (11:51) (10 Feb 2025)](https://www.youtube.com/watch?v=GkGVZE1XMI4&t=14s)
  * [Colab Self-Improving Math Reasoning Data Distillation from DeepSeek R1 with CAMEL](https://colab.research.google.com/drive/1_u8mKhj-6t09NrebX6ru4HW9jpu3BcmJ?usp=sharing)

Kaggle

* [GRPOTrainer-DeepseekR1](https://www.kaggle.com/code/jchun2000/grpotrainer-deepseekr1/edit)
* [Fine-tuning DeepSeek R1 - DataCamp Walkthrough (10 Feb 2025)](https://www.kaggle.com/code/smohajer85/fine-tuning-deepseek-r1-datacamp-walkthrough)
* [LLM-generate-good-essay (10 Feb 2025)](https://www.kaggle.com/code/johnsonhk88/llm-generate-good-essay)
* [AIMO2_deepseek-r1_cross-validation_demonstration 10 (8 Feb 2025)](AIMO2_deepseek-r1_cross-validation_demonstration)
* [Fine-tuning DeepSeek R1 (Reasoning Model)_Raza - EMPTY](https://www.kaggle.com/code/razaali10/fine-tuning-deepseek-r1-reasoning-model-raza)
* [Fine-tuning DeepSeek R1 (Reasoning Model) 19 (26 Jan 2025)](www.kaggle.com/code/kingabzpro/fine-tuning-deepseek-r1-reasoning-model)
* [Fine-Tuning DeepSeek-R1-Distill-Qwen-1.5B (KTO) 18 (25 Jan 2025)](https://www.kaggle.com/code/ksmooi/fine-tuning-deepseek-r1-distill-qwen-1-5b-kto)
* [DeepSeek-R1-Distill-Llama-8B-QA-Fine-Tuning](https://www.kaggle.com/code/jchun2000/deepseek-r1-distill-llama-8b-qa-fine-tuning/edit)

* [DeepSeek R1 Distill Qwen 1.5B](https://colab.research.google.com/drive/1JGvU1lM2fwfEtmNKERQGh7L0t42ip15q?usp=sharing)
* [Youtube_demos/Llama32_TrainReason
/training.py](https://github.com/yeyu2/Youtube_demos/blob/main/Llama32_TrainReason/training.py)