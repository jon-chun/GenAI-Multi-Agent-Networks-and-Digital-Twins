# AI Benchmarks

## LLM Benchmarks

### Awesome Benchmarks

* [BradyFU/Awesome-Multimodal-Large-Language-Models (Mar 2025)](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)
* [bebetterest/AwesomeCode_on_LLMReasoningRL (Mar 2025)](https://github.com/bebetterest/AwesomeCode_on_LLMReasoningRL)
* [haizelabs/Awesome-LLM-Judges (Feb 2024)](https://github.com/haizelabs/Awesome-LLM-Judges)
* [underlines/awesome-ml (Feb 2025)](https://github.com/underlines/awesome-ml/blob/master/llm-tools.md#benchmarking)
* [HqWu-HITCS/Awesome-LLM-Survey (Sep 2024)](https://github.com/HqWu-HITCS/Awesome-LLM-Survey)
* [horseee/Awesome-Efficient-LLM (Feb 2025)](https://github.com/horseee/Awesome-Efficient-LLM)
* [samkhur006/awesome-llm-planning-reasoning](https://github.com/samkhur006/awesome-llm-planning-reasoning)
* [JShollaj/awesome-llm-interpretability (Dec 2024)](https://github.com/JShollaj/awesome-llm-interpretability)
* [johnnyhwu/Awesome-LLM-Tabular (Dec 2024  )](https://github.com/johnnyhwu/Awesome-LLM-Tabular)
* [huybery/Awesome-Code-LLM (Dec 2024)](https://github.com/huybery/Awesome-Code-LLM?tab=readme-ov-file#-awesome-code-llms-leaderboard)
* [Hannibal046/Awesome-LLM?tab=readme-ov-file (2025 )](https://github.com/Hannibal046/Awesome-LLM?tab=readme-ov-file)
* [wgwang/awesome-LLMs-In-China (Dec 2025)](https://github.com/wgwang/awesome-LLMs-In-China)
* [SihyeongPark/Awesome-LLM-Benchmark](https://github.com/SihyeongPark/Awesome-LLM-Benchmark)
* *[onejune2018/Awesome-LLM-Eval (Oct 2024)](https://github.com/onejune2018/Awesome-LLM-Eval)
* [xatkit-bot-platform/awesome-nlp-chatbot-benchmarks (Oct 2024)](https://github.com/xatkit-bot-platform/awesome-nlp-chatbot-benchmarks)
* [ndrwmlnk/Awesome-Benchmarks-for-Physical-Reasoning-AI (2024)](https://github.com/ndrwmlnk/Awesome-Benchmarks-for-Physical-Reasoning-AI)
* *[tjunlp-lab/Awesome-LLMs-Evaluation-Papers (2023)](https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers)
* [GSL-Benchmark/GSLB (2024)](https://github.com/GSL-Benchmark/GSLB)
* [Schuture/Benchmarking-Awesome-Diffusion-Models (2022)](https://github.com/Schuture/Benchmarking-Awesome-Diffusion-Models)
* Misc
* [SuperBruceJia/Awesome-LLM-Self-Consistency (Oct 2024)](https://github.com/SuperBruceJia/Awesome-LLM-Self-Consistency)
* [Geralt-Targaryen/Awesome-Education-LLM (Sep 2024)](https://github.com/Geralt-Targaryen/Awesome-Education-LLM)

### Aggregate Benchmarks

* [Humanity's Last Exam](https://lastexam.ai/)
  * [Github](https://github.com/centerforaisafety/hle)
* [Stanford HELM](https://crfm.stanford.edu/helm/)
* [Google BigBench Hard](https://github.com/suzgunmirac/BIG-Bench-Hard)
* [Evalverse](https://github.com/UpstageAI/evalverse)

## Leaderboards 

* [huggingface.co](https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a)
* [Scale.com](https://scale.com/leaderboard)
* *[extractum](https://llm.extractum.io/)

## LLM Benchmark Leaderboards

### Listing

* [Vellum.ai](https://www.vellum.ai/llm-leaderboard)
* [extractum.io](https://llm.extractum.io/static/llm-leaderboards/) 

### Function Calling

* [Berkeley](https://gorilla.cs.berkeley.edu/leaderboard.html)

### Embedding

* [MTEB](https://huggingface.co/spaces/mteb/leaderboard)

### Multilingual

* [EU OpenGPT](https://multilingual.com/european-llm-leaderboard-a-new-move-in-multilingual-ai-development/)

### Coding

* [Artificial Analysis Coding Benchmark](https://artificialanalysis.ai/leaderboards/models)
* [Aider](https://aider.chat/docs/leaderboards/)
* [CanAICode](https://huggingface.co/spaces/mike-ravkine/can-ai-code-results)

### Uncensored

* [UGI Leaderboard](https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard)

### Reasoning

* [AutoRace](https://www.llm-reasoners.net/leaderboard)

### Emotion

* [EQBench](https://eqbench.com/)

### Voice

* [HF ASR](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)

### Vision

* [OpenCompass](https://rank.opencompass.org.cn/home)

### Specialized

* [Prompt to Leaderboard](https://lmarena.ai/?p2l)

### Agent Benchmarks

* [The AgenticBench](https://github.com/TheAgenticAI/TheAgenticBench)
  * [AgenticBench - Digital Worker Framework - Install and Review (8:51) (22 Feb 2025)](https://www.youtube.com/watch?v=jLh5RwySYrA)
* [AgentEvals](https://github.com/langchain-ai/agentevals)
* [AgentBench](https://github.com/THUDM/AgentBench)
* [GAIA Benchmark](https://huggingface.co/spaces/gaia-benchmark/leaderboard)

## LLM Leaderboards

* [Kaggle ARC Challenge 2024](https://www.kaggle.com/competitions/arc-prize-2024/leaderboard)
* [Huggingface LMsys Leaderboard LLMsys Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)
* [Huggingface LMSys Chatbot Area](https://lmarena.ai/?leaderboard)
* LLM Benchmarks:
    * [Stanford HELM](https://crfm.stanford.edu/helm/)
    * [Google BigBench Hard](https://github.com/suzgunmirac/BIG-Bench-Hard)
* LLM Leaderboards
    * [Kaggle ARC Challenge 2024](https://www.kaggle.com/competitions/arc-prize-2024/leaderboard)
    * [Huggingface LMsys Leaderboard][LLMsys Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)
    * [Huggingface LMSys Chatbot Area](https://lmarena.ai/?leaderboard)
* [LMArena.ai](https://lmarena.ai/
* [LLMArena.ai](https://llmarena.ai/compare?llms=3094,3092&mode=edit)
* [Uncensored Leaderboard](https://llm.extractum.io/list/?uncensored)

## AI AGent Leaderboards

* [AgentLab](https://github.com/ServiceNow/AgentLab/?tab=readme-ov-file)
* [BrowserGym Leaderboard](https://huggingface.co/spaces/ServiceNow/browsergym-leaderboard)

## Libraries

* [TruLens](https://github.com/truera/trulens)

## Research

* [AutoRace (Automated Reasoning Chain Evaluation) by Hao et al. (11 Aug 2024)](https://www.llm-reasoners.net/leaderboard)

## Dataset Annotation

## LLM as Judge

Several open-source and Python libraries are available for automatic evaluations of large language models (LLMs), including using LLMs as judges. Here's a list of some notable options:

## Open-Source LLM Evaluation Libraries

# LLM Evaluation Libraries FAQ

## What are some popular open-source LLM evaluation libraries?

1. [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness)
2. [PromptBench](https://github.com/microsoft/promptbench)
3. [DeepEval](https://github.com/confident-ai/deepeval)
4. [MLflow LLM Evaluate](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html)
5. [OpenAI Evals](https://github.com/openai/evals)

## What features does lm-eval-harness offer?

- Over 200 benchmarks
- Easy extensibility
- Used for new LLM evaluation scores
- Powers Hugging Face's Open LLM Leaderboard

[Source](https://www.restack.io/p/ai-model-evaluation-answer-python-libraries-llm-evaluation-cat-ai)

## What capabilities does PromptBench provide?

- Prompt construction and engineering tools
- Dataset and model loading
- Adversarial prompt attack functionality
- Dynamic evaluation protocols
- Analysis tools

[Source](https://www.jmlr.org/papers/v25/24-0023.html)

## What are the key features of DeepEval?

- 14+ evaluation metrics for RAG and fine-tuning
- Metrics include G-Eval, Hallucination, Faithfulness, and more
- pytest integration
- Real-time production evaluations

[Source](https://dev.to/guybuildingai/-top-5-open-source-llm-evaluation-frameworks-in-2024-98m)

## What does MLflow LLM Evaluate offer?

- Modular package for custom evaluation pipelines
- RAG and question-answering evaluation support
- User-friendly developer experience

[Source](https://www.restack.io/p/ai-model-evaluation-answer-python-libraries-llm-evaluation-cat-ai)

## Are there other notable evaluation options?

- [LangChain](https://python.langchain.com/docs/guides/evaluation/)
- [Promptfoo](https://github.com/promptfoo/promptfoo)
- [Arize AI Phoenix](https://github.com/Arize-ai/phoenix)

## What factors should be considered when choosing an evaluation library?

- Specific metrics needed
- Integration ease
- LLM-as-judge evaluation support
- Scalability requirements
- Visualization and reporting capabilities

[Source](https://www.superannotate.com/blog/llm-evaluation-guide)

## Can these libraries be customized?

Most libraries allow for customization to implement specific evaluation methods.

## How current is the field of LLM evaluation?

The field is rapidly evolving. It's recommended to check for the latest updates and community contributions for each tool.

[Source](https://www.evidentlyai.com/blog/open-source-llm-evaluation)

# NOTES:

1. **DeepEval**
DeepEval is a comprehensive evaluation framework for LLMs that offers over 14 evaluation metrics for both retrieval-augmented generation (RAG) and fine-tuning use cases[5]. It includes metrics like:
- G-Eval
- Hallucination
- Faithfulness
- Contextual Relevancy
- Answer Relevancy
- RAGAS
- Bias
- Toxicity

DeepEval allows for real-time evaluations in production and integrates with pytest for easy testing[5].

2. **MLflow LLM Evaluate**
MLflow provides a modular package for running evaluations in custom pipelines. It supports RAG evaluation and question-answering evaluation with an intuitive developer experience[5].

3. **Promptfoo**
Promptfoo is an open-source tool for testing and evaluating LLM outputs across different models and parameter combinations[6].

4. **LangChain**
While primarily known for building LLM applications, LangChain also includes evaluation tools that can be used to assess model performance[3].

5. **lm-eval-harness**
This library contains over 200 benchmarks and is easily extendable. It has been used to generate evaluation scores for new LLMs and powers platforms like Hugging Face's Open LLM Leaderboard[3].

6. **PromptBench**
PromptBench is a unified library for evaluating LLMs, offering components for prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools[4].

7. **Arize AI Phoenix**
While more limited in scope, Arize AI Phoenix offers evaluation criteria for QA correctness, hallucination, and toxicity[5].

8. **OpenAI Evals**
OpenAI's open-source framework for evaluating LLMs, which can be adapted for various evaluation tasks[5].

## Commercial Options with Open-Source Components

1. **TruLens**
TruLens, while having commercial offerings, also provides open-source components. It focuses on transparency and interpretability in LLM evaluation[7].

2. **Deepchecks**
Deepchecks offers both open-source and commercial versions, focusing on evaluating the LLM itself rather than LLM applications. It provides extensive dashboards and visualization UI for evaluation results[5].

When choosing an evaluation library, consider factors such as:
- The specific metrics you need
- Ease of integration with your existing workflow
- Support for LLM-as-judge evaluations
- Scalability requirements
- Visualization and reporting capabilities

Many of these libraries allow for customization, so you can implement LLM-as-judge evaluations even if they're not provided out-of-the-box. Additionally, the field of LLM evaluation is rapidly evolving, so it's worth checking the latest updates and community contributions for each of these tools.

Citations:
[1] https://www.deepchecks.com/best-llm-evaluation-tools/
[2] https://github.com/Praveengovianalytics/falcon-evaluate
[3] https://www.restack.io/p/ai-model-evaluation-answer-python-libraries-llm-evaluation-cat-ai
[4] https://www.jmlr.org/papers/v25/24-0023.html
[5] https://dev.to/guybuildingai/-top-5-open-source-llm-evaluation-frameworks-in-2024-98m
[6] https://www.reddit.com/r/LocalLLaMA/comments/1cecoja/easy_llm_evaluation_library/
[7] https://www.superannotate.com/blog/llm-evaluation-guide
[8] https://truera.com/llm-evaluation-and-llm-observability-at-enterprise-scale-for-effective-llm-ops/

While Falcon Evaluate is not a widely known open-source library for LLM evaluation, there are several other open-source libraries and frameworks that serve similar purposes for evaluating large language models. Here are some notable options:

## Popular Open-Source LLM Evaluation Libraries

### 1. lm-eval-harness

This is one of the most comprehensive and widely used open-source libraries for LLM evaluation[1]. Key features include:

- Contains over 200 benchmarks
- Easily extendable
- Used to generate evaluation scores for new LLMs
- Powers platforms like Hugging Face's Open LLM Leaderboard

### 2. PromptBench

PromptBench is a unified library specifically designed for evaluating LLMs[2]. It offers:

- Prompt construction and engineering tools
- Dataset and model loading capabilities
- Adversarial prompt attack functionality
- Dynamic evaluation protocols
- Analysis tools

The library aims to facilitate research in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols.

### 3. DeepEval

DeepEval is a comprehensive evaluation framework that provides[4]:

- Over 14 evaluation metrics for both retrieval-augmented generation (RAG) and fine-tuning use cases
- Metrics like G-Eval, Hallucination, Faithfulness, Contextual Relevancy, Answer Relevancy, RAGAS, Bias, and Toxicity
- Integration with pytest for easy testing
- Real-time evaluations in production environments

### 4. MLflow LLM Evaluate

MLflow, while known for experiment tracking, also offers LLM evaluation capabilities[1]:

- Modular package for running evaluations in custom pipelines
- Support for RAG evaluation and question-answering evaluation
- Intuitive developer experience

### 5. OpenAI Evals

OpenAI's open-source framework for evaluating LLMs can be adapted for various evaluation tasks[4].

## Other Notable Options

- **LangChain**: While primarily for building LLM applications, it includes evaluation tools[1].
- **Promptfoo**: An open-source tool for testing and evaluating LLM outputs across different models and parameter combinations[4].
- **Arize AI Phoenix**: Offers evaluation criteria for QA correctness, hallucination, and toxicity, though more limited in scope[4].

When choosing an evaluation library, consider factors such as:
- Specific metrics needed for your use case
- Ease of integration with your existing workflow
- Support for LLM-as-judge evaluations
- Scalability requirements
- Visualization and reporting capabilities

Many of these libraries allow for customization, so you can implement specific evaluation methods even if they're not provided out-of-the-box. The field of LLM evaluation is rapidly evolving, so it's worth checking the latest updates and community contributions for each of these tools.

Citations:
[1] https://www.restack.io/p/ai-model-evaluation-answer-python-libraries-llm-evaluation-cat-ai
[2] https://www.jmlr.org/papers/v25/24-0023.html
[3] https://www.evidentlyai.com/blog/open-source-llm-evaluation
[4] https://dev.to/guybuildingai/-top-5-open-source-llm-evaluation-frameworks-in-2024-98m
[5] https://www.superannotate.com/blog/llm-evaluation-guide