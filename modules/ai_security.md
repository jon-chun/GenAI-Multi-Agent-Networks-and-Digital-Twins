# AI SECURITY

## LLM Jailbreaking

* [How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs (12 Jan 2023)](https://arxiv.org/pdf/2401.06373)
* [Github](https://github.com/CHATS-lab/persuasive_jailbreaker)
* [Huggingface Dataset](https://huggingface.co/datasets/CHATS-Lab/Persuasive-Jailbreaker-Data)

* [Unpacking the AI Adversarial Toolkit (4 Oct 2022)](https://hiddenlayer.com/innovation-hub/whats-in-the-box/)